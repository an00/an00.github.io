[{"title":"CentOS L2TP/IPSec vpn","url":"/2017/12/08/CentOS-L2TP-IPSec-vpn/","content":"CentOS L2TP&#x2F;IPSec vpn\n基于 centos 6\n\n\n需要开发人员在外网可以拨号到开发环境调试代码。\n开发人员拨号后只能访问部分地址，需要通过vpn服务器做SNAT，但是只对服务器内网的某几台机器。\n需要控制vpn服务器的用户密码，系统用户登陆vpn机器后是可以通达整个内网的。\n对vpn端口做外网固定IP的映射。\n\n安装软件\n关闭selinux  \n\n\n安装依赖和xl2tpd服务 yum install -y make gcc gmp-devel xmlto bison flex xmlto libpcap-devel lsof  manyum install openswan ppp xl2tpd\n\n修改配置文件配置ipsec\nvim &#x2F;etc.ipsec.conf \n config setup        protostack=netkey        dumpdir=/var/run/pluto/        virtual_private=%v4:10.0.0.0/8,%v4:192.168.0.0/16,%v4:172.16.0.0/12,%v4:25.0.0.0/8,%v4:100.64.0.0/10,%v6:fd00::/8,%v6:fe80::/10conn L2TP-PSK-NAT    rightsubnet=vhost:%priv    also=L2TP-PSK-noNAT conn L2TP-PSK-noNAT    authby=secret    pfs=no    auto=add    keyingtries=3    rekey=no    ikelifetime=8h    keylife=1h    type=transport    left=0.0.0.0             # 监听地址，也可以改为外网IP或域名   leftprotoport=17/1701    # 监听的端口1701   right=%any    rightprotoport=17/%anyinclude /etc/ipsec.d/*.conf\n\nvim &#x2F;etc&#x2F;ipsec.secrets\n include /etc/ipsec.d/*.secrets0.0.0.0 %any: PSK &quot;xxx.gz&quot;# 上面&quot;xxx.gz&quot;为预共享密钥，客户端连接时候需要填写\n\n修改内核参数\n\nvim &#x2F;etc&#x2F;sysctl.conf net.ipv4.ip_forward = 1 net.ipv4.conf.default.rp_filter = 0 net.ipv4.conf.all.send_redirects = 0 net.ipv4.conf.default.send_redirects = 0 net.ipv4.conf.all.log_martians = 0 net.ipv4.conf.default.log_martians = 0 net.ipv4.conf.default.accept_source_route = 0 net.ipv4.conf.all.accept_redirects = 0 net.ipv4.conf.default.accept_redirects = 0 net.ipv4.icmp_ignore_bogus_error_responses = 1\n\n\n 修改完执行sysctl -p生效\n\n\n验证ipsec状态\n\nipsec setup restart\nipsec verify         #可能有部分出错信息   .....Hardware random device [N/A]Two or more interfaces found, checking IP forwarding [OK]Checking rp_filter [ENABLED]/proc/sys/net/ipv4/conf/enp2s0/rp_filter [ENABLED]            # 出现这种网卡设备/proc/sys/net/ipv4/conf/enp3s7/rp_filter [ENABLED]            # 错误的不用管rp_filter is not fully aware of IPsec and should be disabled  # 继续后面步骤Checking that pluto is running [OK]Pluto listening for IKE on udp 500 [OK]Pluto listening for IKE/NAT-T on udp 4500 [OK]Pluto ipsec.secret syntax [OK]Checking ‘ip’ command [OK]Checking ‘iptables’ command [OK]Checking ‘prelink’ command does not interfere with FIPSChecking for obsolete ipsec.conf options [OK]Opportunistic Encryption [DISABLED]ipsec verify: encountered 5 errors – see ‘man ipsec_verify’ for help  \n\n\n启动ipsec服务，加入开机启动项\n service ipsec restartchkconfig ipsec on\n\n配置xl2tpd和ppp认证\nvim &#x2F;etc&#x2F;xl2tpd&#x2F;xl2tpd.conf \n [global]ipsec saref = yes[lns default]; vpn客户端地址段ip range = 10.1.2.10-10.1.2.254; vpn服务器地址local ip = 10.1.2.1require chap = yesrefuse pap = yesrequire authentication = yesname = LinuxVPNserverppp debug = yespppoptfile = /etc/ppp/options.xl2tpdlength bit = yes\ncat &#x2F;etc&#x2F;ppp&#x2F;options.xl2tpd\n require-mschap-v2#ms-dns 8.8.8.8#ms-dns 8.8.4.4asyncmap 0authcrtsctslockhide-passwordmodemdebugname l2tpdproxyarplcp-echo-interval 30lcp-echo-failure 4# 指定日志文件logfile /var/log/xl2tpd.log\n\n配置登陆用户的认证vim &#x2F;etc&#x2F;ppp&#x2F;chap-secrets\n # Secrets for authentication using CHAP# client    server  secret          IP addressesan00    *    passwd    *#用户名  服务器   密码  ip地址\n\n重启xl2tpd服务，添加到开机启动项\n service xl2tpd restartchkconfig xl2tpd on\n\n配置SNAT，允许vpn客户端访问服务器内网部分机器\n原来是没开启iptables的，所以没有其他规则\n\n\n在NAT链添加如下规则，只对vpn客户端到内网部分地址做NAT转发vim &#x2F;etc&#x2F;sysconfig&#x2F;iptables\n *nat:PREROUTING ACCEPT [0:0]:POSTROUTING ACCEPT [0:0]:OUTPUT ACCEPT [0:0]-A POSTROUTING -s 10.1.2.0/24 -d 172.16.88.125 -o eth0 -j MASQUERADE-A POSTROUTING -s 10.1.2.0/24 -d 172.16.88.126 -o eth0 -j MASQUERADE-A POSTROUTING -s 10.1.2.0/24 -d 172.16.88.185 -o eth0 -j MASQUERADECOMMIT\n\n启动iptables，加入到开机启动项\n service iptables restartchkconfig iptables on\n\n客户端连接配置\n点击右下角网络连接图标\n\n在出现的设置中左侧点击”VPN”，然后右侧点击”添加VPN连接”预共享密钥是上文描述的ipsec.secrets中配置项保存，后续即可连接。  \n\n需要修改注册表按win+r键输入regedit  \n 路径：[HKEY_LOCAL_MACHINE/SYSTEM/CurrentControlSet/Services/RasMan/Parameters] 修改：ProhibitIPSec的值修改为1如果没有ProhibitIPSec项，需要点击右键新建一个dword，重命名为ProhibitIPSec，值设置为1重启你的本地电脑，一定记得重启。\n\n建议做完下面步骤再拨号。\n\n\n解决拨号后外网不能上的问题\n按上面第一步打开”网络设置“左侧选择”以太网“，在右侧”相关设置中“点击”更改适配器选项“新窗口中找到vpn适配器右击 –&gt; 属性点击”网络“选项卡   –&gt;双击internet协议版本(TCP&#x2F;IPv4)点击高级 –&gt; IP设置选项卡 –&gt; 取消勾选”在远程网络上使用默认网关“和它的子项然后一路确定。\n\n在桌面编辑一个”xxx.bat”结尾的文件，添加如下内容\n route add 172.16.88.0 mask 255.255.255.0 10.1.2.1\n\n每次拨号后以管理员身份执行上面的命令会添加一个路由，外网还是走本地网络。\n\n\n","categories":["Linux"],"tags":["vpn"]},{"title":"MongoDB 上手简记","url":"/2018/10/11/MongoDB-%E4%B8%8A%E6%89%8B%E7%AE%80%E8%AE%B0/","content":"安装和配置\n官网获取当前稳定版本 4.0.3 \n\n配置文件示例\n\n\nsystemLog: # 日志文件路径  destination: file  logAppend: true  path: /usr/local/mongodb/logs/mongodb.logstorage:  # 数据文件路径  dbPath: /usr/local/mongodb/data/  journal:    enabled: trueprocessManagement:  # 执行mongod启动后在后台创建守护进程运行mongo  fork: true  # pid文件路径  pidFilePath: /usr/local/mongodb/logs/mongod.pidnet:  # 监听地址  port: 27017  bindIp: 0.0.0.0#security:  # 创建管理用户后开启#  authorization: enabled  # 登陆认证#  # 多用于replication时节点间的认证`openssl rand -base64 756`生成#  keyFile: /usr/local/mongodb/etc/security.key#replication:  # 开启复制集功能#    replSetName: rs0  # 设置复制名称\n\n启动mongo服务，这里第一次启动没加认证，创建用户后加上 --auth 启动用户认证\nshell&gt; cd /usr/local/mongodbshell&gt; ./bin/mongod -f mongod.yml about to fork child process, waiting until server is ready for connections.forked process: 16090 # fork子进程为mongo的守护进程child process started successfully, parent exiting\n\n停止 mongo\nshell&gt; cd /usr/local/mongodbshell&gt; ./bin/mongod -f mongod.yml --shutdown\n\nmongo 的日志轮转，旧的日志会重命名成日志名加时间的文件。\n# 系统命令方式shell&gt; kill –SIGUSR1 [PID]# mongo shell命令方式&gt; db.runCommand(&#123;logRotate:1&#125;)\n\n\n\nMongo的使用连接mongo# 连接单节点shell&gt; mongo 127.0.0.1:28001/admin -u admin -p 111111 --authenticationDatabase admin# 连接副本集，需要用mogno url字符串的方式，可以带上用户名密码，不带将提示输入密码shell&gt; mongo mongodb://user:password@10.10.10.15:27001,10.10.10.16:27002,10.10.10.17:27000/mydbname?replicaSet=replicaSet1\n\n\n\nCUDR操作创建 db 和 collection 并插入数据。\n# 进入mongo终端shell&gt; mongo&gt; show dbsadmin   0.000GBconfig  0.000GBlocal   0.000GB# 新建一个db，直接use即可&gt; use acswitched to db ac# use之后若没存储数据，是不会保存的&gt; show dbsadmin   0.000GBconfig  0.000GBlocal   0.000GB# 在当前db创建一个me的collection（类似于关系型数据库的表的意思）&gt; db.me.insert(&#123;name: &#x27;abc&#x27;&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt; db.me.insert(&#123;name: &#x27;EFG&#x27;&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)# 保存后有db已经可以show出来&gt; show dbsac      0.000GBadmin   0.000GBconfig  0.000GBlocal   0.000GB# 查看当前db的collection&gt; show collectionsme# 多条插入&gt; db.me.insertMany([&#123;name: &#x27;hij&#x27;&#125;,&#123;name: &#x27;lmn&#x27;&#125;])&#123;        &quot;acknowledged&quot; : true,        &quot;insertedIds&quot; : [                ObjectId(&quot;5bbd5c530c08adaa73c5d5ef&quot;),                ObjectId(&quot;5bbd5c530c08adaa73c5d5f0&quot;)        ]&#125;# 查看collection me中的数据，若只取一条可db.me.find().limit(1)&gt; db.me.find()db.me.find()&#123; &quot;_id&quot; : ObjectId(&quot;5bbd55750c08adaa73c5d5ed&quot;), &quot;name&quot; : &quot;abc&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5bbd5c030c08adaa73c5d5ee&quot;), &quot;name&quot; : &quot;EFG&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5bbd5c530c08adaa73c5d5ef&quot;), &quot;name&quot; : &quot;hij&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5bbd5c530c08adaa73c5d5f0&quot;), &quot;name&quot; : &quot;lmn&quot; &#125;# 过滤name=&#x27;abc&#x27;的数据&gt; db.me.find(&#123;name: &#x27;abc&#x27;&#125;)&#123; &quot;_id&quot; : ObjectId(&quot;5bbd55750c08adaa73c5d5ed&quot;), &quot;name&quot; : &quot;abc&quot; &#125;# 更新数据，第一个参数是filter条件，第二个为更新动作的内容，也有Many操作。&gt; db.me.update(&#123;name:&#x27;abc&#x27;&#125;, &#123;$set: &#123; age: 20&#125;&#125;)WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 &#125;)&gt; db.me.find(&#123;name:&#x27;abc&#x27;&#125;)&#123; &quot;_id&quot; : ObjectId(&quot;5bbd55750c08adaa73c5d5ed&quot;), &quot;name&quot; : &quot;abc&quot;, &quot;age&quot; : 20 &#125;# 删除记录，传入过滤条件&gt; db.me.deleteOne(&#123;age: 20&#125;)&#123; &quot;acknowledged&quot; : true, &quot;deletedCount&quot; : 1 &#125;&gt; db.me.find()&#123; &quot;_id&quot; : ObjectId(&quot;5bbd5c030c08adaa73c5d5ee&quot;), &quot;name&quot; : &quot;EFG&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5bbd5c530c08adaa73c5d5ef&quot;), &quot;name&quot; : &quot;hij&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5bbd5c530c08adaa73c5d5f0&quot;), &quot;name&quot; : &quot;lmn&quot; &#125;# 删除collection me&gt; db.me.drop()# 删除当前库ac，因为之前已经use进了ac才可以这样删除ac&gt; db.dropDatabase()\n\n用户管理内置角色说明 参考官档\n\n数据库用户角色（Database User Roles）：\n\nread：授予 User 只读数据的权限\nreadWrite：授予 User 读写数据的权限\n\n\n数据库管理角色（Database Administration Roles）：\n\ndbAdmin：在当前 DB 中执行管理操作\ndbOwner：在当前 DB 中执行任意操作\nuserAdmin：在当前 DB 中管理 User\n\n\n备份和还原角色（Backup and Restoration Roles）：\n\nbackup\nrestore\n\n\n跨库角色（All-Database Roles）：\n\nreadAnyDatabase：授予在所有数据库上读取数据的权限\nreadWriteAnyDatabase：授予在所有数据库上读写数据的权限\nuserAdminAnyDatabase：授予在所有数据库上管理User的权限\ndbAdminAnyDatabase：授予管理所有数据库的权限\n\n\n集群管理角色（Cluster Administration Roles）：\n\nclusterAdmin：授予管理集群的最高权限\nclusterManager：授予管理和监控集群的权限，A user with this role can access the config and local databases, which are used in sharding and replication, respectively.\nclusterMonitor：授予监控集群的权限，对监控工具具有readonly的权限\nhostManager：管理 Server\n\n\n超级用户角色\n\nroot：提供对 readWriteAnyDatabase、dbAdminAnyDatabase、userAdminAnyDatabase、clusterAdmin、restore 和 backup 组合的操作和所有资源的访问。\n\n\n\n操作示例\n# 创建管理用户，对所有db有管理权限（按角色分配，userAdminAnyDatabase有所有库的userAdmin权限，也就是可以给其他用户授权）&gt; use admin;&gt; db.createUser(&#123;user: &#x27;admin&#x27;, pwd:&#x27;111111&#x27;, roles: [&#123;role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot;&#125;]&#125;)Successfully added user: &#123;        &quot;user&quot; : &quot;admin&quot;,        &quot;roles&quot; : [                &#123;                        &quot;role&quot; : &quot;userAdminAnyDatabase&quot;,                        &quot;db&quot; : &quot;admin&quot;                &#125;        ]&#125;# 查看用户集合&gt; db.system.users.find().pretty()&#123;        &quot;_id&quot; : &quot;admin.admin&quot;,        &quot;user&quot; : &quot;admin&quot;,        &quot;db&quot; : &quot;admin&quot;,        &quot;credentials&quot; : &#123;                &quot;SCRAM-SHA-1&quot; : &#123;                        &quot;iterationCount&quot; : 10000,                        &quot;salt&quot; : &quot;nf7YX8afzXgn/tbDKW6Pig==&quot;,                        &quot;storedKey&quot; : &quot;2k9Lf+yv77NuKMZkVCahgPBrI4M=&quot;,                        &quot;serverKey&quot; : &quot;DKuwSPYry4sqDtano2o+ttVVRJE=&quot;                &#125;,                &quot;SCRAM-SHA-256&quot; : &#123;                        &quot;iterationCount&quot; : 15000,                        &quot;salt&quot; : &quot;MmKYC+wSetKmZXTYt5DH4nx9ncyDei6obz/xzA==&quot;,                        &quot;storedKey&quot; : &quot;GnlHlaE2mHt5HjyLjv3oMvtSEuzt7MYqD3LTmV03f7c=&quot;,                        &quot;serverKey&quot; : &quot;jrfiEghGv44JdRyc0LRgQaCV05WQiNdtA7fiGHVH9VQ=&quot;                &#125;        &#125;,        &quot;roles&quot; : [                &#123;                        &quot;role&quot; : &quot;userAdminAnyDatabase&quot;,                        &quot;db&quot; : &quot;admin&quot;                &#125;        ]&#125;# 重启mongod，配置文件开启authentication，换admin用户登录shell&gt; ./bin/mongo -u admin -p 111111 --authenticationDatabase admin&gt; use ac&gt; db.me.find()   # 这里admin用户对ac库没有权限，需要创建Error: error: &#123;        &quot;ok&quot; : 0,        &quot;errmsg&quot; : &quot;not authorized on ac to execute command &#123; find: \\&quot;me\\&quot;, filter: &#123;&#125;, lsid: &#123; id: UUID(\\&quot;aa798cc9-a036-4733-adbf-861e9c51a446\\&quot;) &#125;, $db: \\&quot;ac\\&quot; &#125;&quot;,        &quot;code&quot; : 13,        &quot;codeName&quot; : &quot;Unauthorized&quot;&#125;# 创建在ac库下创建an00用户，角色是ac的属主&gt; db.createUser(&#123;user: &#x27;an00&#x27;, pwd:&#x27;111111&#x27;, roles: [&#123;role: &#x27;dbOwner&#x27;, db: &#x27;ac&#x27;&#125;]&#125;)# 换an00用户登录ac库，查看me集合的内容shell&gt; ./bin/mongo -u an00 -p 111111 --authenticationDatabase ac&gt; show dbsac  0.000GB&gt; use acswitched to db ac&gt; show collectionsme&gt; db.me.find()&#123; &quot;_id&quot; : ObjectId(&quot;5bbd5c030c08adaa73c5d5ee&quot;), &quot;name&quot; : &quot;EFG&quot;, &quot;age&quot; : 20 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5bbd5c530c08adaa73c5d5ef&quot;), &quot;name&quot; : &quot;hij&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5bbd5c530c08adaa73c5d5f0&quot;), &quot;name&quot; : &quot;lmn&quot; &#125;&gt; show users&#123;        &quot;_id&quot; : &quot;ac.an00&quot;,        &quot;user&quot; : &quot;an00&quot;,        &quot;db&quot; : &quot;ac&quot;,        &quot;roles&quot; : [                &#123;                        &quot;role&quot; : &quot;dbOwner&quot;,                        &quot;db&quot; : &quot;ac&quot;                &#125;        ],        &quot;mechanisms&quot; : [                &quot;SCRAM-SHA-1&quot;,                &quot;SCRAM-SHA-256&quot;        ]&#125;# 创建root用户，使用内置的root角色&gt; use admin&gt; db.createUser(&#123;user: &#x27;root&#x27;, pwd:&#x27;111111&#x27;, roles: [&#123;role: &quot;root&quot;, db: &quot;admin&quot;&#125;]&#125;)\n\n备份和恢复mongodump备份，-h 指定主机地址，-d 指定 database，-c 指定 collection，-o 指定备份文件保存路径\n./bin/mongodump -u an00 -p &#x27;111111&#x27; --authenticationDatabase ac -h 127.0.0.1:27017 -d ac -o dump/\n\n恢复备份，–drop 表示先删除数据，然后恢复备份，慎用。-d 指定 database，最后跟上有 bson、json 备份数据的路径（有副本集时指定PRIMARY节点）\n./bin/mongorestore -u an00 -p &#x27;111111&#x27; --authenticationDatabase ac -h 127.0.0.1:27018 -d ac dump/ac/\n\n\n\n副本集\n若 mongo 提供给非本机连接，副本集的成员 IP 不应该设成 127.0.0.1，当客户端连接副本集时，使用的是副本集中配置的 IP，127.0.0.1 会导致客户端连接自己本地的端口\n使用文章最开始的配置文件，打开注释\n\n副本集成员配置可以用 rs.config() 查看，配置中的 members 是一个列表，可以包含多个成员。副本集成员常用的配置选项如下，参考官档 \n\n_id 数值，0-255之间，副本集中每个成员的标识符，不可重复，设置后不可修改\n\nhost 字符串，通常是 IP 或 IP 加指定端口的形式，如：”192.168.1.2:28011”\n\npriority 数值，选举 primary 时的优先级，值为0时该成员不会被选举成 primary 节点\n\nhidden 布尔值，对客户端隐藏，通常给延时节点和仲裁节点配上\n\nslaveDelay 数值，设置此成员落后 primary 的秒数，成员成为延时节点\n\narbiterOnly 布尔值，为 true 时，节点将不存数据，只用于选举时投票用\n\nvotes 数值，表示成员选举时的票数，通常是 0 或 1，当为 0 时不具备选举权（无选举权的成员必须priority&#x3D;0），一个副本集最多有 7 个投票选举的成员，如果超过 7 个可以将部分成员此值设为 0\n\n\n配置示例\n&gt; cfg = rs.conf()   // 获取当前配置对象，修改成员ip，重设成员列表&gt; cfg.members = [&#123;&quot;_id&quot;:2,&quot;host&quot; : &quot;192.168.10.234:27017&quot;&#125;, &#123;&quot;_id&quot;: 3,&quot;host&quot; : &quot;192.168.10.234:27018&quot;&#125;, &#123;&quot;_id&quot;: 4,&quot;host&quot; : &quot;192.168.10.234:27019&quot;&#125;]&gt; rs.reconfig(cfg, &#123;force: true&#125;)   // 修改后重设副本集# 或者使用 rs.add 增加成员，&gt; rs.add(&#123;&#x27;_id&#x27; : 10, &#x27;priority&#x27;:2, &#x27;host&#x27; : &#x27;192.168.10.235:27017&#x27;&#125;)# 添加仲裁节点&gt; rs.addArb(&#x27;11.22.33.44:27017&#x27;)&gt; rs.status()  // 查看副本集状态，状态正常客户端重新连接即可正常&gt; rs.config()  // 再次查看配置","categories":["MongoDB"],"tags":["MongoDB"]},{"title":"SSH 隧道","url":"/2018/04/27/SSH%E9%9A%A7%E9%81%93/","content":"SSH 隧道\n对端ip的端口映射到本机；\n本机的端口映射到对端，这时通常做内网穿透。\n\n\n\nSSH命令行\n重要：netstat 查看端口，若最外层主机只监听 127.0.0.1 则修改 sshd_config ，打开 GatewayPorts yes\n\n\n远程转发：本机监听的端口映射到对端机器\n # ssh -R remote_port:local_ip:port [remote]# 将本地服务监听的8080映射到125服务器的8080，125可以是一个云服务器的外网ipssh -CfNg -p 22 -R 8080:localhost:8080 root@172.16.88.125ssh -CfNg -p 22 -R 0.0.0.0:8080:localhost:8080 root@172.16.88.125\n\n本地转发：将远程的服务映射到本机端口访问\n # 将远程服务器监听的端口映射到本机的某个端口ssh -CfNg -p 22 -L 80:localhost:80 root@172.16.88.125\n\n常用参数\n -N      不执行远程命令. 用于转发端口. -n      把 stdin 重定向到 /dev/null (实际上防止从 stdin 读取数据).  ssh在后台运行时一定会用到这个选项. 它的常用技巧是远程运行 X11 程序. -f      要求 ssh 在执行命令前退至后台.         它用于当 ssh 准备询问口令或密语,但是用户希望它在后台进行.         该选项隐含了-n 选项.         在远端机器上启动X11程序的推荐手法就是类似于 ssh -f host xterm 的命令. -C      要求进行数据压缩 -g      允许远端主机连接本地转发的端口-R      Remote 映射本地主机服务监听的端口到远程-L      Local 映射远程主机服务监听端口到本地\n\n","categories":["Linux"],"tags":["ssh"]},{"title":"golang 文件操作","url":"/2019/12/08/golang-%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/","content":"golang 文件操作打开文件在 golang 中，用 os.Open() 来打开文件，将返回一个文件实例 *File 和错误 err，文件实例调用 Close() 方法来关闭文件\n\n\npackage mainimport (    &quot;fmt&quot;    &quot;os&quot;)func main() &#123;    // 打开文件    file, err := os.Open(filename)\tdefer file.Close()  // 关闭文件\tif err != nil &#123;\t\tfmt.Println(&quot;打开文件失败：&quot;, err)\t\treturn\t&#125;\t// 文件操作&#125;\n\n写文件的打开方式又略有不同，需要另外指定参数，用 os.OpenFile() 打开，该方法签名如下，参数在后面写文件部分介绍\nfunc OpenFile(name string, flag int, perm FileMode) (*File, error)\n\n\n\n读取文件读取文件有多种方式，速度可以测试一下再选用  \n\nFile.Read  文件实例自带的操作  \nbufio  带缓冲的IO，速度快，读到内存中缓冲，发起读写操作时会先冲缓冲中获取数据\nioutil 重点是方便，里面一些操作有封装 bufio 库\nioutil.ReadAll\nioutil.Read\n\n\n\n示例代码，分别是文件实例直接读取和循环、bufio 库读取、逐行读取、ioutil 库读取的内容\npackage mainimport (\t&quot;bufio&quot;\t&quot;fmt&quot;\t&quot;io&quot;\t&quot;io/ioutil&quot;\t&quot;os&quot;)// file.Read读取文件func osReader(filename string) &#123;\tfile, err := os.Open(filename)\tif err != nil &#123;\t\tfmt.Println(&quot;打开文件失败：&quot;, err)\t\treturn\t&#125;\tdefer file.Close()\tvar content [32]byte\t// 读取需要一个接收一个byte切片，返回读取数据的大小和错误\tn, err := file.Read(content[:])\tif err == io.EOF &#123; // 文件结束时返回错误是EOF，n是0\t\tfmt.Println(&quot;文件读完了！&quot;)\t&#125;\tif err != nil &#123;\t\tfmt.Println(&quot;读取文件失败！&quot;)\t&#125;\tfmt.Println(string(content[:n]))&#125;// bufio 读取文件func bufioReader(filename string) &#123;\tfile, err := os.Open(filename)\tif err != nil &#123;\t\tfmt.Println(&quot;打开文件失败：&quot;, err)\t&#125;\tdefer file.Close()\t//reader := bufio.NewReader(file)  // 默认大小\treader := bufio.NewReaderSize(file, 32) // 可以指定大小\tcontent := make([]byte, 32)\tn, err := reader.Read(content)\tif err == io.EOF &#123; // 文件结束时返回错误是EOF，n是0\t\tfmt.Println(&quot;文件读完了！&quot;)\t&#125;\tif err != nil &#123;\t\tfmt.Println(&quot;读取文件失败！err:&quot;, err)\t&#125;\tfmt.Println(string(content[:n]))&#125;// fileRead循环读取文件func osReaderLoop(filename string) &#123;\tfile, err := os.Open(filename)\tif err != nil &#123;\t\tfmt.Println(&quot;打开文件失败：&quot;, err)\t\treturn\t&#125;\tdefer file.Close()\tvar content []byte\tvar tmp = make([]byte, 128)\tfor &#123;\t\tn, err := file.Read(tmp)\t\tif err == io.EOF &#123;\t\t\tfmt.Println(&quot;文件读完了！&quot;)\t\t\tbreak\t\t&#125;\t\tif err != nil &#123;\t\t\tfmt.Println(&quot;读取文件失败！err:&quot;, err)\t\t\treturn\t\t&#125;\t\tcontent = append(content, tmp[:n]...) // 追加到content中\t&#125;\tfmt.Println(string(content))&#125;// bufio逐行读取func bufioReaderLine(filename string) &#123;\tfile, err := os.Open(filename)\tif err != nil &#123;\t\tfmt.Println(&quot;打开文件失败：&quot;, err)\t&#125;\tdefer file.Close()\treader := bufio.NewReader(file) // 默认大小\t//var content []byte\tfor &#123;\t\tline, err := reader.ReadString(&#x27;\\n&#x27;) // 指定定界符，读到该字符时返回一次，返回字符串\t\tif err == io.EOF &#123;\t\t\tfmt.Println(&quot;文件读完了！&quot;)\t\t\tbreak\t\t&#125;\t\tif err != nil &#123;\t\t\tfmt.Println(&quot;读取文件失败！err:&quot;, err)\t\t\treturn\t\t&#125;\t\tfmt.Print(line)\t&#125;&#125;// ioutil读取文件func ioutilReader(filename string) &#123;\t// 可以直接读取文件，代码中不用另外打开，返回byte切片和err\tcontent, err := ioutil.ReadFile(filename)\tif err != nil &#123;\t\tfmt.Println(&quot;读取文件失败！err:&quot;, err)\t\treturn\t&#125;\tfmt.Print(string(content)) // 转字符串打印&#125;func main() &#123;\tfilename := &quot;./tmp.txt&quot;\tfmt.Println(&quot;file.Read:&quot;)\tosReader(filename)\tfmt.Println(&quot;bufio读取文件:&quot;)\tbufioReader(filename)\tfmt.Println(&quot;file.Read循环读取:&quot;)\tosReaderLoop(filename)\tfmt.Println(&quot;bufio循环读取:&quot;)\tbufioReaderLine(filename)\tfmt.Println(&quot;ioutil读取:&quot;)\tioutilReader(filename)&#125;\n\n假设有文件 ./tmp.txt，内容如下：\nhello world1！hello world2！hello world3！hello world4！hello world5！\n\n运行结果\nfile.Read:hello world1！hello world2！bufio读取文件:hello world1！hello world2！file.Read循环读取:文件读完了！hello world1！hello world2！hello world3！hello world4！hello world5！bufio循环读取:hello world1！hello world2！hello world3！hello world4！文件读完了！\n\n文件写入需要写文件时，需要用 os.OpenFile() 指定操作模式打开文件，得到文件实例后进行写入，看下它的签名\n\nfilename 文件名 \nflag 是操作模式，在 os 包中有定义这些常量可以用，多个之间可以用 | 拼接\nperm 指定文件权限，为一个八进制数，读写执行分别为(r : 04，w : 02，x : 01)// flag常量定义的部分const (\t// Exactly one of O_RDONLY, O_WRONLY, or O_RDWR must be specified.\tO_RDONLY int = syscall.O_RDONLY // open the file read-only.(只读)\tO_WRONLY int = syscall.O_WRONLY // open the file write-only.(只写)\tO_RDWR   int = syscall.O_RDWR   // open the file read-write.(可读可写)\t// The remaining values may be or&#x27;ed in to control behavior.\tO_APPEND int = syscall.O_APPEND // append data to the file when writing.(追加写)\tO_CREATE int = syscall.O_CREAT  // create a new file if none exists.(不存在将创建)\tO_EXCL   int = syscall.O_EXCL   // used with O_CREATE, file must not exist.(与创建搭配使用，文件必须不存在)\tO_SYNC   int = syscall.O_SYNC   // open for synchronous I/O.(用同步IO，不写缓冲直接落盘)\tO_TRUNC  int = syscall.O_TRUNC  // truncate regular writable file when opened.(清空再写))// OpenFile 函数签名func OpenFile(name string, flag int, perm FileMode) (*File, error)\n\n写文件示例，可以看到 ioutil 是封装的比较方便使用，；另外 bufio 因为有缓冲，所以有个刷盘操作\npackage mainimport (\t&quot;bufio&quot;\t&quot;fmt&quot;\t&quot;io/ioutil&quot;\t&quot;os&quot;)//File实例的写入操作，将创建文件，并清空func fileInstanceWriter(filename string, content string) &#123;\t// 注意若多次重复写入文件，又未指定os.O_TRUNC模式，将从文件开头写，覆盖旧内容\t// 若旧内容比新内容长，覆盖了部分长度的内容，容易导致字符不完整\tfile, err := os.OpenFile(filename, os.O_CREATE|os.O_TRUNC|os.O_WRONLY, 0666)\tif err != nil &#123;\t\tfmt.Println(&quot;打开文件失败：&quot;, err)\t\treturn\t&#125;\tdefer file.Close()\t_, err = file.Write([]byte(content)) // 返回写入的长度\t//_, err = file.WriteString(&quot;测试写入文件！&quot;) // 写入字符串，方法内自动加了[]byte(s)\tif err != nil &#123;\t\tfmt.Println(&quot;写入文件失败：&quot;, err)\t\treturn\t&#125;&#125;// bufio写文件示例func bufioWriter(filename string, content string) &#123;\tfile, err := os.OpenFile(filename, os.O_CREATE|os.O_TRUNC|os.O_WRONLY, 0666)\tif err != nil &#123;\t\tfmt.Println(&quot;打开文件失败：&quot;, err)\t\treturn\t&#125;\tdefer file.Close()\twriter := bufio.NewWriter(file)\tif _, err = writer.WriteString(content); err != nil &#123;\t\tfmt.Println(&quot;写入文件失败：&quot;, err)\t\treturn\t&#125;\tif err = writer.Flush(); err != nil &#123; // 将缓存内容刷盘\t\tfmt.Println(&quot;写入磁盘失败&quot;)\t&#125;&#125;// ioutil写文件示例func ioutilWriter(filename string, content string) &#123;\tif err := ioutil.WriteFile(filename, []byte(content), 0666); err != nil &#123;\t\tfmt.Println(&quot;写入文件失败：&quot;, err)\t\treturn\t&#125;&#125;func main() &#123;\tfileInstanceWriter(&quot;./fileInstanceWriter.txt&quot;, &quot;测试写入文件！&quot;)\tbufioWriter(&quot;./bufioWriter.txt&quot;, &quot;测试用bufio写入文件&quot;)\tioutilWriter(&quot;./ioutilWriter.txt&quot;, &quot;测试用ioutil写入文件&quot;)&#125;\n\n\n\n刚接触，备忘。\n","tags":["golang"]},{"title":"innodb cluster 和 mysqlsh","url":"/2020/09/27/innodb-cluster-%E5%92%8C-mysqlsh/","content":"innodb-cluster 和 mysqlsh官网下载并安装下面几个包（不带过程）\nmysql-server mysql-shell mysql-router\n\n配置 innodb cluster\nmysql 官网提供了一个很好的工具，叫 mysql-shell 以前没体会到妙处，在配置 innodb cluster 时发现非常方便，推荐尝试。\n\ninnodb 集群在 mysql-shell 中配置的大致命令，交互命令都是 js 的语法\n\n\n# 进入mysqlshellshell&gt; mysqlsh root@192.168.234.128:3306# 检查实例配置，如果需要修复某些选项，提示中会建议执行 `dba.configureInstance()`dba.checkInstanceConfiguration(&#x27;root@192.168.234.128:3306&#x27;)dba.checkInstanceConfiguration(&#x27;root@192.168.234.128:3307&#x27;)# 最好记录修复后的配置加到配置文件 -- 不确定dba.configureInstance(&#x27;root@192.168.234.128:3307&#x27;)dba.configureInstance(&#x27;root@192.168.234.128:3306&#x27;)# 创建集群、获取集群，加入实例，实例必须已配置好var cluster = dba.createCluster(&#x27;mine_db&#x27;)cluster.addInstance(&#x27;root@192.168.234.128:3306&#x27;)cluster.addInstance(&#x27;root@192.168.234.128:3307&#x27;)# 查看集群，status中列出mode为R/W的为可读写的节点cluster.describe()cluster.status()# 获取集群var cluster = dba.getCluster(&#x27;mine_db&#x27;)# 若集群节点都离线，会获取失败，提示:Dba.getCluster: This function is not available through a session to a standalone instance (metadata exists, instance belongs to that metadata, but GR is not active) (RuntimeError)# 可以执行下面命令恢复集群dba.rebootClusterFromCompleteOutage(&#x27;mine_db&#x27;)# 切换多主、单主模式cluster.switchToMultiPrimaryMode()cluster.switchToSinglePrimaryMode()# 有节点UNREACHABLE了不能做出冲裁时，写请求hang住，修复需要强制指定某活跃的节点cluster.forceQuorumUsingPartitionOf(&#x27;root@192.168.234.128:3307&#x27;)# mysql-router相关cluster.listRouters()cluster.removeRouterMetadata(&#x27;192.168.234.128::system&#x27;)\n\n当前状态\ncluster.status()&#123;    &quot;clusterName&quot;: &quot;mine_db&quot;,  # 集群名称    &quot;defaultReplicaSet&quot;: &#123;        &quot;name&quot;: &quot;default&quot;,         &quot;primary&quot;: &quot;192.168.234.128:3307&quot;,  # 主节点        &quot;ssl&quot;: &quot;REQUIRED&quot;,         &quot;status&quot;: &quot;OK_NO_TOLERANCE&quot;,         &quot;statusText&quot;: &quot;Cluster is NOT tolerant to any failures.&quot;,  # 由于测试只有2个节点，不能容忍任何一个故障        &quot;topology&quot;: &#123;            &quot;192.168.234.128:3306&quot;: &#123;                &quot;address&quot;: &quot;192.168.234.128:3306&quot;,                 &quot;mode&quot;: &quot;R/O&quot;,   # 只读                &quot;readReplicas&quot;: &#123;&#125;,                 &quot;replicationLag&quot;: null,                 &quot;role&quot;: &quot;HA&quot;,                 &quot;status&quot;: &quot;ONLINE&quot;,                 &quot;version&quot;: &quot;8.0.20&quot;            &#125;,             &quot;192.168.234.128:3307&quot;: &#123;                &quot;address&quot;: &quot;192.168.234.128:3307&quot;,                 &quot;mode&quot;: &quot;R/W&quot;,  # 可读写                &quot;readReplicas&quot;: &#123;&#125;,                 &quot;replicationLag&quot;: null,                 &quot;role&quot;: &quot;HA&quot;,                 &quot;status&quot;: &quot;ONLINE&quot;,                 &quot;version&quot;: &quot;8.0.20&quot;            &#125;        &#125;,         &quot;topologyMode&quot;: &quot;Single-Primary&quot;  # 当前单主模式，cluster.switchToMultiPrimaryMode可切换    &#125;,     &quot;groupInformationSourceMember&quot;: &quot;192.168.234.128:3307&quot;&#125;\n\n增加 mysql-router若router已经在 cluster.listRouters 中，需要 cluster.removeRouterMetadata 后才能添加，rpm包默认会创建mysqlrouter用户，并且service也以此用户身份运行\n生成mysqlrouter配置，执行后可以 cluster.listRouters() 看到\nshell&gt; mysqlrouter --bootstrap root@192.168.234.128:3306 --user=mysqlrouter --force-password-validation --report-host 192.168.234.128shell&gt; systemctl start mysqlrouter.service\n\n生成后提示有监听4个端口，两种协议的读写端口各不同\n## MySQL Classic protocol- Read/Write Connections: 192.168.234.128:6446- Read/Only Connections:  192.168.234.128:6447## MySQL X protocol- Read/Write Connections: 192.168.234.128:64460- Read/Only Connections:  192.168.234.128:64470\n\n连接测试，R/O 模式的节点是 super-read-only 不可写的\nmysql -uroot -p111111 -h192.168.234.128 -P6446 -e &quot;select @@port&quot;mysql -uroot -p111111 -h192.168.234.128 -P6447 -e &quot;select @@port&quot;# 测试语句-- use test;-- create table user (id int unsigned auto_increment primary key, name varchar(32), create_time datetime not null default current_timestamp);-- insert into user (name) values (&#x27;aa&#x27;), (&#x27;bb&#x27;);\n\n中途停掉一个之后，通过 mysqlrouter 连接同样可以读写，mysqlshell中看到有实例状态是 MISSING 并且连接错误。启动后自动加入到了集群中，开始同步数据，恢复状态。\n# mysqld_multi stop 3\n\n测试配置\n从 5.7 改的，不一定是优选，仅供参考\n\n使用 mysqld_multi 配置，8.0.20 版本\n[mysql]#prompt=&quot;(\\\\u@\\\\h:\\\\p) [\\\\d]&gt; &quot;[client]user=root# default use 3306 instancesocket  = /data/3306/mysql.sock[mysqld_multi]mysqld = /usr/local/mysql/bin/mysqld_safemysqladmin = /usr/local/mysql/bin/mysqladminuser = rootpassword = your_passwordlog = /data/multi_mysqld.log[mysqld]########basic settings#########server-id = 10#port = 3306user = mysqlbind_address = 192.168.234.128socket = mysql.sock#autocommit = 0character_set_server=utf8mb4skip_name_resolve = 1max_connections = 800max_connect_errors = 1000#datadir = /data/3306transaction_isolation = REPEATABLE-READexplicit_defaults_for_timestamp = 1join_buffer_size = 134217728tmp_table_size = 67108864tmpdir = /tmp#max_allowed_packet = 16777216max_allowed_packet = 100M#sql_mode = &quot;STRICT_TRANS_TABLES,NO_ENGINE_SUBSTITUTION,NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER&quot;interactive_timeout = 1800wait_timeout = 1800read_buffer_size = 16777216read_rnd_buffer_size = 33554432sort_buffer_size = 33554432########log settings########log_error = error.logslow_query_log = 1slow_query_log_file = slow.loglog_queries_not_using_indexes = 1log_slow_admin_statements = 1log_slow_slave_statements = 1log_throttle_queries_not_using_indexes = 10expire_logs_days = 90long_query_time = 2min_examined_row_limit = 100########replication settings########master_info_repository = TABLErelay_log_info_repository = TABLElog_bin = bin.logsync_binlog = 1gtid_mode = onenforce_gtid_consistency = 1log_slave_updatesbinlog_format = row relay_log = relay.logrelay_log_recovery = 1binlog_gtid_simple_recovery = 1slave_skip_errors = ddl_exist_errors#report-host = 192.168.234.128#report-port = 3306# 并行复制slave-parallel-type=LOGICAL_CLOCKslave-parallel-workers=2    # 并行的Coordinator线程数量master_info_repository=TABLErelay_log_info_repository=TABLEbinlog_checksum = noneslave_preserve_commit_order = on########innodb settings########innodb_page_size = 16384#innodb_buffer_pool_size = 512minnodb_buffer_pool_size = 256minnodb_buffer_pool_instances = 2innodb_buffer_pool_load_at_startup = 1innodb_buffer_pool_dump_at_shutdown = 1innodb_lru_scan_depth = 100    # no more than innodb_io_capacity / innodb_buffer_pool_instancesinnodb_lock_wait_timeout = 50innodb_io_capacity = 800       # innodb_io_capacity_max * 80%innodb_io_capacity_max = 1000  # current disk raid 5, max iops 1000+-innodb_flush_method = O_DIRECTinnodb_flush_neighbors = 1innodb_log_file_size = 128Minnodb_log_buffer_size = 16777216innodb_purge_threads = 4innodb_thread_concurrency = 64innodb_print_all_deadlocks = 1innodb_strict_mode = 1innodb_sort_buffer_size = 67108864########semi sync replication settings########plugin_dir=/usr/local/mysql/lib/pluginplugin_load = &quot;rpl_semi_sync_master=semisync_master.so;rpl_semi_sync_slave=semisync_slave.so&quot;loose_rpl_semi_sync_master_enabled = 1loose_rpl_semi_sync_slave_enabled = 1# - AFTER_SYNC 表示的是无损复制（ 5.7 默认）# - AFTER_COMMIT 表示的是半同步复制rpl_semi_sync_master_wait_point=AFTER_SYNC# 超时五秒后换回异步的方式#loose_rpl_semi_sync_master_timeout = 5000########ssl settings########ssl-ca = ca.pemssl-cert = server-cert.pemssl-key = server-key.peminnodb_buffer_pool_dump_pct = 40innodb_page_cleaners = 4innodb_undo_log_truncate = 1innodb_max_undo_log_size = 256Minnodb_purge_rseg_truncate_frequency = 128binlog_gtid_simple_recovery=1log_timestamps=systemtransaction_write_set_extraction=MURMUR32[mysqld2]report-host = 192.168.234.128report-port = 3306datadir = /data/3306server-id = 10port = 3306socket = /data/3306/mysql.sockpid-file = /data/3307/mysql2.pid[mysqld3]report-host = 192.168.234.128report-port = 3307datadir = /data/3307server-id = 20port = 3307socket = /data/3307/mysql.sockpid-file = /data/3307/mysql3.pid\n\n","categories":["MySQL"],"tags":["innodb","mysqlsh"]},{"title":"k8s 测试环境问题记录","url":"/2018/06/29/k8s%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/","content":"k8s测试环境问题记录\n在公司准备测试环境，记录一些过程中碰到的问题\n\ndashboard认证\ndashboard 大概就发现一个比较明显的好处，用来执行容器命令比较方便 : )\n\n安装 dashboard 先去 github获取 kubernetes-dashboard.yaml\n最后 Service 部分添加 NodePort 映射。\n\n\n# ...省略前面内容...kind: ServiceapiVersion: v1metadata:  labels:    k8s-app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kube-systemspec:  ports:    - port: 443      targetPort: 8443      nodePort: 9000  type: NodePort  selector:    k8s-app: kubernetes-dashboard\n打开 kube-proxy 节点的9000端口，提示认证使用 token，通过下面的命令获取。复制整个长串的 token 到页面登陆 dashboard。\n# kubectl -n kube-system get secret NAME                                TYPE                                  DATA      AGEan00-token-brpds                    kubernetes.io/service-account-token   3         14ddefault-token-jct5w                 kubernetes.io/service-account-token   3         22delasticsearch-logging-token-zwr2b   kubernetes.io/service-account-token   3         14dfluentd-es-token-rxrxc              kubernetes.io/service-account-token   3         44mheapster-token-qk4m9                kubernetes.io/service-account-token   3         14dkube-dns-autoscaler-token-nmgjs     kubernetes.io/service-account-token   3         16dkube-dns-token-s5hkb                kubernetes.io/service-account-token   3         16dkubernetes-dashboard-certs          Opaque                                0         14dkubernetes-dashboard-key-holder     Opaque                                2         21dkubernetes-dashboard-token-7nt98    kubernetes.io/service-account-token   3         14d# 找token关键字的secret，获取到最后的token字段内容粘贴到页面登陆。# kubectl -n kube-system describe secret kubernetes-dashboard-token-7nt98Name:         kubernetes-dashboard-token-7nt98Namespace:    kube-systemLabels:       &lt;none&gt;Annotations:  kubernetes.io/service-account.name=kubernetes-dashboard              kubernetes.io/service-account.uid=54c71522-6f80-11e8-bc0b-525400eac085Type:  kubernetes.io/service-account-tokenData====ca.crt:     1107 bytesnamespace:  11 bytestoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi03bnQ5OCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjU0YzcxNTIyLTZmODAtMTFlOC1iYzBiLTUyNTQwMGVhYzA4NSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.leD2gC5FkkN1_0mt5_AwveStC6vh5H8-UL1LqwF7N07xQ2ZKSh1matYyWyv-buMflrks1-my88MKwaYNmMaNRk2-WrlybNLJKrf-QLpmGLdCB3IHBuSViuHHQwPS4g7CD5GNAsuPZF3GAszuBamBD3HJT1okrrH8J3KlstqMpYsEbwullLfgQaznfd02YjrR6izC3sneJpj0vTKSrY8LxweI2xcYVNshZHRacEgdNzwBTe48dU_9pCqyUWOSS2J2Y4EimAMyPQlwDbazgGuHn027neIosxO0ooSbEeiqaEnu9-ATpyJCCWW4ukOxt_PG8VJsNzmZuG18LIA_KImd6A\nkube-dns为k8s中的pod增加service名字解析和自动发现，通过api监控service变动，可以将service名字解析到对应的VIP中。\nkube-dns包含三个镜像（谷歌gcr的仓库，若服务器环境没外网需要先准备）\n\nkube-dns：通过k8s api监控service和ip变动以及它们的对应关系，保存到内存中后就是dns记录了。\ndnsmasq-nanny：通过kube-dns容器获取dns规则，在集群中相当于dns服务器，减轻kube-dns压力，提高稳定性和查询性能\nsidecar：\n\n使用官方的YAML文件创建，在源码的”kubernetes&#x2F;cluster&#x2F;addons&#x2F;“目录下，关于kube-dns主要有两个文件：\ndns服务定义文件，包含Service、ServiceAccount、Deployment等。需要将文件中的 __PILLAR__DNS__SERVER__ 修改成一个clusterip，将 __PILLAR__DNS__DOMAIN__ 修改成 cluster.local（注意保留后面的点）。\n\ndns&#x2F;kube-dns.yaml.base\n\ndns在编排的服务通信和发现中有很大的作用，所以它不能有闪失。官方提供了一个为kube-dns自动scale的配置，文件可以直接创建\n\ndns-horizontal-autoscaler&#x2F;dns-horizontal-autoscaler.yaml\n\n在创建了kube-dns服务后，需要修改pod默认的dns服务器配置。pod被kubelet创建，找到kubelet的配置文件，添加dns配置内容，注意 kubelet.service 启动文件也添加新增的配置选项名，刷新配置后重启 kubelet 。\n# cat /etc/kubernetes/kubelet.conf KUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;KUBE_LOG_LEVEL=&quot;--v=2&quot;NODE_ADDRESS=&quot;--address=0.0.0.0&quot;NODE_HOSTNAME=&quot;--hostname-override=debian-70&quot;KUBE_ALLOW_PRIV=&quot;--allow-privileged=false&quot;KUBE_POD_INFRA_CONTAINER_IMAGE=&quot;--pod-infra-container-image=k8s.gcr.io/pause-amd64:3.1&quot;KUBE_RUNTIME_CGROUPS=&quot;--runtime-cgroups=/systemd/system.slice&quot;KUBE_CGROUPS=&quot;--kubelet-cgroups=/systemd/system.slice&quot;KUBE_FAIL_SWAP_ON=&quot;--fail-swap-on=false&quot;KUBE_CONFIG=&quot;--kubeconfig=/etc/kubernetes/kubeconfig.yml&quot;KUBELET_DNS_IP=&quot;--cluster-dns=10.66.77.2&quot;  # 修改成__PILLAR__DNS__SERVER__替换的clusteripKUBELET_DNS_DOMAIN=&quot;--cluster-domain=cluster.local&quot;  # 修改成__PILLAR__DNS__DOMAIN__的内容\n\n可以创建简单的busybox应用，查看它的 /etc/resolv.conf 文件，默认有 kubernetes.default 指向apiserver。解析得到的是service的clusterIP，不能被ping通。\n# kubectl exec -it busybox sh/ # nslookup kubernetes.defaultServer:    10.66.77.2Address 1: 10.66.77.2 kube-dns.kube-system.svc.cluster.localName:      kubernetes.defaultAddress 1: 10.66.77.1 kubernetes.default.svc.cluster.local/ # / # / # nslookup nginx-serviceServer:    10.66.77.2Address 1: 10.66.77.2 kube-dns.kube-system.svc.cluster.localName:      nginx-serviceAddress 1: 10.66.77.225 nginx-service.default.svc.cluster.local/ # exit# 当前的service，和上面解析的ip是一致的# kubectl get svc            NAME                TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)       AGEkubernetes          ClusterIP   10.66.77.1     &lt;none&gt;        443/TCP       6dnginx-service       ClusterIP   10.66.77.225   &lt;none&gt;        80/TCP        5h\nelasticsearch fluentd kibana官方提供了日志监控方案，资源定义文件在源码的 kubernetes&#x2F;cluster&#x2F;addons&#x2F;fluentd-elasticsearch 目录，获取YAML文件可以创建和运行EFK。\n依赖镜像：( elasticsearch 和 kibana 非常大…)\n\nk8s.gcr.io&#x2F;elasticsearch:v5.6.4\nalpine:3.6\nk8s.gcr.io&#x2F;fluentd-elasticsearch:v2.0.4\ndocker.elastic.co&#x2F;kibana&#x2F;kibana:5.6.4\n\nelasticsearch 是一个搜索引擎和日志存储的数据库flunetd 会将节点中保存的其他 pod 输出的日志流收集到 elasticsearch 中kibana 展示日志，提供人性化搜索界面和图表等功能，需要访问可以改 kibana-service.yaml 映射 NodePort 。\n从源码中获取定义资源的 YAML 文件，创建前注释一下 kibana-deployment.yaml 中 env 的 SERVER_BASEPATH 变量和 value 保存，开始创建。kibana 启动有点久，如果 pod 日志没异常的情况，需要耐心等几分钟。\n# ls -ltotal 36-rw-r--r-- 1 root root   382 Jun 13 11:34 es-service.yaml-rw-r--r-- 1 root root  2820 Jun 13 11:34 es-statefulset.yaml-rw-r--r-- 1 root root 15648 Jun 13 11:34 fluentd-es-configmap.yaml-rw-r--r-- 1 root root  2774 Jun 13 11:34 fluentd-es-ds.yaml-rw-r--r-- 1 root root  1186 Jun 13 11:34 kibana-deployment.yaml-rw-r--r-- 1 root root   354 Jun 13 11:34 kibana-service.yaml# kubectl create -f .# 查看部分资源情况# kubectl get statefulset,pod,daemonset -n kube-system  NAME                                     DESIRED   CURRENT   AGEstatefulset.apps/elasticsearch-logging   2         2         11mNAME                                        READY     STATUS              RESTARTS   AGEpod/elasticsearch-logging-0                 1/1       Running             0          6mpod/elasticsearch-logging-1                 1/1       Unknown             0          6mpod/kibana-logging-bc776986-7vtf7           1/1       Unknown             0          11mpod/kibana-logging-bc776986-cm69s           0/1       ContainerCreating   0          34spod/kube-dns-659bc9899c-ghj2n               0/3       ContainerCreating   0          20spod/kube-dns-659bc9899c-lm4pd               3/3       Running             0          1dpod/kube-dns-659bc9899c-r655f               3/3       Unknown             0          1dpod/kube-dns-autoscaler-79b4b844b9-6v856    1/1       Running             0          1dpod/kubernetes-dashboard-5c469b58b8-pf7cg   0/1       ContainerCreating   0          16spod/kubernetes-dashboard-5c469b58b8-pkttx   1/1       Unknown             2          5dNAME                                     DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR                              AGEdaemonset.extensions/fluentd-es-v2.0.4   0         0         0         0            0           beta.kubernetes.io/fluentd-ds-ready=true   11m\n\n1. elasticsearch statefulset启动和删除可能的错误若是启动 elasticsearch-logging 碰到如下错误，请为 apiserver 和所有 kubelet 添加启动参数 --allow-privileged （默认为 flase ），重载配置文件后重新启动\n# kubectl describe statefulset -n kube-system   ...  Type     Reason            Age               From                    Message  ----     ------            ----              ----                    -------  Warning  FailedCreate      1m (x24 over 6m)  statefulset-controller  create Pod elasticsearch-logging-0 in StatefulSet elasticsearch-logging failed error: Pod &quot;elasticsearch-logging-0&quot; is invalid: spec.initContainers[0].securityContext.privileged: Forbidden: disallowed by cluster policy  # 重启后启动成功# kubectl describe statefulset -n kube-system     Type     Reason            Age               From                    Message  ----     ------            ----              ----                    -------  Warning  FailedCreate      1m (x24 over 6m)  statefulset-controller  create Pod elasticsearch-logging-0 in StatefulSet elasticsearch-logging failed error: Pod &quot;elasticsearch-logging-0&quot; is invalid: spec.initContainers[0].securityContext.privileged: Forbidden: disallowed by cluster policy  Normal   SuccessfulCreate  49s               statefulset-controller  create Pod elasticsearch-logging-0 in StatefulSet elasticsearch-logging successful  Normal   SuccessfulCreate  42s               statefulset-controller  create Pod elasticsearch-logging-1 in StatefulSet elasticsearch-logging successful\n\n另外在删除资源时我面临一个问题，其他的资源都删除了，但是 elasticsearch-logging 总是不能删除成功，即使是强制 --force ，提示我超时了\n# kubectl delete -f .# 还存在elasticsearch-logging这个statefulset# kubectl get -f .NAME                    DESIRED   CURRENT   AGEelasticsearch-logging   0         2         2hError from server (NotFound): services &quot;elasticsearch-logging&quot; not foundError from server (NotFound): serviceaccounts &quot;elasticsearch-logging&quot; not foundError from server (NotFound): clusterroles.rbac.authorization.k8s.io &quot;elasticsearch-logging&quot; not foundError from server (NotFound): clusterrolebindings.rbac.authorization.k8s.io &quot;elasticsearch-logging&quot; not foundError from server (NotFound): configmaps &quot;fluentd-es-config-v0.1.4&quot; not foundError from server (NotFound): serviceaccounts &quot;fluentd-es&quot; not foundError from server (NotFound): clusterroles.rbac.authorization.k8s.io &quot;fluentd-es&quot; not foundError from server (NotFound): clusterrolebindings.rbac.authorization.k8s.io &quot;fluentd-es&quot; not foundError from server (NotFound): daemonsets.apps &quot;fluentd-es-v2.0.4&quot; not foundError from server (NotFound): deployments.apps &quot;kibana-logging&quot; not foundError from server (NotFound): services &quot;kibana-logging&quot; not found# kubectl delete statefulset elasticsearch-logging -n kube-system --forcetimed out waiting for &quot;elasticsearch-logging&quot; to be synced\n\n后面在google找到一个方法，删除失败的资源，指定 --cascade=false ，解决了我的问题。若删除后还可以看到被删除资源定义的pod，可以指定 --grace-period=0 删除它。\n# kubectl delete -f es-statefulset.yaml --cascade=false# 如果删除了定义elastic的stateful，但是get pod发现还有stateful定义的pod没删除，可以执行下面命令强制删除# kubectl --namespace=kube-system delete pods elasticsearch-logging-0 --grace-period=0 --force # kubectl --namespace=kube-system delete pods elasticsearch-logging-1 --grace-period=0 --force \n\n2. fluentd daemonset启动问题另外一个 fluentd 问题，get 发现 daemonset fluentd-es-v2.0.4 运行0个，这里需要为所有 node 添加一个label，如下\n# kubectl get daemonset fluentd-es-v2.0.4 -n kube-system NAME                DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR                              AGEfluentd-es-v2.0.4   0         0         0         0            0           beta.kubernetes.io/fluentd-ds-ready=true   32s# 为所有node添加一个label，有此label的node才会运行fluentd# kubectl label node debian-70 beta.kubernetes.io/fluentd-ds-ready=true# kubectl label node sl-80 beta.kubernetes.io/fluentd-ds-ready=true# 再次get，已经有在运行了# kubectl get daemonset fluentd-es-v2.0.4 -n kube-system NAME                DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR                              AGEfluentd-es-v2.0.4   2         2         2         2            2           beta.kubernetes.io/fluentd-ds-ready=true   4m\n\n一次偶然，公司测试机房断电了（我猜测与这个有关系），再启动集群的时候有一台节点的 fluentd 无法启动，如下体现\n# pod状态一直是CrashLoopBackOff，describe查看到的信息也有限，重启也不能正常# kubectl get -n kube-system podsNAME                                    READY     STATUS             RESTARTS   AGEelasticsearch-logging-0                 1/1       Running            3          14delasticsearch-logging-1                 1/1       Running            3          14dfluentd-es-v2.0.4-4lthw                 1/1       Running            0          16hfluentd-es-v2.0.4-lvmj7                 0/1       CrashLoopBackOff   4          16hheapster-69b5d4974d-4dzm8               1/1       Running            3          14dkibana-logging-799d8b46db-rn6fq         1/1       Running            3          14dkube-dns-659bc9899c-ghj2n               3/3       Running            9          15dkube-dns-659bc9899c-lm4pd               3/3       Running            12         16dkube-dns-autoscaler-79b4b844b9-6v856    1/1       Running            4          16dkubernetes-dashboard-7d5dcdb6d9-f4j6n   1/1       Running            3          14dmonitoring-grafana-69df66f668-gg9kl     1/1       Running            3          14dmonitoring-influxdb-78d4c6f5b6-2phht    1/1       Running            3          14d# 另外看一下pod日志，这个帮助很大# kubectl logs -n kube-system fluentd-es-v2.0.4-lvmj7 2018-06-29 01:21:19 +0000 [warn]: parameter &#x27;time_format&#x27; in &lt;source&gt;  @id fluentd-containers.log  @type tail  path &quot;/var/log/containers/*.log&quot;  pos_file &quot;/var/log/es-containers.log.pos&quot;  time_format %Y-%m-%dT%H:%M:%S.%NZ  tag &quot;raw.kubernetes.*&quot;  read_from_head true  &lt;parse&gt;    @type &quot;multi_format&quot;    &lt;pattern&gt;      format json      time_key &quot;time&quot;      time_format &quot;%Y-%m-%dT%H:%M:%S.%NZ&quot;      time_type string    &lt;/pattern&gt;    &lt;pattern&gt;      format /^(?&lt;time&gt;.+) (?&lt;stream&gt;stdout|stderr) [^ ]* (?&lt;log&gt;.*)$/      time_format &quot;%Y-%m-%dT%H:%M:%S.%N%:z&quot;      expression &quot;^(?&lt;time&gt;.+) (?&lt;stream&gt;stdout|stderr) [^ ]* (?&lt;log&gt;.*)$&quot;      ignorecase false      multiline false    &lt;/pattern&gt;  &lt;/parse&gt;&lt;/source&gt; is not used.  # 下面开始error，而另外正常的pod下面是传输到elastic的info日志了2018-06-29 01:21:19 +0000 [error]: unexpected error error_class=TypeError error=&quot;no implicit conversion of Symbol into Integer&quot;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/buffer/file_chunk.rb:219:in `[]&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/buffer/file_chunk.rb:219:in `restore_metadata&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/buffer/file_chunk.rb:322:in `load_existing_staged_chunk&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/buffer/file_chunk.rb:51:in `initialize&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/buf_file.rb:144:in `new&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/buf_file.rb:144:in `block in resume&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/buf_file.rb:133:in `glob&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/buf_file.rb:133:in `resume&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/buffer.rb:171:in `start&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/buf_file.rb:120:in `start&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/plugin/output.rb:415:in `start&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/root_agent.rb:165:in `block in start&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/root_agent.rb:154:in `block (2 levels) in lifecycle&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/root_agent.rb:153:in `each&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/root_agent.rb:153:in `block in lifecycle&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/root_agent.rb:140:in `each&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/root_agent.rb:140:in `lifecycle&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/root_agent.rb:164:in `start&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/engine.rb:274:in `start&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/engine.rb:219:in `run&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/supervisor.rb:774:in `run_engine&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/supervisor.rb:523:in `block in run_worker&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/supervisor.rb:699:in `main_process&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/supervisor.rb:518:in `run_worker&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/lib/fluent/command/fluentd.rb:316:in `&lt;top (required)&gt;&#x27;  2018-06-29 01:21:19 +0000 [error]: /usr/lib/ruby/2.3.0/rubygems/core_ext/kernel_require.rb:55:in `require&#x27;  2018-06-29 01:21:19 +0000 [error]: /usr/lib/ruby/2.3.0/rubygems/core_ext/kernel_require.rb:55:in `require&#x27;  2018-06-29 01:21:19 +0000 [error]: /var/lib/gems/2.3.0/gems/fluentd-1.1.0/bin/fluentd:8:in `&lt;top (required)&gt;&#x27;  2018-06-29 01:21:19 +0000 [error]: /usr/local/bin/fluentd:22:in `load&#x27;  2018-06-29 01:21:19 +0000 [error]: /usr/local/bin/fluentd:22:in `&lt;main&gt;&#x27;2018-06-29 01:21:19 +0000 [error]: unexpected error error_class=TypeError error=&quot;no implicit conversion of Symbol into Integer&quot;  2018-06-29 01:21:19 +0000 [error]: suppressed same stacktrace\n\n在 google 找到一个 GitHub 的 fluentd&#x2F;issues&#x2F;#1760，发现和缓存的元数据损坏有关系。不同的是这里 2.0.4 版本 buffer 存放位置变了，映射在宿主机的 /var/log/fluentd-buffers/kubernetes.system.buffer 目录。我将 *.meta 删除后节点的 fluentd 启动正常（可以删除 pod 会自动创建，或者等它重启）。\n# cd /var/log/fluentd-buffers/kubernetes.system.buffer/# lsbuffer.b56efd1a03768b2f7eabddf200cf50b79.log       buffer.b56efd1a03ad20d927856cabfb9e0b1d7.log.metabuffer.b56efd1a03768b2f7eabddf200cf50b79.log.meta  buffer.b56efd1a1f72114c77ad018dec6591873.logbuffer.b56efd1a03ad20d927856cabfb9e0b1d7.log       buffer.b56efd1a1f72114c77ad018dec6591873.log.meta# rm -rf *.meta\n\n\n关于监控heapster influxdb grafana官方提供了监控方案heapster\n获取 YAML 文件在项目的 deploy&#x2F;kube-config&#x2F;influxdb 路径下，获取后直接可以创建。镜像请先获取，过滤文件的image字段，拉取这些再创建比较妥。\n启动了这些资源后，若 heapster 日志无异常，dashboard 过一会就有关于容器组 CPU 内存等监控状态。可以修改 grafana 的 service 映射成 NodePort，配置图形展示（admin&#x2F;admin）。\n关于自动水平扩展HPA参考官方 https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/\n","categories":["kubernetes"]},{"title":"mysql router 中间件体验","url":"/2018/03/02/mysql-router-%E4%B8%AD%E9%97%B4%E4%BB%B6%E4%BD%93%E9%AA%8C/","content":"mysql router 中间件mysql router 通过两个不同的端口做读写分离（这点不是很方便，仅体验下，生产没需求），不对执行的 SQL 做判断是读写。另外mysql router在master 挂了的时候不会将 slave 节点的 master 指向新的 slave，和 mysqlfailover 有很大区别（当然它们是两个用途的工具）。可以自动剔除故障节点，并且恢复后自动上线。\n环境用已有的 mysql 集群拓扑，带级联的复制\n\n\nshell&gt; mysqlrplshow --master=root:111111@&quot;192.168.122.66:3307&quot; \\             --discover-slaves-login=root:111111 -r -v         WARNING: Using a password on the command line interface can be insecure.# master on 192.168.122.66: ... connected.# Finding slaves for master: 192.168.122.66:3307# master on 192.168.122.70: ... connected.# Finding slaves for master: 192.168.122.70:3307# master on 192.168.122.70: ... connected.# Finding slaves for master: 192.168.122.70:3306# master on 192.168.122.80: ... connected.# Finding slaves for master: 192.168.122.80:3307# Replication Topology Graph192.168.122.66:3307 (MASTER)                                   【hostname：centos-66】   |   +--- 192.168.122.70:3307 [IO: Yes, SQL: Yes] - (SLAVE + MASTER)   |   |   |   +--- 192.168.122.70:3306 [IO: Yes, SQL: Yes] - (SLAVE)  【hostname：debian-70】   |   +--- 192.168.122.80:3307 [IO: Yes, SQL: Yes] - (SLAVE)      【hostname：sl-80】\n\n\n安装\n前往官网下载当前版本MySQL Router 2.1.5  \n\n\n安装Linux二进制包 shell&gt; cd /usr/srcshell&gt; wget https://dev.mysql.com/get/Downloads/MySQL-Router/mysql-router-2.1.5-linux-glibc2.12-x86-64bit.tar.gzshell&gt; tar xf mysql-router-2.1.5-linux-glibc2.12-x86-64bit.tar.gz  -C /usr/localshell&gt; cd /usr/localshell&gt; ln -sv /usr/local/mysql-router-2.1.5-linux-glibc2.12-x86-64bit/ mysql-routershell&gt; cd mysql-routershell&gt; mkdir etcshell&gt; cp share/doc/mysqlrouter/sample_mysqlrouter.conf etc/mysqlrouter.conf\n\n配置\n修改配置文件，选项文档\n编辑/usr/local/mysql-router/etc/mysqlrouter.conf   # MySQL Router sample configuration## The following is a sample configuration file which shows# most of the plugins available and most of their options.## The paths used are defaults and should be adapted based# on how MySQL Router was installed, for example, using the# CMake option CMAKE_INSTALL_PREFIX## The logging_folder is kept empty so message go to the# console.#[DEFAULT]logging_folder = /usr/local/mysql-router/logs  # 日志目录，需要创建plugin_folder = /usr/local/mysql-router/lib/mysqlrouter  # 插件目录config_folder = /usr/local/mysql-router/etc    # 配置文件目录runtime_folder = /var/rundata_folder = /usr/local/mysql-router/data     # 程序数据文件目录#keyring_path = /var/lib/keyring-data          # 认证密钥地址，不用它#master_key_path = /var/lib/keyring-key[logger]level = INFO                                   # 日志级别[routing:read_write]                           # 添加一个routing段，read_write为自定义名称# To be more transparent, use MySQL Server port 3306bind_address= 192.168.122.66                   # 此配置段监听地址bind_port = 7001                               # 监听端口mode = read-write                              # 读写模式(只有读写和只读两种)destinations = 192.168.122.66:3307, 192.168.122.70:3307  # 第一个为当前的master，逗号分隔的为备选master，若当前master不可用将读写请求发送到备选master[routing:read_only]                            # 定义一个只读配置段bind_address= 192.168.122.66                   # 读请求监听地址bind_port = 7002connect_timeout = 3max_connections = 1024                         # 最大连接destinations = 192.168.122.70:3307,192.168.122.70:3306,192.168.122.80:3307mode = read-only                               # 只读模式，destinations的地址会轮询得到请求，会自动下线关闭的服务器# If no plugin is configured which starts a service, keepalive# will make sure MySQL Router will not immediately exit. It is# safe to remove once Router is configured.[keepalive]interval = 60\n\n测试\n 启动mysql router：  shell&gt; cd /usr/local/mysql-routershell&gt; nohup ./bin/mysqlrouter -c etc/mysqlrouter.conf &amp;\n\n\n写入是一直由一个节点承担，除非down掉才换\n # 测试写操作端口的[root@centos-66 ~]# mysql -uroot -p&#x27;111111&#x27; -P7001 -h192.168.122.66 -e &quot;show variables like &#x27;hostname&#x27;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+---------------+-----------+| Variable_name | Value     |+---------------+-----------+| hostname      | centos-66 |+---------------+-----------+# 测试读操作的端口，是否轮询转发[root@centos-66 ~]# mysql -uroot -p&#x27;111111&#x27; -P7002 -h192.168.122.66 -e &quot;show variables like &#x27;hostname&#x27;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+---------------+-----------+| Variable_name | Value     |+---------------+-----------+| hostname      | debian-70 |   -- 这里因为看host没端口，这台机有3306和3307两个实例在的+---------------+-----------+[root@centos-66 ~]# mysql -uroot -p&#x27;111111&#x27; -P7002 -h192.168.122.66 -e &quot;show variables like &#x27;hostname&#x27;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+---------------+-----------+| Variable_name | Value     |+---------------+-----------+| hostname      | debian-70 |   -- 另外一个端口的+---------------+-----------+[root@centos-66 ~]# mysql -uroot -p&#x27;111111&#x27; -P7002 -h192.168.122.66 -e &quot;show variables like &#x27;hostname&#x27;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+---------------+-------+| Variable_name | Value |+---------------+-------+| hostname      | sl-80 |       -- sl-80轮询+---------------+-------+\n\n此时停止sl-80的slave实例，测试是否自动剔除故障节点\n [root@sl-80 ~]# mysqld_multi stop 3307[root@sl-80 ~]# mysqld_multi report   Reporting MySQL serversMySQL server from group: mysqld3307 is not running# 已经剔除了sl-80的读节点[root@centos-66 ~]# mysql -uroot -p&#x27;111111&#x27; -P7002 -h192.168.122.66 -e &quot;show variables like &#x27;hostname&#x27;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+---------------+-----------+| Variable_name | Value     |+---------------+-----------+| hostname      | debian-70 |+---------------+-----------+[root@centos-66 ~]# mysql -uroot -p&#x27;111111&#x27; -P7002 -h192.168.122.66 -e &quot;show variables like &#x27;hostname&#x27;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+---------------+-----------+| Variable_name | Value     |+---------------+-----------+| hostname      | debian-70 |+---------------+-----------+[root@centos-66 ~]# mysql -uroot -p&#x27;111111&#x27; -P7002 -h192.168.122.66 -e &quot;show variables like &#x27;hostname&#x27;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+---------------+-----------+| Variable_name | Value     |+---------------+-----------+| hostname      | debian-70 |+---------------+-----------+[root@centos-66 ~]# mysql -uroot -p&#x27;111111&#x27; -P7002 -h192.168.122.66 -e &quot;show variables like &#x27;hostname&#x27;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+---------------+-----------+| Variable_name | Value     |+---------------+-----------+| hostname      | debian-70 |+---------------+-----------+\n\n\n测试自动上线\n # 启动读节点[root@sl-80 ~]# mysqld_multi start 3307[root@sl-80 ~]# mysqld_multi reportReporting MySQL serversMySQL server from group: mysqld3307 is running# 已经自动上线[root@centos-66 ~]# mysql -uroot -p&#x27;111111&#x27; -P7002 -h192.168.122.66 -e &quot;show variables like &#x27;hostname&#x27;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+---------------+-----------+| Variable_name | Value     |+---------------+-----------+| hostname      | debian-70 |+---------------+-----------+[root@centos-66 ~]# mysql -uroot -p&#x27;111111&#x27; -P7002 -h192.168.122.66 -e &quot;show variables like &#x27;hostname&#x27;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+---------------+-------+| Variable_name | Value |+---------------+-------+| hostname      | sl-80 |+---------------+-------+[root@centos-66 ~]# mysql -uroot -p&#x27;111111&#x27; -P7002 -h192.168.122.66 -e &quot;show variables like &#x27;hostname&#x27;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+---------------+-----------+| Variable_name | Value     |+---------------+-----------+| hostname      | debian-70 |+---------------+-----------+[root@centos-66 ~]# mysql -uroot -p&#x27;111111&#x27; -P7002 -h192.168.122.66 -e &quot;show variables like &#x27;hostname&#x27;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+---------------+-----------+| Variable_name | Value     |+---------------+-----------+| hostname      | debian-70 |+---------------+-----------+\n\n","categories":["MySQL"],"tags":["mysql-router"]},{"title":"mysql MHA 记录","url":"/2018/03/13/mysql-MHA-%E8%AE%B0%E5%BD%95/","content":"MySQL MHA配置记录MHA介绍MHA是一套 MySQL 高可用管理软件 ，除了检测 Master 宕机后，提升候选 Slave 为 New Master 之外，还会自动让其他 Slave 与 New Master 建立复制关系。切换通过浮动 ip 的方式，浮动 ip 将绑定在任何时期的主节点上（若切换则 ip 也漂移），通过浮动 ip 提供服务。\nMHA Manager可以 单独部署在一台独立的机器上，并管理多个 master-slave 集群，只需要指定不通的配置文件。  \n\n\n-- +------------------------------------+ (app1.conf) +----------+|                           VIP      |   heartbeat |    MHA   ||  +----------+    rpl   +--------+  &lt;-------------+  Manager ||  | Slave(M) +----------&gt; Master |  | rpl manager +----------+|  +----------+          +-^----^-+  |                  ||                  rpl     |    |    |                  ||       +------------------+    |rpl |                  ||       |                       |    |      (app2.conf) ||  +---------+           +-------+-+ |             +----V----+|  | Slave 1 |  ......   | Slave N | |             |  Other  ||  +---------+           +---------+ |             |  MySQL  ||                                    |             |   rpl   ||             MySQL Replication      |             +---------++------------------------------------+\n\n集群节点描述\nMHA Manager： MHA的管理节点，负责检测MySQL的状态，以及管理MySQL的复制关系\nMaster： MySQL对外提供的服务（ 通过 VIP ）的主节点\n**Slave(M)**： MySQL候选主节点，本质上为一个Slave节点，只是在Master宕机后，会被提升为 New Master \nSlave N： MySQL从机节点，对外可以提供只读服务\n\n故障检测过程&emsp;&emsp;当Master宕机后，MHA Manager会让 Slave(M) 提升为 New Master，然后修改 Slave(N) 的复制关系（CHANGE MASTER），指向 New Master\n\nMHA Manager检测Master是否宕机，不会简单的只检查自身与Master之间的直连状态\nMHA Manager会 透过其他Slave节点去检测Master，进行二次探测，最大限度的避免脑裂的发生\n若检测到Master宕机了， MHA Manager会 透过Slave(M) 检测是否为宕机；如果检测后仍为宕机状态，会继续 透过Slave1…SlaveN进行探测，只有在透过所有节点检测到Master宕机了，才真的认为Master宕机了。\n\n\n以上操作步骤的前提是，**MHA Manager 到所有 MySQL 节点的 SSH 免密码登录要打通**。最后将提供对外服务的 VIP 漂移到 New Master 上，继续对外提供服务。\n\nMHA Failover过程\n从宕机崩溃的 Master 节点 保存二进制日志事件（binlog events）\n此种情况为 MySQL 挂了，但是服务器没挂 ，还是可以通过 SSH 连接过去\n如果服务器彻底宕机了，该步骤略过\n\n\n识别 含有最新的更新 的 Slave 节点\n应用差异的中继日志 （ relay-log）到其他的 Slave\n应用从 Master 保存的二进制日志事件（binlog events）\n如果之前 Master 彻底宕机了，就没有保存的 binlog，该步骤略过\n\n\n提升一个 Slave 为新的 Master（ New）  \n使其他的 Slave 连接新的 Master（ New） 进行复制\n\n注意&emsp;&emsp;从上面的步骤看， MHA本身无法完全保证数据不会丢  \n\nMySQL 5.7 之前数据不丢的前提是 Master 服务器还可以被 MHA Manager 进行 SSH 连接 ，通过 应用保存的binlog的方式来保证。  \nMySQL 5.7 之后通过无损复制，仅仅是减少了丢数据的可能性，假如此时的状态为切成异步的状态 ，那就和之前一样了（可以设置超时的时间很大）。  \n当 Master 恢复的时候，最后一部分数据是否需要 Flashback， MHA 不负责这个操作，需要人工确认。\n\n准备工作\n会用到 mysql utilities 中的工具，请安装。  \n\n测试集群拓扑图&emsp;&emsp;注意： slave 中的 /etc/my.cnf 中需要配置 report-host=slave ip_addr ，否则命令执行失败。  \nshell&gt; mysqlrplshow --master=root:111111@&quot;192.168.122.66:3307&quot; --discover-slaves-login=root:111111 -r -vWARNING: Using a password on the command line interface can be insecure.# master on 192.168.122.66: ... connected.# Finding slaves for master: 192.168.122.66:3307# master on 192.168.122.70: ... connected.# Finding slaves for master: 192.168.122.70:3307WARNING: Cannot connect to some slaves: - 192.168.122.70:3306: Can&#x27;t connect to MySQL server on &#x27;192.168.122.70:3306&#x27; (111 Connection refused)# master on 192.168.122.80: ... connected.# Finding slaves for master: 192.168.122.80:3307# Replication Topology Graph192.168.122.66:3307 (MASTER)  [由于机器数量问题，此节点会跑MHA manager和node]   |   +--- 192.168.122.70:3307 [IO: Yes, SQL: Yes] - (SLAVE)  [做MHA node]   |   +--- 192.168.122.80:3307 [IO: Yes, SQL: Yes] - (SLAVE)  [做MHA node]\n\n&emsp;&emsp;另外如果使用 mysqlreplicate 创建复制关系，则他会帮你在你的 Master上创建 rpl@192.168.122.70 ， rpl@192.168.122.80 用户，需要注意这个 rpl 用户需要赋予 replication和slave 权限，可以手动确认一下。\nMHA软件角色分布&emsp;&emsp;上述架构，MHA 中 Manager 节点最好不要在集群中(这里空闲的测试机有限，so..，需要打通 Manager 节点到所有 mysql 节点的 ssh 密钥连接，mysql 节点相互也需要打通，方便无密码登陆。操作不演示。 \n安装MHA软件\n示例基于1主2从，需要先搭建好，打通 ssh 认证。MHA Node 在所有节点上都要安装  \n\n获取MHA源码&emsp;&emsp;获取MHA源码，github中tar包最新的貌似是 0.56 下载地址，rpm 包有 0.57 下载地址，需要源码版可以往百度云找，已收藏…PS：有梯子可以打开 此处下载0.57\n如上准备主从，mysqlreplicate搭建，不演示。安装依赖包安装 perl 依赖包\n-- redhat系安装依赖，若装完执行makefile报错可以尝试重新登陆下yum install perl-DBD-MySQL perl-Config-Tiny perl-Log-Dispatch perl-Parallel-ForkManager perl-Time-HiRes \\    perl-ExtUtils-CBuilder perl-ExtUtils-MakeMaker perl-ExtUtils-Embed cpan perl-CPAN   -- debian系安装依赖apt install libdbd-mysql-perl libconfig-tiny-perl liblog-dispatch-perl libparallel-forkmanager-perl   \n\n安装MHA Managermysql master 节点安装 mha node 和 manager，先解压好（这里环境的 MHA Manager 放在了 mysql 节点中）\nshell&gt; cd mha4mysql-manager-0.57shell&gt; perl Makefile.PLshell&gt; make &amp;&amp; make installshell&gt; ll bin/total 40-rwxr-xr-x 1 git mysql 1995 May 31  2015 masterha_check_repl         # 检查MySQL复制状态-rwxr-xr-x 1 git mysql 1779 May 31  2015 masterha_check_ssh          # 检查MHA的SSH状态-rwxr-xr-x 1 git mysql 1865 May 31  2015 masterha_check_status       # 检查当前MHA的状态-rwxr-xr-x 1 git mysql 3201 May 31  2015 masterha_conf_host          # 添加或删除配置的server信息-rwxr-xr-x 1 git mysql 2517 May 31  2015 masterha_manager            # 启动MHA-rwxr-xr-x 1 git mysql 2165 May 31  2015 masterha_master_monitor     # 检测master是否宕机-rwxr-xr-x 1 git mysql 2373 May 31  2015 masterha_master_switch      # 控制故障转移（手动或者自动）-rwxr-xr-x 1 git mysql 5171 May 31  2015 masterha_secondary_check    # 通过slave检测master是否宕机（二次探测）-rwxr-xr-x 1 git mysql 1739 May 31  2015 masterha_stop               # MHA停止shell&gt; mkdir -p /etc/masterha          # MHA配置文件存放目录shell&gt; mkdir -p /var/log/masterha/app1 # 建议和MHA配置文件保持一致，若管理多套mysql集群可以再新建appxxxshell&gt; mkdir -p /usr/local/mha/        # 准备存放MHA可执行程序和libshell&gt; cp -rf bin/ lib/ /usr/local/mha/shell&gt; cd ../mha4mysql-node-0.57/      # manager也需要安装node工具shell&gt; perl Makefile.PLshell&gt; make &amp;&amp; make installshell&gt; cp bin/* /usr/local/mha/bin/shell&gt; cp lib/* /usr/local/mha/lib/ \n\n安装MHA node为所有节点安装 MHA node，这里两个 slave 只安装 mha node\nshell&gt; yum install perl-DBI perl-DBD-MySQL -y   # node节点依赖较少shell&gt; cd mha4mysql-node-0.57/shell&gt; perl Makefile.PLshell&gt; make &amp;&amp; make installshell&gt; ll bin/total 44-rwxr-xr-x 1 1001 mysql 16381 May 31  2015 apply_diff_relay_logs  # 识别差异的中继日志事件并将其差异的事件应用于其他的slave-rwxr-xr-x 1 1001 mysql  4807 May 31  2015 filter_mysqlbinlog     # 去除不必要的ROLLBACK事件（脚本中指出该脚本已经过时，应该是被废弃了）-rwxr-xr-x 1 1001 mysql  8261 May 31  2015 purge_relay_logs       # 清除中继日志（不会阻塞SQL线程）-rwxr-xr-x 1 1001 mysql  7525 May 31  2015 save_binary_logs       # 保存和复制master的二进制日志\n\nMHA自带配置文件 MHA-manager 编译完在 samples 目录下有配置文件模板和 failover 脚本供参考，这里用另外写的   \nshell&gt; tree samples/samples/├── conf│   ├── app1.cnf│   └── masterha_default.cnf└── scripts    ├── master_ip_failover    ├── master_ip_online_change    ├── power_manager    └── send_report\n\nMHA配置文件和脚本\n配置文件统一放在 &#x2F;etc&#x2F;masterha&#x2F; 中，针对当前这个 MySQL 复制组，我们定义为 app1.conf ，如果还有 其他 MySQL 复制组 ，可以 继续定义 app2.conf 、app3.conf等等  \n\n示例配置文件细述/etc/masterha/app1.conf 定义一个 mysql 集群如下内容\n## MHA Manager 端 /etc/masterha/app1.conf#[server default]# 这两个参数需要根据不同的集群进行修改manager_workdir=/var/log/masterha/app1manager_log=/var/log/masterha/app1/manager.log# 按照master服务器存放binlog的实际路径进行修改，主要为了让MHA拉取binlogmaster_binlog_dir=/data/mysql_data/5.7.11/# 设置自动failover的脚本，下面会给出master_ip_failover_script= /usr/local/mha/bin/master_ip_failover# 设置手动切换时候的脚本 （供(masterha_master_switch使用）master_ip_online_change_script= /usr/local/mha/bin/master_ip_online_changelog_level=debug# 监控的用户user=root# 监控用户的密码password=111111# 监控主库的时间间隔，默认是3秒，尝试三次没有回应的时候自动进行failoverping_interval=3# 检测方式是insert， MHA-0.56开始支持insert# 会在Master中生成一个 infra 数据库ping_type=INSERT# 设置远端mysql在发生切换时binlog的保存位置remote_workdir=/tmp# 复制用的密码repl_password=rpl# 复制的用户repl_user=rpl# 告警脚本，可自行修改，这里没有使用#report_script=/usr/local/mha/bin/send_report# 通过从机进行二次探测的脚本， IP地址按照实际的情况进行修改secondary_check_script=/usr/local/mha/bin/masterha_secondary_check -s 192.168.122.70 -s 192.168.122.80 --user=root --master_host=192.168.122.66 --master_port=3307# 设置故障发生后关闭故障主机的脚本（主要作用是关闭主机防止发生脑裂,这里没有使用，类似Fence功能）#shutdown_script=&quot;/usr/local/mha/bin/power_manager --command=stopssh2 --host=test-1 --ssh_user=root&quot;# 定义ssh的用户ssh_user=root[server1]# 这个hostname也可以配置成IP地址，同 ip 参数一样# 如果这里写名字，需要DNS配合，或者使用 /etc/hostshostname=masterip=192.168.122.66port=3307# candidate_master参数的意思为：设置为候选Master，如果发生主从切换，该主机会被提升为Master，即使这个服务器上的数据不是最新的（会用relay-log补全）candidate_master=1[server2]hostname=slave1ip=192.168.122.70port=3307candidate_master=1# check_repl_delay参数的意思为：默认情况下如果一个slave落后master 100M的relay logs的话， MHA将不会选择该slave作为一个新的master；# 因为对于这个slave的恢复需要花费很长时间;# 通过设置check_repl_delay=0,MHA触发切换在选择一个新的master的时候将会忽略复制延时;# 这个参数对于设置了candidate_master=1的主机非常有用，因为这个候选主在切换的过程中一定是新的mastercheck_repl_delay=0[server3]hostname=slave2ip=192.168.122.80port=3307# no_master 表示该主机不会被提升为Masterno_master=1\n\nfailover脚本failover 脚本，新建 /usr/local/mha/bin/master_ip_failover 文件，记得加执行权限\n#!/usr/bin/env perluse strict;use warnings FATAL =&gt; &#x27;all&#x27;;use Getopt::Long;my (    $command, $ssh_user, $orig_master_host, $orig_master_ip,    $orig_master_port, $new_master_host, $new_master_ip, $new_master_port);# 注意修改系统对应的接口名称（CentOS比较变态，若多节点可以升级成master，记得接口名称需要一样，比如都为eth0...）my $vip = &#x27;192.168.122.99/24&#x27;;my $eth = &#x27;eth0&#x27;;my $key = &#x27;88&#x27;;my $ssh_start_vip = &quot;/sbin/ifconfig $eth:$key $vip&quot;;my $ssh_stop_vip = &quot;/sbin/ifconfig $eth:$key down&quot;;GetOptions(    &#x27;command=s&#x27; =&gt; \\$command,    &#x27;ssh_user=s&#x27; =&gt; \\$ssh_user,    &#x27;orig_master_host=s&#x27; =&gt; \\$orig_master_host,    &#x27;orig_master_ip=s&#x27; =&gt; \\$orig_master_ip,    &#x27;orig_master_port=i&#x27; =&gt; \\$orig_master_port,    &#x27;new_master_host=s&#x27; =&gt; \\$new_master_host,    &#x27;new_master_ip=s&#x27; =&gt; \\$new_master_ip,    &#x27;new_master_port=i&#x27; =&gt; \\$new_master_port,);exit &amp;main();sub main &#123;    print &quot;\\n\\nIN SCRIPT TEST====$ssh_stop_vip==$ssh_start_vip===\\n\\n&quot;;        if ( $command eq &quot;stop&quot; || $command eq &quot;stopssh&quot; ) &#123;            my $exit_code = 1;        eval &#123;            print &quot;Disabling the VIP on old master: $orig_master_host \\n&quot;;            &amp;stop_vip();            $exit_code = 0;        &#125;;        if ($@) &#123;            warn &quot;Got Error: $@\\n&quot;;            exit $exit_code;        &#125;        exit $exit_code;    &#125;    elsif ( $command eq &quot;start&quot; ) &#123;        my $exit_code = 10;        eval &#123;            print &quot;Enabling the VIP - $vip on the new master - $new_master_host \\n&quot;;            &amp;start_vip();            $exit_code = 0;        &#125;;        if ($@) &#123;            warn $@;            exit $exit_code;        &#125;        exit $exit_code;    &#125;    elsif ( $command eq &quot;status&quot; ) &#123;        print &quot;Checking the Status of the script.. OK \\n&quot;;        exit 0;    &#125;    else &#123;        &amp;usage();        exit 1;    &#125;&#125;sub start_vip() &#123;    `ssh $ssh_user\\@$new_master_host \\&quot; $ssh_start_vip \\&quot;`;&#125;sub stop_vip() &#123;    return 0 unless ($ssh_user);    `ssh $ssh_user\\@$orig_master_host \\&quot; $ssh_stop_vip \\&quot;`;&#125;sub usage &#123;    print    &quot;Usage: master_ip_failover --command=start|stop|stopssh|status --orig_master_host=host --orig_master_ip=ip --orig_master_port=port --new_master_host=host --new_master_ip=ip --new_master_port=port\\n&quot;;&#125;\n\n在线切换master的脚本创建在线切换 master 的脚本 /usr/local/mha/bin/master_ip_online_change，同样加执行权限  \n#!/usr/bin/env perluse strict;use warnings FATAL =&gt; &#x27;all&#x27;;use Getopt::Long;#my (# $command, $ssh_user, $orig_master_host, $orig_master_ip,# $orig_master_port, $new_master_host, $new_master_ip, $new_master_port#);my (    $command,              $orig_master_is_new_slave, $orig_master_host,    $orig_master_ip,       $orig_master_port,         $orig_master_user,    $orig_master_password, $orig_master_ssh_user,     $new_master_host,    $new_master_ip,        $new_master_port,          $new_master_user,    $new_master_password,  $new_master_ssh_user,);# 同样注意修改系统对应的接口名称，和对应的VIPmy $vip = &#x27;192.168.122.99/24&#x27;;my $eth = &#x27;eth0&#x27;;my $key = &#x27;88&#x27;;my $ssh_start_vip = &quot;/sbin/ifconfig $eth:$key $vip&quot;;my $ssh_stop_vip = &quot;/sbin/ifconfig $eth:$key down&quot;;my $ssh_user = &quot;root&quot;;GetOptions(    &#x27;command=s&#x27; =&gt; \\$command,    #&#x27;ssh_user=s&#x27; =&gt; \\$ssh_user,    #&#x27;orig_master_host=s&#x27; =&gt; \\$orig_master_host,    #&#x27;orig_master_ip=s&#x27; =&gt; \\$orig_master_ip,    #&#x27;orig_master_port=i&#x27; =&gt; \\$orig_master_port,    #&#x27;new_master_host=s&#x27; =&gt; \\$new_master_host,    #&#x27;new_master_ip=s&#x27; =&gt; \\$new_master_ip,    #&#x27;new_master_port=i&#x27; =&gt; \\$new_master_port,    &#x27;orig_master_is_new_slave&#x27; =&gt; \\$orig_master_is_new_slave,    &#x27;orig_master_host=s&#x27; =&gt; \\$orig_master_host,    &#x27;orig_master_ip=s&#x27; =&gt; \\$orig_master_ip,    &#x27;orig_master_port=i&#x27; =&gt; \\$orig_master_port,    &#x27;orig_master_user=s&#x27; =&gt; \\$orig_master_user,    &#x27;orig_master_password=s&#x27; =&gt; \\$orig_master_password,    &#x27;orig_master_ssh_user=s&#x27; =&gt; \\$orig_master_ssh_user,    &#x27;new_master_host=s&#x27; =&gt; \\$new_master_host,    &#x27;new_master_ip=s&#x27; =&gt; \\$new_master_ip,    &#x27;new_master_port=i&#x27; =&gt; \\$new_master_port,    &#x27;new_master_user=s&#x27; =&gt; \\$new_master_user,    &#x27;new_master_password=s&#x27; =&gt; \\$new_master_password,    &#x27;new_master_ssh_user=s&#x27; =&gt; \\$new_master_ssh_user,);exit &amp;main();sub main &#123;    print &quot;\\n\\nIN SCRIPT TEST====$ssh_stop_vip==$ssh_start_vip===\\n\\n&quot;;        if ( $command eq &quot;stop&quot; || $command eq &quot;stopssh&quot; ) &#123;        my $exit_code = 1;        eval &#123;            print &quot;Disabling the VIP on old master: $orig_master_host \\n&quot;;            &amp;stop_vip();            $exit_code = 0;        &#125;;        if ($@) &#123;            warn &quot;Got Error: $@\\n&quot;;            exit $exit_code;        &#125;        exit $exit_code;    &#125;    elsif ( $command eq &quot;start&quot; ) &#123;        my $exit_code = 10;        eval &#123;            print &quot;Enabling the VIP - $vip on the new master - $new_master_host \\n&quot;;            &amp;start_vip();            $exit_code = 0;        &#125;;        if ($@) &#123;            warn $@;            exit $exit_code;        &#125;        exit $exit_code;    &#125;    elsif ( $command eq &quot;status&quot; ) &#123;        print &quot;Checking the Status of the script.. OK \\n&quot;;        exit 0;    &#125;    else &#123;        &amp;usage();        exit 1;    &#125;&#125;sub start_vip() &#123;    `ssh $ssh_user\\@$new_master_host \\&quot; $ssh_start_vip \\&quot;`;&#125;sub stop_vip() &#123;    return 0 unless ($ssh_user);    `ssh $ssh_user\\@$orig_master_host \\&quot; $ssh_stop_vip \\&quot;`;&#125;sub usage &#123;    print    &quot;Usage: master_ip_failover --command=start|stop|stopssh|status --ssh-user=user --orig_master_host=host --orig_master_ip=ip --orig_master_port=port --new_master_host=host --new_master_ip=ip --new_master_port=port\\n&quot;;&#125;\n\nSlave上的relay_log配置&emsp;&emsp;从 MHA Failover 的过程中可以了解到， MHA Manager 在恢复（ 补齐）其他 Slave 数据时会用到 relay-log ，因此这些 relay-log 需要被保留。\n&emsp;&emsp;而默认情况下， SQL 线程在回放完毕后， MySQL 会 主动删除 relay-log ，需要 禁用 该功能，确保 relay-log 不被自动删除。\n所以在所有 Slave 节点 中配置如下参数，然后重启mysql服务。\n# 所有的Slave节点[mysqld]# 关闭relay-log主动删除的功能relay_log_purge = 0\n但是这样做了以后又带来另外一个问题，relay-log 会大量堆积导致磁盘空间紧张，所以需要定时清空过时的relay-log。MHA Node 的安装包中有一个 pure_relay_logs 工具，提供删除大量 relay-log 的功能。\npure_relay_logs工具\n该工具原理：在Linux下删除体积较大的文件需要一定的时间，且消耗一定的资源，所以 pure_relay_logs 在删除relay-log 之前会做一次 硬连接 ，然后删除对应的 relay-log （只删除指向 relay-log 的 指针 ），这样不会造成系统资源消耗，从而影响复制（ 造成复制延时），最后再 删除硬连接的relay-log（_hardlink_） 。    \n\n参数\n--user ： 用户名--password ： 密码--port ： 端口号--workdir ： 指定 创建relay-log的硬链接 的位置    默认是 /var/tmp ，由于 硬连接不能跨分区 ，所以请 确保这个目录和你的relay-log在同一个分区 ；    当脚本执行成功后，硬链接会被删除--disable_relay_log_purge ： 默认情况下，如果 relay_log_purge=1 ，脚本会什么都不清理，自动退出    通过设定这个参数，当 relay_log_purge=1 的情况下，该参数会将relay_log_purge设置为0。清理relay log之后，最后再设置 relay_log_purge=0 。    但是还是要在 /etc/my.cnf 中显示配置 relay_log_purge=0 ，避免重启服务后被还原。\n\n可以增加如下定时清理脚本到定时任务中，大概几个小时执行一次\n#!/bin/bashuser=rootpasswd=123port=3306# 目录可以创建log_dir=&#x27;/mysql/purge_relay_logs&#x27;work_dir=&#x27;/mysql/relay_log_hardlink&#x27;purge=&#x27;/usr/local/mha/bin/purge_relay_logs&#x27;if [[ ! -d $&#123;work_dir&#125; ]];then\tmkdir $&#123;work_dir&#125; -pfiif [[ ! -d $&#123;log_dir&#125; ]];then\tmkdir $&#123;log_dir&#125; -pfi$&#123;purge&#125; --user=$&#123;user&#125; --password=$&#123;passwd&#125; --port=$&#123;port&#125; --workdir=$&#123;work_dir&#125; \\         --disable_relay_log_purge &gt;&gt; $&#123;log_dir&#125;/purge_relay_logs.log 2&gt;&amp;1\n\nManager检测配置中主机状态到次配置几乎完成，但是还未启动，可以在 MAH manager 机器运行 ssh 检测和复制关系检测\n# ssh检测shell&gt; masterha_check_ssh --conf=/etc/masterha/app1.conf   # 下面是检测正常Wed Mar 14 09:55:45 2018 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping.Wed Mar 14 09:55:45 2018 - [info] Reading application default configuration from /etc/masterha/app1.conf..Wed Mar 14 09:55:45 2018 - [info] Reading server configuration from /etc/masterha/app1.conf..Wed Mar 14 09:55:45 2018 - [info] Starting SSH connection tests..Wed Mar 14 09:55:46 2018 - [debug] Wed Mar 14 09:55:45 2018 - [debug]  Connecting via SSH from root@master(192.168.122.66:22) to root@slave1(192.168.122.70:22)..Wed Mar 14 09:55:45 2018 - [debug]   ok.Wed Mar 14 09:55:45 2018 - [debug]  Connecting via SSH from root@master(192.168.122.66:22) to root@slave2(192.168.122.80:22)..Wed Mar 14 09:55:46 2018 - [debug]   ok.Wed Mar 14 09:55:47 2018 - [debug] Wed Mar 14 09:55:46 2018 - [debug]  Connecting via SSH from root@slave2(192.168.122.80:22) to root@master(192.168.122.66:22)..Wed Mar 14 09:55:47 2018 - [debug]   ok.Wed Mar 14 09:55:47 2018 - [debug]  Connecting via SSH from root@slave2(192.168.122.80:22) to root@slave1(192.168.122.70:22)..Wed Mar 14 09:55:47 2018 - [debug]   ok.Wed Mar 14 09:55:47 2018 - [debug] Wed Mar 14 09:55:45 2018 - [debug]  Connecting via SSH from root@slave1(192.168.122.70:22) to root@master(192.168.122.66:22)..Wed Mar 14 09:55:46 2018 - [debug]   ok.Wed Mar 14 09:55:46 2018 - [debug]  Connecting via SSH from root@slave1(192.168.122.70:22) to root@slave2(192.168.122.80:22)..Wed Mar 14 09:55:47 2018 - [debug]   ok.Wed Mar 14 09:55:47 2018 - [info] All SSH connection tests passed successfully.# 复制关系检测shell&gt; masterha_check_repl --conf=/etc/masterha/app1.confWed Mar 14 09:57:00 2018 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping.Wed Mar 14 09:57:00 2018 - [info] Reading application default configuration from /etc/masterha/app1.conf..Wed Mar 14 09:57:00 2018 - [info] Reading server configuration from /etc/masterha/app1.conf..Wed Mar 14 09:57:00 2018 - [info] MHA::MasterMonitor version 0.57.Wed Mar 14 09:57:00 2018 - [debug] Connecting to servers..Wed Mar 14 09:57:01 2018 - [debug]  Connected to: master(192.168.122.66:3307), user=rootWed Mar 14 09:57:01 2018 - [debug]  Number of slave worker threads on host master(192.168.122.66:3307): 0Wed Mar 14 09:57:01 2018 - [debug]  Connected to: slave1(192.168.122.70:3307), user=rootWed Mar 14 09:57:01 2018 - [debug]  Number of slave worker threads on host slave1(192.168.122.70:3307): 8Wed Mar 14 09:57:01 2018 - [debug]  Connected to: slave2(192.168.122.80:3307), user=rootWed Mar 14 09:57:01 2018 - [debug]  Number of slave worker threads on host slave2(192.168.122.80:3307): 16Wed Mar 14 09:57:01 2018 - [debug]  Comparing MySQL versions..Wed Mar 14 09:57:01 2018 - [debug]   Comparing MySQL versions done.Wed Mar 14 09:57:01 2018 - [debug] Connecting to servers done.Wed Mar 14 09:57:01 2018 - [info] GTID failover mode = 1Wed Mar 14 09:57:01 2018 - [info] Dead Servers:Wed Mar 14 09:57:01 2018 - [info] Alive Servers:Wed Mar 14 09:57:01 2018 - [info]   master(192.168.122.66:3307)Wed Mar 14 09:57:01 2018 - [info]   slave1(192.168.122.70:3307)Wed Mar 14 09:57:01 2018 - [info]   slave2(192.168.122.80:3307)Wed Mar 14 09:57:01 2018 - [info] Alive Slaves:Wed Mar 14 09:57:01 2018 - [info]   slave1(192.168.122.70:3307)  Version=5.7.21-log (oldest major version between slaves) log-bin:enabledWed Mar 14 09:57:01 2018 - [info]     GTID ONWed Mar 14 09:57:01 2018 - [debug]    Relay log info repository: TABLEWed Mar 14 09:57:01 2018 - [info]     Replicating from 192.168.122.66(192.168.122.66:3307)Wed Mar 14 09:57:01 2018 - [info]     Primary candidate for the new Master (candidate_master is set)Wed Mar 14 09:57:01 2018 - [info]   slave2(192.168.122.80:3307)  Version=5.7.21-log (oldest major version between slaves) log-bin:enabledWed Mar 14 09:57:01 2018 - [info]     GTID ONWed Mar 14 09:57:01 2018 - [debug]    Relay log info repository: TABLEWed Mar 14 09:57:01 2018 - [info]     Replicating from 192.168.122.66(192.168.122.66:3307)Wed Mar 14 09:57:01 2018 - [info]     Not candidate for the new Master (no_master is set)Wed Mar 14 09:57:01 2018 - [info] Current Alive Master: master(192.168.122.66:3307)Wed Mar 14 09:57:01 2018 - [info] Checking slave configurations..Wed Mar 14 09:57:01 2018 - [info]  read_only=1 is not set on slave slave1(192.168.122.70:3307).Wed Mar 14 09:57:01 2018 - [info]  read_only=1 is not set on slave slave2(192.168.122.80:3307).Wed Mar 14 09:57:01 2018 - [info] Checking replication filtering settings..Wed Mar 14 09:57:01 2018 - [info]  binlog_do_db= , binlog_ignore_db= Wed Mar 14 09:57:01 2018 - [info]  Replication filtering check ok.Wed Mar 14 09:57:01 2018 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Wed Mar 14 09:57:01 2018 - [info] Checking SSH publickey authentication settings on the current master..Wed Mar 14 09:57:01 2018 - [debug] SSH connection test to master, option -o StrictHostKeyChecking=no -o PasswordAuthentication=no -o BatchMode=yes -o ConnectTimeout=5, timeout 5Wed Mar 14 09:57:01 2018 - [info] HealthCheck: SSH to master is reachable.Wed Mar 14 09:57:01 2018 - [info] master(192.168.122.66:3307) (current master)  # -- 有输出拓扑图和slave健康状况的信息 +--slave1(192.168.122.70:3307) +--slave2(192.168.122.80:3307)Wed Mar 14 09:57:01 2018 - [info] Checking replication health on slave1..Wed Mar 14 09:57:01 2018 - [info]  ok.Wed Mar 14 09:57:01 2018 - [info] Checking replication health on slave2..Wed Mar 14 09:57:01 2018 - [info]  ok.Wed Mar 14 09:57:01 2018 - [info] Checking master_ip_failover_script status:Wed Mar 14 09:57:01 2018 - [info]   /usr/local/mha/bin/master_ip_failover --command=status --ssh_user=root --orig_master_host=master --orig_master_ip=192.168.122.66 --orig_master_port=3307 IN SCRIPT TEST====/sbin/ifconfig eth0:88 down==/sbin/ifconfig eth0:88 192.168.122.99/24===Checking the Status of the script.. OK Wed Mar 14 09:57:01 2018 - [info]  OK.Wed Mar 14 09:57:01 2018 - [warning] shutdown_script is not defined.Wed Mar 14 09:57:01 2018 - [debug]  Disconnected from master(192.168.122.66:3307)Wed Mar 14 09:57:01 2018 - [debug]  Disconnected from slave1(192.168.122.70:3307)Wed Mar 14 09:57:01 2018 - [debug]  Disconnected from slave2(192.168.122.80:3307)Wed Mar 14 09:57:01 2018 - [info] Got exit code 0 (Not master dead).MySQL Replication Health is OK.\n\n启动MHA为master添加浮动ip&emsp;&emsp;添加 VIP，因为这里使用的是 masterha_ip_failover 脚本切换 VIP，由于这个脚本的 start/stop 操作只有在 Failover (切换)期间才会执行，所以即便设置了该脚本的 vip，MHA 也不会在masterha_manager一开始启动的时候，在 Master 上设置 VIP，初次在 Master 上设置 VIP 需要人工操作，后期如果有 Failover 操作， MHA 会执行脚本，自动切换到新选取的主机器上。\n#### Master 端### 在我的测试机中，网卡名称为eth0，两台可以提升成主的设备都是eth0网卡shell&gt; ifconfig eth0:88 192.168.122.99/24\n\n启动masterha_manager一些参数\n\n--conf ：当前 MySQL 集群的配置文件，可以有多个，应用于不同的集群\n--remove_dead_master_conf ：当发生切换操作 Failover 后，需要把之前的 Dead Master 从配置文件中删除。\n如果不删除，且没有恢复的话，此时  masterha_manager 重启后，会报错 “there is a dead slave”\n\n\n--ignore_last_failover ：如果前一次 Failover 失败了， MHA 不会去再一次去做 Failover 操作，除非人为的删除 (manager_workdir)/(app_name).failover.error ，或者增加此参数， MHA 会继续进行 Failover 操作。\n\n启动 MHA 的管理服务\n# 启动MHA managershell&gt; nohup masterha_manager --conf=/etc/masterha/app1.conf --remove_dead_master_conf --ignore_last_failover &lt; /dev/null &gt; /var/log/masterha/app1/manager.log 2&gt;&amp;1 &amp;shell&gt; masterha_check_status --conf=/etc/masterha/app1.confapp1 (pid:18702) is running(0:PING_OK), master:Master# 状态正常，且当前主机是 Master\n\n&emsp;&emsp;日志可以查看 /var/log/masterha/app1/manager.log，在 failover 成功后会把旧 master 的配置段删掉，如切换 VIP 后我们配置文件中的 server1(192.168.122.66:3307) 配置段将被删。\n测试MHA自动Failover\n在做 Failover 测试之前，把 MHA Manager 停掉，把 MySQL 节点也都停掉（ 注意停止顺序 ），把数据库进行一次冷备份，方便以后测试。若没测试成功可以移回冷备文件继续测。\n\n\n停掉 Slave的 IO 线程 ，模拟复制延时（ Slave1保持不变）\n使用 sysbench 对 Master 进行测试，生成测试数据（可以产生大量的 binlog）\n等待步骤2完成 后，开启 Slave 上的 IO 线程，让它去追 Master 的 binlog，同时立即操作第四步\n关闭  Master 上的 MySQL，让 MHA 产生 Failover 操作\n观察最终状态\n\n附发生failover时的MHA Manager日志\nTue Mar 13 17:10:50 2018 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping.Tue Mar 13 17:10:50 2018 - [info] Reading application default configuration from /etc/masterha/app1.conf..Tue Mar 13 17:10:50 2018 - [info] Reading server configuration from /etc/masterha/app1.conf..Tue Mar 13 17:12:23 2018 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping.Tue Mar 13 17:12:23 2018 - [info] Reading application default configuration from /etc/masterha/app1.conf..Tue Mar 13 17:12:23 2018 - [info] Reading server configuration from /etc/masterha/app1.conf..tory. Check for details, and consider setting --workdir separately.Tue Mar 13 17:10:50 2018 - [debug] Connecting to servers..Tue Mar 13 17:10:51 2018 - [debug]  Connected to: master(192.168.122.66:3307), user=rootTue Mar 13 17:10:51 2018 - [debug]  Number of slave worker threads on host master(192.168.122.66:3307): 0Tue Mar 13 17:10:51 2018 - [debug]  Connected to: slave1(192.168.122.70:3307), user=rootTue Mar 13 17:10:51 2018 - [debug]  Number of slave worker threads on host slave1(192.168.122.70:3307): 8Tue Mar 13 17:10:51 2018 - [debug]  Connected to: slave2(192.168.122.80:3307), user=rootTue Mar 13 17:10:51 2018 - [debug]  Number of slave worker threads on host slave2(192.168.122.80:3307): 16Tue Mar 13 17:10:51 2018 - [debug]  Comparing MySQL versions..Tue Mar 13 17:10:51 2018 - [debug]   Comparing MySQL versions done.Tue Mar 13 17:10:51 2018 - [debug] Connecting to servers done.Tue Mar 13 17:10:51 2018 - [info] GTID failover mode = 1Tue Mar 13 17:10:51 2018 - [info] Dead Servers:Tue Mar 13 17:10:51 2018 - [info] Alive Servers:Tue Mar 13 17:10:51 2018 - [info]   master(192.168.122.66:3307)Tue Mar 13 17:10:51 2018 - [info]   slave1(192.168.122.70:3307)Tue Mar 13 17:10:51 2018 - [info]   slave2(192.168.122.80:3307)Tue Mar 13 17:10:51 2018 - [info] Alive Slaves:Tue Mar 13 17:10:51 2018 - [info]   slave1(192.168.122.70:3307)  Version=5.7.21-log (oldest major version between slaves) log-bin:enabledTue Mar 13 17:10:51 2018 - [info]     GTID ONTue Mar 13 17:10:51 2018 - [debug]    Relay log info repository: TABLETue Mar 13 17:10:51 2018 - [info]     Replicating from 192.168.122.66(192.168.122.66:3307)Tue Mar 13 17:10:51 2018 - [info]     Primary candidate for the new Master (candidate_master is set)Tue Mar 13 17:10:51 2018 - [info]   slave2(192.168.122.80:3307)  Version=5.7.21-log (oldest major version between slaves) log-bin:enabledTue Mar 13 17:10:51 2018 - [info]     GTID ONTue Mar 13 17:10:51 2018 - [debug]    Relay log info repository: TABLETue Mar 13 17:10:51 2018 - [info]     Replicating from 192.168.122.66(192.168.122.66:3307)Tue Mar 13 17:10:51 2018 - [info]     Not candidate for the new Master (no_master is set)Tue Mar 13 17:10:51 2018 - [info] Current Alive Master: master(192.168.122.66:3307)Tue Mar 13 17:10:51 2018 - [info] Checking slave configurations..Tue Mar 13 17:10:51 2018 - [info]  read_only=1 is not set on slave slave1(192.168.122.70:3307).Tue Mar 13 17:10:51 2018 - [info]  read_only=1 is not set on slave slave2(192.168.122.80:3307).Tue Mar 13 17:10:51 2018 - [info] Checking replication filtering settings..Tue Mar 13 17:10:51 2018 - [info]  binlog_do_db= , binlog_ignore_db= Tue Mar 13 17:10:51 2018 - [info]  Replication filtering check ok.Tue Mar 13 17:10:51 2018 - [info] GTID (with auto-pos) is supported. Skipping all SSH and Node package checking.Tue Mar 13 17:10:51 2018 - [info] Checking SSH publickey authentication settings on the current master..Tue Mar 13 17:10:51 2018 - [debug] SSH connection test to master, option -o StrictHostKeyChecking=no -o PasswordAuthentication=no -o BatchMode=yes -o ConnectTimeout=5, timeout 5Tue Mar 13 17:10:52 2018 - [info] HealthCheck: SSH to master is reachable.Tue Mar 13 17:10:52 2018 - [info] master(192.168.122.66:3307) (current master) +--slave1(192.168.122.70:3307) +--slave2(192.168.122.80:3307)Tue Mar 13 17:10:52 2018 - [info] Checking master_ip_failover_script status:Tue Mar 13 17:10:52 2018 - [info]   /usr/local/mha/bin/master_ip_failover --command=status --ssh_user=root --orig_master_host=master --orig_master_ip=192.168.122.66 --orig_master_port=3307 IN SCRIPT TEST====/sbin/ifconfig eth0:88 down==/sbin/ifconfig eth0:88 192.168.122.99/24===# 还没故障转移的正常启动日志Checking the Status of the script.. OK Tue Mar 13 17:10:52 2018 - [info]  OK.Tue Mar 13 17:10:52 2018 - [warning] shutdown_script is not defined.Tue Mar 13 17:10:52 2018 - [debug]  Disconnected from master(192.168.122.66:3307)Tue Mar 13 17:10:52 2018 - [debug]  Disconnected from slave1(192.168.122.70:3307)Tue Mar 13 17:10:52 2018 - [debug]  Disconnected from slave2(192.168.122.80:3307)Tue Mar 13 17:10:52 2018 - [debug] SSH check command: exit 0Tue Mar 13 17:10:52 2018 - [info] Set master ping interval 3 seconds.Tue Mar 13 17:10:52 2018 - [info] Set secondary check script: /usr/local/mha/bin/masterha_secondary_check -s 192.168.122.70 -s 192.168.122.80 --user=root --master_host=192.168.122.66 --master_port=3307Tue Mar 13 17:10:52 2018 - [info] Starting ping health check on master(192.168.122.66:3307)..Tue Mar 13 17:10:52 2018 - [debug] Connected on master.Tue Mar 13 17:10:52 2018 - [debug] Set short wait_timeout on master: 6 secondsTue Mar 13 17:10:52 2018 - [debug] Trying to get advisory lock..Tue Mar 13 17:10:52 2018 - [info] Ping(INSERT) succeeded, waiting until MySQL doesn&#x27;t respond..  # 开始等待mysql不响应Tue Mar 13 17:12:13 2018 - [warning] Got error on MySQL insert ping: 2006 (MySQL server has gone away)    # 此时发现master实例ping不通了Tue Mar 13 17:12:13 2018 - [info] Executing secondary network check script: /usr/local/mha/bin/masterha_secondary_check -s 192.168.122.70 -s 192.168.122.80 --user=root --master_host=192.168.122.66 --master_port=3307  --user=root  --master_host=master  --master_ip=192.168.122.66  --master_port=3307 --master_user=root --master_password=111111 --ping_type=INSERTTue Mar 13 17:12:13 2018 - [info] Executing SSH check script: exit 0Tue Mar 13 17:12:13 2018 - [debug] SSH connection test to master, option -o StrictHostKeyChecking=no -o PasswordAuthentication=no -o BatchMode=yes -o ConnectTimeout=5, timeout 5Tue Mar 13 17:12:13 2018 - [info] HealthCheck: SSH to master is reachable.    # 用两个slave实例检测ssh是否能到masterMonitoring server 192.168.122.70 is reachable, Master is not reachable from 192.168.122.70. OK.Monitoring server 192.168.122.80 is reachable, Master is not reachable from 192.168.122.80. OK.Tue Mar 13 17:12:14 2018 - [info] Master is not reachable from all other monitoring servers. Failover should start.  # 两个slave都不能连到master的mysql实例，重试3次Tue Mar 13 17:12:16 2018 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;192.168.122.66&#x27; (111))Tue Mar 13 17:12:16 2018 - [warning] Connection failed 2 time(s)..Tue Mar 13 17:12:19 2018 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;192.168.122.66&#x27; (111))Tue Mar 13 17:12:19 2018 - [warning] Connection failed 3 time(s)..Tue Mar 13 17:12:22 2018 - [warning] Got error on MySQL connect: 2003 (Can&#x27;t connect to MySQL server on &#x27;192.168.122.66&#x27; (111))Tue Mar 13 17:12:22 2018 - [warning] Connection failed 4 time(s)..Tue Mar 13 17:12:22 2018 - [warning] Master is not reachable from health checker!Tue Mar 13 17:12:22 2018 - [warning] Master master(192.168.122.66:3307) is not reachable!Tue Mar 13 17:12:22 2018 - [warning] SSH is reachable.Tue Mar 13 17:12:22 2018 - [info] Connecting to a master server failed. Reading configuration file /etc/masterha_default.cnf and /etc/masterha/app1.conf again, and trying to connect to all servers to check server status..Tue Mar 13 17:12:22 2018 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping.Tue Mar 13 17:12:22 2018 - [info] Reading application default configuration from /etc/masterha/app1.conf..Tue Mar 13 17:12:22 2018 - [info] Reading server configuration from /etc/masterha/app1.conf..Tue Mar 13 17:12:22 2018 - [debug] Skipping connecting to dead master master(192.168.122.66:3307).Tue Mar 13 17:12:22 2018 - [debug] Connecting to servers..Tue Mar 13 17:12:23 2018 - [debug]  Connected to: slave1(192.168.122.70:3307), user=rootTue Mar 13 17:12:23 2018 - [debug]  Number of slave worker threads on host slave1(192.168.122.70:3307): 8Tue Mar 13 17:12:23 2018 - [debug]  Connected to: slave2(192.168.122.80:3307), user=rootTue Mar 13 17:12:23 2018 - [debug]  Number of slave worker threads on host slave2(192.168.122.80:3307): 16Tue Mar 13 17:12:23 2018 - [debug]  Comparing MySQL versions..Tue Mar 13 17:12:23 2018 - [debug]   Comparing MySQL versions done.Tue Mar 13 17:12:23 2018 - [debug] Connecting to servers done.Tue Mar 13 17:12:23 2018 - [info] GTID failover mode = 1Tue Mar 13 17:12:23 2018 - [info] Dead Servers:    # master 已经deadTue Mar 13 17:12:23 2018 - [info]   master(192.168.122.66:3307)Tue Mar 13 17:12:23 2018 - [info] Alive Servers:Tue Mar 13 17:12:23 2018 - [info]   slave1(192.168.122.70:3307)Tue Mar 13 17:12:23 2018 - [info]   slave2(192.168.122.80:3307)Tue Mar 13 17:12:23 2018 - [info] Alive Slaves:Tue Mar 13 17:12:23 2018 - [info]   slave1(192.168.122.70:3307)  Version=5.7.21-log (oldest major version between slaves) log-bin:enabledTue Mar 13 17:12:23 2018 - [info]     GTID ONTue Mar 13 17:12:23 2018 - [debug]    Relay log info repository: TABLETue Mar 13 17:12:23 2018 - [info]     Replicating from 192.168.122.66(192.168.122.66:3307)Tue Mar 13 17:12:23 2018 - [info]     Primary candidate for the new Master (candidate_master is set)    Tue Mar 13 17:12:23 2018 - [info]   slave2(192.168.122.80:3307)  Version=5.7.21-log (oldest major version between slaves) log-bin:enabledTue Mar 13 17:12:23 2018 - [info]     GTID ONTue Mar 13 17:12:23 2018 - [debug]    Relay log info repository: TABLETue Mar 13 17:12:23 2018 - [info]     Replicating from 192.168.122.66(192.168.122.66:3307)Tue Mar 13 17:12:23 2018 - [info]     Not candidate for the new Master (no_master is set)    Tue Mar 13 17:12:23 2018 - [info] Checking slave configurations..Tue Mar 13 17:12:23 2018 - [info]  read_only=1 is not set on slave slave1(192.168.122.70:3307).Tue Mar 13 17:12:23 2018 - [info]  read_only=1 is not set on slave slave2(192.168.122.80:3307).Tue Mar 13 17:12:23 2018 - [info] Checking replication filtering settings..Tue Mar 13 17:12:23 2018 - [info]  Replication filtering check ok.Tue Mar 13 17:12:23 2018 - [info] Master is down!Tue Mar 13 17:12:23 2018 - [info] Terminating monitoring script.Tue Mar 13 17:12:23 2018 - [debug]  Disconnected from slave1(192.168.122.70:3307)Tue Mar 13 17:12:23 2018 - [debug]  Disconnected from slave2(192.168.122.80:3307)Tue Mar 13 17:12:23 2018 - [info] Got exit code 20 (Master dead).Tue Mar 13 17:12:23 2018 - [info] MHA::MasterFailover version 0.57.Tue Mar 13 17:12:23 2018 - [info] Starting master failover.Tue Mar 13 17:12:23 2018 - [info] Tue Mar 13 17:12:23 2018 - [info] * Phase 1: Configuration Check Phase..Tue Mar 13 17:12:23 2018 - [info] Tue Mar 13 17:12:23 2018 - [debug] Skipping connecting to dead master master.Tue Mar 13 17:12:23 2018 - [debug] Connecting to servers..Tue Mar 13 17:12:24 2018 - [debug]  Connected to: slave1(192.168.122.70:3307), user=rootTue Mar 13 17:12:24 2018 - [debug]  Number of slave worker threads on host slave1(192.168.122.70:3307): 8Tue Mar 13 17:12:24 2018 - [debug]  Connected to: slave2(192.168.122.80:3307), user=rootTue Mar 13 17:12:24 2018 - [debug]  Number of slave worker threads on host slave2(192.168.122.80:3307): 16Tue Mar 13 17:12:24 2018 - [debug]  Comparing MySQL versions..Tue Mar 13 17:12:24 2018 - [debug]   Comparing MySQL versions done.Tue Mar 13 17:12:24 2018 - [debug] Connecting to servers done.Tue Mar 13 17:12:24 2018 - [info] GTID failover mode = 1Tue Mar 13 17:12:24 2018 - [info] Dead Servers:Tue Mar 13 17:12:24 2018 - [info]   master(192.168.122.66:3307)Tue Mar 13 17:12:24 2018 - [info] Alive Servers:Tue Mar 13 17:12:24 2018 - [info]   slave1(192.168.122.70:3307)Tue Mar 13 17:12:24 2018 - [info]   slave2(192.168.122.80:3307)Tue Mar 13 17:12:24 2018 - [info] Alive Slaves:Tue Mar 13 17:12:24 2018 - [info]   slave1(192.168.122.70:3307)  Version=5.7.21-log (oldest major version between slaves) log-bin:enabledTue Mar 13 17:12:24 2018 - [info]     GTID ONTue Mar 13 17:12:24 2018 - [debug]    Relay log info repository: TABLETue Mar 13 17:12:24 2018 - [info]     Replicating from 192.168.122.66(192.168.122.66:3307)Tue Mar 13 17:12:24 2018 - [info]     Primary candidate for the new Master (candidate_master is set)     # 存活的slave中slave1是配置了可以提升为master的Tue Mar 13 17:12:24 2018 - [info]   slave2(192.168.122.80:3307)  Version=5.7.21-log (oldest major version between slaves) log-bin:enabledTue Mar 13 17:12:24 2018 - [info]     GTID ONTue Mar 13 17:12:24 2018 - [debug]    Relay log info repository: TABLETue Mar 13 17:12:24 2018 - [info]     Replicating from 192.168.122.66(192.168.122.66:3307)Tue Mar 13 17:12:24 2018 - [info]     Not candidate for the new Master (no_master is set)    # slave2配置文件未设置能提升为masterTue Mar 13 17:12:24 2018 - [info] Starting GTID based failover.Tue Mar 13 17:12:24 2018 - [info] Tue Mar 13 17:12:24 2018 - [info] ** Phase 1: Configuration Check Phase completed.Tue Mar 13 17:12:24 2018 - [info] Tue Mar 13 17:12:24 2018 - [info] * Phase 2: Dead Master Shutdown Phase..Tue Mar 13 17:12:24 2018 - [info] Tue Mar 13 17:12:24 2018 - [debug]  Stopping IO thread on slave1(192.168.122.70:3307)..      # 停止slave1的IO线程Tue Mar 13 17:12:24 2018 - [info] Forcing shutdown so that applications never connect to the current master..Tue Mar 13 17:12:24 2018 - [info] Executing master IP deactivation script:Tue Mar 13 17:12:24 2018 - [info]   /usr/local/mha/bin/master_ip_failover --orig_master_host=master --orig_master_ip=192.168.122.66 --orig_master_port=3307 --command=stopssh --ssh_user=root  Tue Mar 13 17:12:24 2018 - [debug]  Stopping IO thread on slave2(192.168.122.80:3307)..Tue Mar 13 17:12:24 2018 - [debug]  Stop IO thread on slave2(192.168.122.80:3307) done.INSCRIPT TEST====/sbin/ifconfig eth0:88 down==/sbin/ifconfig eth0:88 192.168.122.99/24===Disabling the VIP on old master: master Tue Mar 13 17:12:24 2018 - [debug]  Stop IO thread on slave1(192.168.122.70:3307) done.Tue Mar 13 17:12:24 2018 - [info]  done.Tue Mar 13 17:12:24 2018 - [warning] shutdown_script is not set. Skipping explicit shutting down of the dead master.Tue Mar 13 17:12:24 2018 - [info] * Phase 2: Dead Master Shutdown Phase completed.Tue Mar 13 17:12:24 2018 - [info] Tue Mar 13 17:12:24 2018 - [info] * Phase 3: Master Recovery Phase..Tue Mar 13 17:12:24 2018 - [info] Tue Mar 13 17:12:24 2018 - [info] * Phase 3.1: Getting Latest Slaves Phase..Tue Mar 13 17:12:24 2018 - [info] Tue Mar 13 17:12:24 2018 - [debug] Fetching current slave status..Tue Mar 13 17:12:24 2018 - [debug]  Fetching current slave status done.Tue Mar 13 17:12:24 2018 - [info] The latest binary log file/position on all slaves is bin.000004:20816Tue Mar 13 17:12:24 2018 - [info] Retrieved Gtid Set: 0c154ad5-2699-11e8-94a1-525400eac085:309-394Tue Mar 13 17:12:24 2018 - [info] Latest slaves (Slaves that received relay log files to the latest):Tue Mar 13 17:12:24 2018 - [info]   slave1(192.168.122.70:3307)  Version=5.7.21-log (oldest major version between slaves) log-bin:enabledTue Mar 13 17:12:24 2018 - [info]     GTID ONTue Mar 13 17:12:24 2018 - [debug]    Relay log info repository: TABLETue Mar 13 17:12:24 2018 - [info]     Replicating from 192.168.122.66(192.168.122.66:3307)Tue Mar 13 17:12:24 2018 - [info]     Primary candidate for the new Master (candidate_master is set)Tue Mar 13 17:12:24 2018 - [info]   slave2(192.168.122.80:3307)  Version=5.7.21-log (oldest major version between slaves) log-bin:enabledTue Mar 13 17:12:24 2018 - [info]     GTID ONTue Mar 13 17:12:24 2018 - [debug]    Relay log info repository: TABLETue Mar 13 17:12:24 2018 - [info]     Replicating from 192.168.122.66(192.168.122.66:3307)Tue Mar 13 17:12:24 2018 - [info]     Not candidate for the new Master (no_master is set)Tue Mar 13 17:12:24 2018 - [info] The oldest binary log file/position on all slaves is bin.000004:20816Tue Mar 13 17:12:24 2018 - [info] Retrieved Gtid Set: 0c154ad5-2699-11e8-94a1-525400eac085:309-394Tue Mar 13 17:12:24 2018 - [info] Oldest slaves:Tue Mar 13 17:12:24 2018 - [info]   slave1(192.168.122.70:3307)  Version=5.7.21-log (oldest major version between slaves) log-bin:enabledTue Mar 13 17:12:24 2018 - [info]     GTID ONTue Mar 13 17:12:24 2018 - [debug]    Relay log info repository: TABLETue Mar 13 17:12:24 2018 - [info]     Replicating from 192.168.122.66(192.168.122.66:3307)Tue Mar 13 17:12:24 2018 - [info]     Primary candidate for the new Master (candidate_master is set)Tue Mar 13 17:12:24 2018 - [info]   slave2(192.168.122.80:3307)  Version=5.7.21-log (oldest major version between slaves) log-bin:enabledTue Mar 13 17:12:24 2018 - [info]     GTID ONTue Mar 13 17:12:24 2018 - [debug]    Relay log info repository: TABLETue Mar 13 17:12:24 2018 - [info]     Replicating from 192.168.122.66(192.168.122.66:3307)Tue Mar 13 17:12:24 2018 - [info]     Not candidate for the new Master (no_master is set)Tue Mar 13 17:12:24 2018 - [info] Tue Mar 13 17:12:24 2018 - [info] * Phase 3.3: Determining New Master Phase..Tue Mar 13 17:12:24 2018 - [info] Tue Mar 13 17:12:24 2018 - [info] Searching new master from slaves..Tue Mar 13 17:12:24 2018 - [info]  Candidate masters from the configuration file:Tue Mar 13 17:12:24 2018 - [info]   slave1(192.168.122.70:3307)  Version=5.7.21-log (oldest major version between slaves) log-bin:enabledTue Mar 13 17:12:24 2018 - [info]     GTID ONTue Mar 13 17:12:24 2018 - [debug]    Relay log info repository: TABLETue Mar 13 17:12:24 2018 - [info]     Replicating from 192.168.122.66(192.168.122.66:3307)Tue Mar 13 17:12:24 2018 - [info]     Primary candidate for the new Master (candidate_master is set)Tue Mar 13 17:12:24 2018 - [info]  Non-candidate masters:Tue Mar 13 17:12:24 2018 - [info]   slave2(192.168.122.80:3307)  Version=5.7.21-log (oldest major version between slaves) log-bin:enabledTue Mar 13 17:12:24 2018 - [info]     GTID ONTue Mar 13 17:12:24 2018 - [debug]    Relay log info repository: TABLETue Mar 13 17:12:24 2018 - [info]     Replicating from 192.168.122.66(192.168.122.66:3307)Tue Mar 13 17:12:24 2018 - [info]     Not candidate for the new Master (no_master is set)Tue Mar 13 17:12:24 2018 - [info]  Searching from candidate_master slaves which have received the latest relay log events..Tue Mar 13 17:12:24 2018 - [info] New master is slave1(192.168.122.70:3307)    # 选择slave1提升为新的masterTue Mar 13 17:12:24 2018 - [info] Starting master failover..    # 开始failover切换Tue Mar 13 17:12:24 2018 - [info] From:    # 原架构拓扑master(192.168.122.66:3307) (current master) +--slave1(192.168.122.70:3307) +--slave2(192.168.122.80:3307)To:\t\t # failover后的拓扑slave1(192.168.122.70:3307) (new master) +--slave2(192.168.122.80:3307)Tue Mar 13 17:12:24 2018 - [info] Tue Mar 13 17:12:24 2018 - [info] * Phase 3.3: New Master Recovery Phase..Tue Mar 13 17:12:24 2018 - [info] Tue Mar 13 17:12:24 2018 - [info]  Waiting all logs to be applied.. Tue Mar 13 17:12:24 2018 - [info]   done.Tue Mar 13 17:12:24 2018 - [debug]  Stopping slave IO/SQL thread on slave1(192.168.122.70:3307)..Tue Mar 13 17:12:24 2018 - [debug]   done.Tue Mar 13 17:12:24 2018 - [info] Getting new master&#x27;s binlog name and position..Tue Mar 13 17:12:24 2018 - [info]  bin.000003:94211Tue Mar 13 17:12:24 2018 - [info]  All other slaves should start replication from here. Statement should be: CHANGE MASTER TO MASTER_HOST=&#x27;slave1 or 192.168.122.70&#x27;, MASTER_PORT=3307, MASTER_AUTO_POSITION=1, MASTER_USER=&#x27;rpl&#x27;, MASTER_PASSWORD=&#x27;xxx&#x27;;Tue Mar 13 17:12:24 2018 - [info] Master Recovery succeeded. File:Pos:Exec_Gtid_Set: bin.000003, 94211, 0c154ad5-2699-11e8-94a1-525400eac085:1-394,c08d09b5-2698-11e8-9ec0-5254004dae68:1-2Tue Mar 13 17:12:24 2018 - [info] Executing master IP activate script:Tue Mar 13 17:12:24 2018 - [info]   /usr/local/mha/bin/master_ip_failover --command=start --ssh_user=root --orig_master_host=master --orig_master_ip=192.168.122.66 --orig_master_port=3307 --new_master_host=slave1 --new_master_ip=192.168.122.70 --new_master_port=3307 --new_master_user=&#x27;root&#x27;   --new_master_password=xxxUnknown option: new_master_userUnknown option: new_master_passwordIN SCRIPT TEST====/sbin/ifconfig eth0:88 down==/sbin/ifconfig eth0:88 192.168.122.99/24===Enabling the VIP - 192.168.122.99/24 on the new master - slave1 Tue Mar 13 17:12:25 2018 - [info]  OK.Tue Mar 13 17:12:25 2018 - [info] ** Finished master recovery successfully.Tue Mar 13 17:12:25 2018 - [info] * Phase 3: Master Recovery Phase completed.Tue Mar 13 17:12:25 2018 - [info] Tue Mar 13 17:12:25 2018 - [info] * Phase 4: Slaves Recovery Phase..Tue Mar 13 17:12:25 2018 - [info] Tue Mar 13 17:12:25 2018 - [info] Tue Mar 13 17:12:25 2018 - [info] * Phase 4.1: Starting Slaves in parallel..Tue Mar 13 17:12:25 2018 - [info] Tue Mar 13 17:12:25 2018 - [info] -- Slave recovery on host slave2(192.168.122.80:3307) started, pid: 10212. Check tmp log /var/log/masterha/app1/slave2_3307_20180313171223.log if it takes time..Tue Mar 13 17:12:27 2018 - [info] Tue Mar 13 17:12:27 2018 - [info] Log messages from slave2 ...Tue Mar 13 17:12:27 2018 - [info] Tue Mar 13 17:12:25 2018 - [info]  Resetting slave slave2(192.168.122.80:3307) and starting replication from the new master slave1(192.168.122.70:3307)..Tue Mar 13 17:12:25 2018 - [debug]  Stopping slave IO/SQL thread on slave2(192.168.122.80:3307)..Tue Mar 13 17:12:25 2018 - [debug]   done.Tue Mar 13 17:12:25 2018 - [info]  Executed CHANGE MASTER.Tue Mar 13 17:12:25 2018 - [debug]  Starting slave IO/SQL thread on slave2(192.168.122.80:3307)..Tue Mar 13 17:12:26 2018 - [debug]   done.Tue Mar 13 17:12:26 2018 - [info]  Slave started.Tue Mar 13 17:12:26 2018 - [info]  gtid_wait(0c154ad5-2699-11e8-94a1-525400eac085:1-394,c08d09b5-2698-11e8-9ec0-5254004dae68:1-2) completed on slave2(192.168.122.80:3307). Executed 3 events.Tue Mar 13 17:12:27 2018 - [info] End of log messages from slave2.Tue Mar 13 17:12:27 2018 - [info] -- Slave on host slave2(192.168.122.80:3307) started.Tue Mar 13 17:12:27 2018 - [info] All new slave servers recovered successfully.Tue Mar 13 17:12:27 2018 - [info] Tue Mar 13 17:12:27 2018 - [info] * Phase 5: New master cleanup phase..Tue Mar 13 17:12:27 2018 - [info] Tue Mar 13 17:12:27 2018 - [info] Resetting slave info on the new master..Tue Mar 13 17:12:27 2018 - [debug]  Clearing slave info..Tue Mar 13 17:12:27 2018 - [debug]  Stopping slave IO/SQL thread on slave1(192.168.122.70:3307)..Tue Mar 13 17:12:27 2018 - [debug]   done.Tue Mar 13 17:12:27 2018 - [debug]  SHOW SLAVE STATUS shows new master does not replicate from anywhere. OK.Tue Mar 13 17:12:27 2018 - [info]  slave1: Resetting slave info succeeded.Tue Mar 13 17:12:27 2018 - [info] Master failover to slave1(192.168.122.70:3307) completed successfully.Tue Mar 13 17:12:27 2018 - [info] Deleted server1 entry from /etc/masterha/app1.conf .Tue Mar 13 17:12:27 2018 - [debug]  Disconnected from slave1(192.168.122.70:3307)Tue Mar 13 17:12:27 2018 - [debug]  Disconnected from slave2(192.168.122.80:3307)Tue Mar 13 17:12:27 2018 - [info] ----- Failover Report -----# 下面是failover的报表，集群中master角色从原来的66:3307切到了70:3307机器app1: MySQL Master failover master(192.168.122.66:3307) to slave1(192.168.122.70:3307) succeededMaster master(192.168.122.66:3307) is down!Check MHA Manager logs at centos-66:/var/log/masterha/app1/manager.log for details.Started automated(non-interactive) failover.Invalidated master IP address on master(192.168.122.66:3307)Selected slave1(192.168.122.70:3307) as a new master.slave1(192.168.122.70:3307): OK: Applying all logs succeeded.slave1(192.168.122.70:3307): OK: Activated master IP address.slave2(192.168.122.80:3307): OK: Slave started, replicating from slave1(192.168.122.70:3307)slave1(192.168.122.70:3307): Resetting slave info succeeded.Master failover to slave1(192.168.122.70:3307) completed successfully.  # 切换成功","categories":["MySQL"],"tags":["MHA"]},{"title":"python 与 openssl 对应的 AES-CBC","url":"/2021/04/05/python-%E4%B8%8E-openssl-%E5%AF%B9%E5%BA%94%E7%9A%84-AES-CBC/","content":"python 实现与 openssl 对应的 AES-CBC\n可对应 openssl 的 AES-CBC 加密，参考来源\n\n奇怪的需求又增加了，项目大量使用的 shell 脚本，有需求加密某内容，让脚本解密后使用。一番尝试后，发现 python 中的 AES 还与操作系统中 openssl 工具加解密不对应。又一番面向 Stack Overflow 编程后，调试出了合适的 demo，以下是处理内容。\n需要注意的是这种方式不够安全！！！\n先安装加密库：\npip3 install pycryptodome\n\n\n\n 对应的 AES-CBC 加密类封装，使用了内存中的 BytesIO，所以别加密太大的内容。\nimport ioimport base64from os import urandomfrom hashlib import md5from Crypto.Cipher import AES&quot;&quot;&quot;pip3 install pycryptodome&quot;&quot;&quot;class AESCipher:    &quot;&quot;&quot;    NOTE 这种方法现在已经不是很安全，只是为了与 `openssl` 命令相兼容，方便脚本解密    使用内存 BytesIO 写入加密、解密内容，模拟文件进行加密、解密，主要用到 AES-CBC 算法，加 salt    参考: https://stackoverflow.com/questions/16761458/how-to-decrypt-openssl-aes-encrypted-files-in-python    &quot;&quot;&quot;    block_size = AES.block_size    def __init__(self, content, password, salt_header=&#x27;Salted__&#x27;, key_length=32):        &quot;&quot;&quot;        注意: 若需要支持 `openssl` 命令，必须指定 salt_header=&#x27;Salted__&#x27;        如果只是 python 内部使用 salt_header 可以为空        &quot;&quot;&quot;        self.content = content        self.password = password        self.salt_header = salt_header        self.key_length = key_length    def derive_key_and_iv(self, salt):        &quot;&quot;&quot;计算得到 `key` 和 `iv` &quot;&quot;&quot;        d = d_i = b&#x27;&#x27;        while len(d) &lt; self.key_length + self.block_size:            d_i = md5(d_i + str.encode(self.password) + salt).digest()            d += d_i        return d[:self.key_length], d[self.key_length:self.key_length + self.block_size]    def encrypt(self) -&gt; bytes:        &quot;&quot;&quot;        返回的是加密后的 `base64` 二进制字符串，bytes        对应 `openssl` 命令解法：            从文件：openssl aes-256-cbc -salt -in secret.txt -d -a -k &#x27;password&#x27;            从输入：echo &quot;U2FsdGVkX1/5sFe6z6+H4CfQvnTZgCEV4yget0PI8XM=&quot; | openssl aes-256-cbc -salt -d -a -k &#x27;password&#x27;        &quot;&quot;&quot;        content = self.content.encode() if not isinstance(self.content, bytes) else self.content        # 字节IO模拟文件        in_file = io.BytesIO(content)        out_file = io.BytesIO()        salt = urandom(self.block_size - len(self.salt_header))        key, iv = self.derive_key_and_iv(salt)        cipher = AES.new(key, AES.MODE_CBC, iv)        out_file.write(str.encode(self.salt_header) + salt)        finished = False        while not finished:            chunk = in_file.read(1024 * self.block_size)            if len(chunk) == 0 or len(chunk) % self.block_size != 0:                padding_length = (self.block_size - len(chunk) % self.block_size) or self.block_size                chunk += str.encode(                    padding_length * chr(padding_length))                finished = True            out_file.write(cipher.encrypt(chunk))        return base64.b64encode(out_file.getvalue())    def decrypt(self, content=None) -&gt; bytes:        &quot;&quot;&quot;        密文 content 可以传入，使用实例对象的密码，返回解密后的明文 bytes        错误的密码、salt_header、key_size解密将报错        openssl 加密命令            从文件：openssl aes-256-cbc -salt -in text.txt -a -k &#x27;password&#x27;            从输入：echo &quot;abc&quot; | openssl aes-256-cbc -salt -a -k &#x27;password&#x27;        &quot;&quot;&quot;        content = content if content else self.content        content = content.encode() if not isinstance(content, bytes) else content        text = base64.b64decode(content)        # 字节IO模拟文件        in_file = io.BytesIO(text)        out_file = io.BytesIO()        salt = in_file.read(self.block_size)[len(self.salt_header):]        key, iv = self.derive_key_and_iv(salt)        cipher = AES.new(key, AES.MODE_CBC, iv)        next_chunk = b&#x27;&#x27;        finished = False        while not finished:            chunk, next_chunk = next_chunk, cipher.decrypt(in_file.read(1024 * self.block_size))            if len(next_chunk) == 0:                padding_length = chunk[-1]                if padding_length &lt; 1 or padding_length &gt; self.block_size:                    # 触发此错误原因可能是密码错误                    raise ValueError(&quot;bad decrypt pad (%d)&quot; % padding_length)                chunk = chunk[:-padding_length]                finished = True            out_file.write(chunk)        # 若为空可能是密码错误了，或者触发上面的异常        return out_file.getvalue()if __name__ == &#x27;__main__&#x27;:    print(&quot;++++++++++++++++++++++++++++++++++++++++++&quot;)    aes = AESCipher(&quot;hello world2223&quot;, &#x27;111111&#x27;)    secret = aes.encrypt()    raw = aes.decrypt(secret)    print(&quot;密文:&quot;, secret)    print(&quot;明文:&quot;, raw)    print(&quot;++++++++++++++++++++++++++++++++++++++++++&quot;)    try:        # 错误的密码解密将报错        aes2 = AESCipher(&quot;&quot;, &#x27;222&#x27;, salt_header=&#x27;&#x27;)        res = aes2.decrypt(secret)        print(res)    except ValueError:        print(&quot;密码错了！&quot;)","categories":["Python"]},{"title":"kubernetes-1.15.3-installation","url":"/2019/10/27/kubernetes-1-15-3-installation/","content":"一、环境准备\nkubernetes 二进制安装\n\n1. 集群信息\n系统：CentOS Linux release 7.6.1810 (Core) \n内核：3.10.0-957.27.2.el7.x86_64\n\n机器和服务分部，master 也计划安装上 kubelet、kube-proxy、docker、flannel 这些\n\n\n\n\n\n主机名\nIP\n集群角色\n服务\n\n\n\nmaster-01\n172.16.10.20\nmaster\netcd、Apiserver、ControllerManager、Schedulerdocker、flannel、kube-proxy、kubelet\n\n\nworker-01\n172.16.10.25\nnode\netcd、kubelet、kube-proxy、docker、flannel\n\n\nworker-02\n172.16.10.26\nnode\netcd、kubelet、kube-proxy、docker、flannel\n\n\nkubernetes 集群规划\n\n\n\n配置项\n值\n\n\n\nCluster CIDR\n10.66.0.0&#x2F;24\n\n\n容器网段（ flannel 大网段 ）\n10.99.0.0&#x2F;16\n\n\n2. 配置系统\n请绑定主机名、配置 master-01 到其他机器的 ssh 等效性，集群机器都执行\n\nsystemctl stop firewalldsystemctl disable firewalldsetenforce 0sed -i &#x27;/^SELINUX=/s/enforcing/disabled/&#x27; /etc/selinux/config# 关闭swap，若有请注释fstab中的swap挂载配置swapoff -a sysctl -w vm.swappiness=0# 加载内核模块modprobe br_netfilter# 安装docker-ce-18.09，用阿里云的源，先不启动，安装完flannel配置后再启动yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum install -y docker-ce-18.09.6-3.el7\n\n\n\n3. 软件包\n这里存放在 master-01 的 &#x2F;repo 中共享给其他机器\n\n\nkubernetes 1.15.3 \netcd 3.3.15\nflannel 0.11.0\ndocker-ce 18.09 后面 yum 安装\n\n二、创建证书创建一些目录\nmkdir -p ~/ssl/&#123;k8s,etcd&#125;mkdir -p /usr/local/kubernetes/binmkdir -p /etc/kubernetesmkdir -p /etc/&#123;etcd,kubernetes&#125;/sslecho &#x27;export PATH=$PATH:/usr/local/kubernetes/bin/&#x27; &gt;&gt; /etc/profilesource /etc/profile\n\n下载 cfssl\nwget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64mv cfssl_linux-amd64 /usr/local/bin/cfsslmv cfssljson_linux-amd64 /usr/local/bin/cfssljsonmv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo\n\n\n\n1. 创建 etcd 证书cd ~/ssl/etcd# 创建 etcd 证书cat &lt;&lt; EOF | tee ca-config.json&#123;  &quot;signing&quot;: &#123;    &quot;default&quot;: &#123;      &quot;expiry&quot;: &quot;87600h&quot;    &#125;,    &quot;profiles&quot;: &#123;      &quot;www&quot;: &#123;         &quot;expiry&quot;: &quot;87600h&quot;,         &quot;usages&quot;: [            &quot;signing&quot;,            &quot;key encipherment&quot;,            &quot;server auth&quot;,            &quot;client auth&quot;        ]      &#125;    &#125;  &#125;&#125;# 创建 CA 配置文件cat &lt;&lt; EOF | tee ca-csr.json&#123;    &quot;CN&quot;: &quot;etcd CA&quot;,    &quot;key&quot;: &#123;        &quot;algo&quot;: &quot;rsa&quot;,        &quot;size&quot;: 2048    &#125;,    &quot;names&quot;: [        &#123;            &quot;C&quot;: &quot;CN&quot;,            &quot;L&quot;: &quot;Guangzhou&quot;,            &quot;ST&quot;: &quot;Guangzhou&quot;        &#125;    ]&#125;EOF# 创建 server 证书cat &lt;&lt; EOF | tee server-csr.json&#123;    &quot;CN&quot;: &quot;etcd&quot;,    &quot;hosts&quot;: [    &quot;172.16.10.20&quot;,    &quot;172.16.10.25&quot;,    &quot;172.16.10.26&quot;    ],    &quot;key&quot;: &#123;        &quot;algo&quot;: &quot;rsa&quot;,        &quot;size&quot;: 2048    &#125;,    &quot;names&quot;: [        &#123;            &quot;C&quot;: &quot;CN&quot;,            &quot;L&quot;: &quot;Guangzhou&quot;,            &quot;ST&quot;: &quot;Guangzhou&quot;        &#125;    ]&#125;EOF# 生成 pem 证书cfssl gencert -initca ca-csr.json | cfssljson -bare ca -cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server\n\n\n\n2. 创建 kubernetes 证书进入准备的临时目录\ncd ~/ssl/k8s\n\n\n\n2.1 生成 CA 证书cat &lt;&lt; EOF | tee ca-config.json&#123;  &quot;signing&quot;: &#123;    &quot;default&quot;: &#123;      &quot;expiry&quot;: &quot;87600h&quot;    &#125;,    &quot;profiles&quot;: &#123;      &quot;kubernetes&quot;: &#123;         &quot;expiry&quot;: &quot;87600h&quot;,         &quot;usages&quot;: [            &quot;signing&quot;,            &quot;key encipherment&quot;,            &quot;server auth&quot;,            &quot;client auth&quot;        ]      &#125;    &#125;  &#125;&#125;EOFcat &lt;&lt; EOF | tee ca-csr.json&#123;    &quot;CN&quot;: &quot;kubernetes&quot;,    &quot;key&quot;: &#123;        &quot;algo&quot;: &quot;rsa&quot;,        &quot;size&quot;: 2048    &#125;,    &quot;names&quot;: [        &#123;            &quot;C&quot;: &quot;CN&quot;,            &quot;L&quot;: &quot;Guangzhou&quot;,            &quot;ST&quot;: &quot;Guangzhou&quot;,            &quot;O&quot;: &quot;k8s&quot;,            &quot;OU&quot;: &quot;System&quot;        &#125;    ]&#125;EOF# 生成证书cfssl gencert -initca ca-csr.json | cfssljson -bare ca -\n\n\n\n2.2 生成 API Server 证书\n10.66.0.1 是计划在Apiserver中指定的 service-cluster-ip-range 网段第一个 ip\n\ncat &lt;&lt; EOF | tee server-csr.json&#123;    &quot;CN&quot;: &quot;kubernetes&quot;,    &quot;hosts&quot;: [        &quot;127.0.0.1&quot;,        &quot;10.66.0.1&quot;,        &quot;172.16.10.20&quot;,        &quot;172.16.10.25&quot;,        &quot;172.16.10.26&quot;,        &quot;kubernetes&quot;,        &quot;kubernetes.default&quot;,        &quot;kubernetes.default.svc&quot;,        &quot;kubernetes.default.svc.cluster&quot;,        &quot;kubernetes.default.svc.cluster.local&quot;    ],    &quot;key&quot;: &#123;        &quot;algo&quot;: &quot;rsa&quot;,        &quot;size&quot;: 2048    &#125;,    &quot;names&quot;: [        &#123;            &quot;C&quot;: &quot;CN&quot;,            &quot;L&quot;: &quot;Guangzhou&quot;,            &quot;ST&quot;: &quot;Guangzhou&quot;,            &quot;O&quot;: &quot;k8s&quot;,            &quot;OU&quot;: &quot;System&quot;        &#125;    ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server\n\n\n\n2.3 创建 Kube Proxy 证书cat &lt;&lt; EOF | tee kube-proxy-csr.json&#123;  &quot;CN&quot;: &quot;system:kube-proxy&quot;,  &quot;hosts&quot;: [],  &quot;key&quot;: &#123;    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  &#125;,  &quot;names&quot;: [    &#123;      &quot;C&quot;: &quot;CN&quot;,      &quot;L&quot;: &quot;Guangzhou&quot;,      &quot;ST&quot;: &quot;Guangzhou&quot;,      &quot;O&quot;: &quot;k8s&quot;,      &quot;OU&quot;: &quot;System&quot;    &#125;  ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy\n\n\n\n2.4 创建 Admin 证书客户端连接 apiserver 的证书\ncat &lt;&lt; EOF | tee admin-csr.json &#123;  &quot;CN&quot;: &quot;admin&quot;,  &quot;hosts&quot;: [],  &quot;key&quot;: &#123;    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  &#125;,  &quot;names&quot;: [    &#123;      &quot;C&quot;: &quot;CN&quot;,      &quot;ST&quot;: &quot;Guangzhou&quot;,      &quot;L&quot;: &quot;Guangzhou&quot;,      &quot;O&quot;: &quot;system:masters&quot;,      &quot;OU&quot;: &quot;System&quot;    &#125;  ]&#125;# 生成证书cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin\n\n\n\n三、安装 etcd\n集群中其他节点除配置文件外内容一致\n\n1. 准备解压 etcd 二进制文件到 $PATH中，复制证书到准备的目录\ncp ~/ssl/etcd/*pem /etc/etcd/sslcd /repotar xf etcd-v3.3.15-linux-amd64.tar.gz # 其他节点记得复制并创建 etcd 目录cp -a etcd-v3.3.15-linux-amd64/etcd* /usr/local/kubernetes/bin/mkdir -p /var/lib/etcd/\n\n\n\n2. 修改配置配置文件 /etc/etcd/etcd.conf ，其他节点除 ETCD_INITIAL_CLUSTER 外的 IP 需要改成自己的\ncat /etc/etcd/etcd.conf ETCD_DATA_DIR=&quot;/var/lib/etcd/&quot;ETCD_LISTEN_PEER_URLS=&quot;https://172.16.10.20:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://172.16.10.20:2379,http://127.0.0.1:2379&quot;# 另外的节点请修改为 infra2、infra3，对应 ETCD_INITIAL_CLUSTERETCD_NAME=&quot;infra1&quot;ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://172.16.10.20:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://172.16.10.20:2379&quot;# 与节点的 ETCD_NAME 对应，三个节点这个配置是相同的ETCD_INITIAL_CLUSTER=&quot;infra1=https://172.16.10.20:2380,infra2=https://172.16.10.25:2380,infra3=https://172.16.10.26:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;ETCD_CERT_FILE=&quot;/etc/etcd/ssl/server.pem&quot;ETCD_KEY_FILE=&quot;/etc/etcd/ssl/server-key.pem&quot;ETCD_TRUSTED_CA_FILE=&quot;/etc/etcd/ssl/ca.pem&quot;ETCD_PEER_CERT_FILE=&quot;/etc/etcd/ssl/server.pem&quot;ETCD_PEER_KEY_FILE=&quot;/etc/etcd/ssl/server-key.pem&quot;ETCD_PEER_TRUSTED_CA_FILE=&quot;/etc/etcd/ssl/ca.pem&quot;\n\n配置 /usr/lib/systemd/system/etcd.service 启动文件，集群中其他节点也是一样\n[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/etc/etcd/etcd.confExecStart=/bin/bash -c &quot;GOMAXPROCS=$(nproc) /usr/local/kubernetes/bin/etcd \\    --name=\\&quot;$&#123;ETCD_NAME&#125;\\&quot; \\    --cert-file=\\&quot;$&#123;ETCD_CERT_FILE&#125;\\&quot; \\    --key-file=\\&quot;$&#123;ETCD_KEY_FILE&#125;\\&quot; \\    --peer-cert-file=\\&quot;$&#123;ETCD_PEER_CERT_FILE&#125;\\&quot; \\    --peer-key-file=\\&quot;$&#123;ETCD_PEER_KEY_FILE&#125;\\&quot; \\    --trusted-ca-file=\\&quot;$&#123;ETCD_TRUSTED_CA_FILE&#125;\\&quot; \\    --peer-trusted-ca-file=\\&quot;$&#123;ETCD_PEER_TRUSTED_CA_FILE&#125;\\&quot; \\    --initial-advertise-peer-urls=\\&quot;$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125;\\&quot; \\    --listen-peer-urls=\\&quot;$&#123;ETCD_LISTEN_PEER_URLS&#125;\\&quot; \\    --listen-client-urls=\\&quot;$&#123;ETCD_LISTEN_CLIENT_URLS&#125;\\&quot; \\    --advertise-client-urls=\\&quot;$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125;\\&quot; \\    --initial-cluster-token=\\&quot;$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125;\\&quot; \\    --initial-cluster=\\&quot;$&#123;ETCD_INITIAL_CLUSTER&#125;\\&quot; \\    --initial-cluster-state=\\&quot;$&#123;ETCD_INITIAL_CLUSTER_STATE&#125;\\&quot; \\    --data-dir=\\&quot;$&#123;ETCD_DATA_DIR&#125;\\&quot;&quot;# 如果上一个配置文件错了，并且启动失败，可以尝试把下面两行注释阻止继续重启，修复后改回来Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target\n\n复制配置文件到集群其他节点\n# 复制完需要修改配置文件scp -r /etc/etcd worker-01:/etc/scp -r /etc/etcd worker-02:/etc/scp /usr/lib/systemd/system/etcd.service worker-01:/usr/lib/systemd/system/etcd.servicescp /usr/lib/systemd/system/etcd.service worker-02:/usr/lib/systemd/system/etcd.service\n\n\n\n3. 启动和测试启动 etcd 服务（注意这里需要有两个节点一起启动，单机启动会卡住）\nsystemctl daemon-reloadsystemctl enable etcdsystemctl start etcd\n\n测试 etcd 集群状态，因为配置了证书所以需要指定很多证书路径参数，为了后面调试可以 alias 一个别名\netcdctl \\--ca-file=/etc/etcd/ssl/ca.pem \\--cert-file=/etc/etcd/ssl/server.pem \\--key-file=/etc/etcd/ssl/server-key.pem \\--endpoints=&quot;https://172.16.10.20:2379,\\https://172.16.10.25:2379,\\https://172.16.10.26:2379&quot; \\cluster-health# 正常输出 cluster is healthymember b15ab296f41fb90 is healthy: got healthy result from https://172.16.10.20:2379member 421127a297dd866e is healthy: got healthy result from https://172.16.10.25:2379member b7f5cc8d67090480 is healthy: got healthy result from https://172.16.10.26:2379cluster is healthy# etcdctl别名加上参数alias etcdctl=&#x27;etcdctl \\--ca-file=/etc/etcd/ssl/ca.pem \\--cert-file=/etc/etcd/ssl/server.pem \\--key-file=/etc/etcd/ssl/server-key.pem \\--endpoints=&quot;https://172.16.10.20:2379,\\https://172.16.10.25:2379,\\https://172.16.10.26:2379&quot; &#x27;# 再次尝试，后面有其他步骤用 etcdctl 命令配置数据时，将直接用这种方式etcdctl cluster-health  \n\n有上面的 cluster is healthy 说明我们的 etcd  集群就完成了\n四、安装 flannel\nworker 节点都安装 flannel ，它在集群中的作用是让不同 docker host 之间的容器互相通信\n\n1. 准备解压软件包\ncd /repotar xf flannel-v0.11.0-linux-amd64.tar.gzcp -a flanneld mk-docker-opts.sh /usr/local/kubernetes/bin/# 发送到 worker 节点scp  flanneld mk-docker-opts.sh worker-01:/usr/local/kubernetes/bin/scp  flanneld mk-docker-opts.sh worker-02:/usr/local/kubernetes/bin/\n\n\n\n2. 修改配置\n10.99.0.0&#x2F;16 对应我们最开始规划的 Cluster IP，这个子网必须是 16 位地址\n\n创建网络配置，所有的 flannel 节点会在 10.99.0.0&#x2F;16 下创建一个 24 位的子网，作为本机的网段，将在后面提供给 docker 中运行的 Pod 使用\n这里使用 host-gw 的方式，另外可以配置 vxlan ，可以详细了解它们的差异\n# 创建网络配置，&quot;/kubernetes/network/config&quot;将在 flannel 配置文件中用到etcdctl mk /kubernetes/network/config \\&#x27;&#123;&quot;Network&quot;:&quot;10.99.0.0/16&quot;,&quot;SubnetLen&quot;:24,&quot;Backend&quot;:&#123;&quot;Type&quot;:&quot;host-gw&quot;&#125;&#125;&#x27;\n\n修改 /etc/sysconfig/flanneld 配置文件，指定 etcd 地址和证书位置\n\n如果是多网卡环境，则需要在 FLANNEL_OPTIONS 中增加指定的外网出口的网卡，例如 -iface=ens33\n\n# Flanneld configuration options  # etcd url location.  Point this to the server where etcd runsFLANNEL_ETCD_ENDPOINTS=&quot;https://172.16.10.20:2379,https://172.16.10.25:2379,https://172.16.10.26:2379&quot;# 上面配置网络时，存在 etcd 中的 key 名FLANNEL_ETCD_PREFIX=&quot;/kubernetes/network&quot;# Any additional options that you want to passFLANNEL_OPTIONS=&quot;-etcd-cafile=/etc/etcd/ssl/ca.pem -etcd-certfile=/etc/etcd/ssl/server.pem -etcd-keyfile=/etc/etcd/ssl/server-key.pem&quot;\n\n\n\n配置 /usr/lib/systemd/system/flanneld.service 启动文件\n\nflannel 启动后将执行 mk-docker-opts.sh 把网段信息写入到 cat /run/flannel/docker 中，后面docker启动的时候会按文件中的变量配置 docker0 网桥\n运行需要 root 权限\n\n[Unit]Description=Flanneld overlay address etcd agentAfter=network.targetAfter=network-online.targetWants=network-online.targetAfter=etcd.serviceBefore=docker.service[Service]Type=notifyEnvironmentFile=/etc/sysconfig/flanneldEnvironmentFile=-/etc/sysconfig/docker-networkExecStart=/usr/local/kubernetes/bin/flanneld -etcd-endpoints=$&#123;FLANNEL_ETCD_ENDPOINTS&#125; \\            -etcd-prefix=$&#123;FLANNEL_ETCD_PREFIX&#125; $FLANNEL_OPTIONSExecStartPost=/usr/local/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/dockerRestart=on-failure[Install]WantedBy=multi-user.targetWantedBy=docker.service\n\n发送配置到 worker 节点\nscp /etc/sysconfig/flanneld worker-01:/etc/sysconfig/ scp /etc/sysconfig/flanneld worker-02:/etc/sysconfig/ scp /usr/lib/systemd/system/flanneld.service worker-01:/usr/lib/systemd/system/scp /usr/lib/systemd/system/flanneld.service worker-02:/usr/lib/systemd/system/\n\n\n\n3. 启动和测试启动所有节点中的 flannel 服务\nsystemctl daemon-reloadsystemctl start flanneld.servicesystemctl enable flanneld.service \n\n再次看下 etcd 中的变化\netcdctl ls /kubernetes/network/subnets# 输出如下，这里三个节点都安装有 flannel/kubernetes/network/subnets/10.99.76.0-24/kubernetes/network/subnets/10.99.41.0-24/kubernetes/network/subnets/10.99.59.0-24# 这里再看 master-01 的两个 flannel 文件，和 etcd 中生成的对应，docker 服务将要用这里的变量cat /run/flannel/subnet.env     FLANNEL_NETWORK=10.99.0.0/16FLANNEL_SUBNET=10.99.41.1/24FLANNEL_MTU=1500FLANNEL_IPMASQ=falsecat /run/flannel/docker DOCKER_OPT_BIP=&quot;--bip=10.99.41.1/24&quot;DOCKER_OPT_IPMASQ=&quot;--ip-masq=true&quot;DOCKER_OPT_MTU=&quot;--mtu=1500&quot;DOCKER_NETWORK_OPTIONS=&quot; --bip=10.99.41.1/24 --ip-masq=true --mtu=1500&quot;\n\n\n\n4. 配置 docker 服务前面已经准备了 flannel 给集群中的 docker host 提供网络通信，现在需要配置 docker 服务把 flannel 的网络配置应用上\n\n--exec-opt 选项指定了 cgroup 的驱动用 systemd ，和后面 kubelet 需要一致\n$DOCKER_NETWORK_OPTIONS 是在 /var/run/flannel/docker 文件中的配置\n另外加了一个阿里云的加速地址 --registry-mirror\n\n[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comBindsTo=containerd.serviceAfter=network-online.target firewalld.service containerd.serviceWants=network-online.targetRequires=docker.socket[Service]Type=notify# the default is not to use systemd for cgroups because the delegate issues still# exists and systemd currently does not support the cgroup feature set required# for containers run by dockerEnvironmentFile=-/var/run/flannel/dockerEnvironmentFile=-/var/run/flannel/subnet.envExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock \\          $DOCKER_NETWORK_OPTIONS \\          --exec-opt native.cgroupdriver=systemd \\          --registry-mirror=https://1u1w02a0.mirror.aliyuncs.com ExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=always# Note that StartLimit* options were moved from &quot;Service&quot; to &quot;Unit&quot; in systemd 229.# Both the old, and new location are accepted by systemd 229 and up, so using the old location# to make them work for either version of systemd.StartLimitBurst=3# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make# this option work for either version of systemd.StartLimitInterval=60s# Having non-zero Limit*s causes performance problems due to accounting overhead# in the kernel. We recommend using cgroups to do container-local accounting.LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinity# Comment TasksMax if your systemd version does not supports it.# Only systemd 226 and above support this option.TasksMax=infinity# set delegate yes so that systemd does not reset the cgroups of docker containersDelegate=yes# kill only the docker process, not all processes in the cgroupKillMode=process[Install]WantedBy=multi-user.target\n\n复制 systemd 文件到 worker 节点\nscp /usr/lib/systemd/system/docker.service worker-01:/usr/lib/systemd/system/scp /usr/lib/systemd/system/docker.service worker-02:/usr/lib/systemd/system/\n\n启动 docker 服务\nsystemctl daemon-reloadsystemctl enable dockersystemctl start docker\n\n以 master-01 为例，检查 docker 生成的网桥地址\nip addr show docker0 # 输出如下3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default     link/ether 02:42:d8:f9:f0:6b brd ff:ff:ff:ff:ff:ff    inet 10.99.41.1/24 brd 10.99.41.255 scope global docker0       valid_lft forever preferred_lft forever\n\n可以看到 10.99.41.255 与 flannel 的子网一致，到这里 docker 与 flannel 就配置完了，docker 应用上了 flannel 生成的网络信息\n五、安装 master 节点\n操作将在 master-01 中执行\n\n在 kubernetes 中， master 节点包含组件：\n\nkube-apiserver\nkube-scheduler\nkube-controller-manager\n\n除apiserver外都可以集群模式运行，并且通过选举产生一个工作进程，其他进程阻塞\n这里的环境 master 节点有计划安装 kubelet 这些 worker 节点的进程，参考配置worker 节点部分说明\n1. 准备解压准备好的二进制包，复制到 /usr/local/kubernetes/bin/ 下\ncd /repotar xf kubernetes-server-linux-amd64.tar.gz cp kubernetes/server/bin/kubectl /usr/local/kubernetes/bin/cp kubernetes/server/bin/kube-apiserver /usr/local/kubernetes/bin/cp kubernetes/server/bin/kube-scheduler /usr/local/kubernetes/bin/cp kubernetes/server/bin/kube-controller-manager /usr/local/kubernetes/bin/cp kubernetes/server/bin/kubelet /usr/local/kubernetes/bin/                cp kubernetes/server/bin/kube-proxy /usr/local/kubernetes/bin/# 复制证书文件cp ~/ssl/k8s/*.pem /etc/kubernetes/ssl/\n\n\n\n2. apiserver创建 TLS Bootstrap Token，用于 Token 认证\necho &quot;$(head -c 16 /dev/urandom | od -An -t x | tr -d &#x27; &#x27;),kubelet-bootstrap,10001,system:kubelet-bootstrap&quot; &gt; /etc/kubernetes/token.csv# 文件内容类似47ec1167391bc238ccdd44367465eb08,kubelet-bootstrap,10001,system:kubelet-bootstrap\n\n创建配置文件 /etc/kubernetes/kube-apiserver \nKUBE_APISERVER_OPTS=&quot;--logtostderr=true \\--v=4 \\--etcd-servers=https://172.16.10.20:2379,https://172.16.10.25:2379,https://172.16.10.26:2379 \\--bind-address=172.16.10.20 \\--secure-port=6443 \\--advertise-address=172.16.10.20 \\--allow-privileged=true \\--service-cluster-ip-range=10.66.0.0/24 \\--enable-admission-plugins=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \\--authorization-mode=RBAC,Node \\--enable-bootstrap-token-auth \\--token-auth-file=/etc/kubernetes/token.csv \\--service-node-port-range=30000-50000 \\--tls-cert-file=/etc/kubernetes/ssl/server.pem \\--tls-private-key-file=/etc/kubernetes/ssl/server-key.pem \\--client-ca-file=/etc/kubernetes/ssl/ca.pem \\--service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\--etcd-cafile=/etc/etcd/ssl/ca.pem \\--etcd-certfile=/etc/etcd/ssl/server.pem \\--etcd-keyfile=/etc/etcd/ssl/server-key.pem \\--kubelet-client-certificate=/etc/kubernetes/ssl/server.pem \\--kubelet-client-key=/etc/kubernetes/ssl/server-key.pem&quot;\n\n创建 systemd 文件 /usr/lib/systemd/system/kube-apiserver.service\n[Unit]Description=Kubernetes API ServiceDocumentation=https://github.com/kubernetes/kubernetesAfter=network.targetAfter=etcd.service[Service]EnvironmentFile=-/etc/kubernetes/kube-apiserverExecStart=/usr/local/kubernetes/bin/kube-apiserver $KUBE_APISERVER_OPTSRestart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target\n\n启动 apiserver ，注意看下状态\nsystemctl daemon-reloadsystemctl enable kube-apiserversystemctl start kube-apiserversystemctl status kube-apiserver\n\n\n\n3. scheduler创建配置文件 /etc/kubernetes/kube-scheduler\nKUBE_SCHEDULER_OPTS=&quot;--logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect&quot;\n\n创建 systemd 文件 /usr/lib/systemd/system/kube-scheduler.service\n[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/schedulerExecStart=/usr/local/kubernetes/bin/kube-scheduler $KUBE_SCHEDULER_OPTSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target\n\n启动 scheduler 服务\nsystemctl daemon-reloadsystemctl start kube-schedulersystemctl enable kube-schedulersystemctl status kube-scheduler\n\n\n\n4. controller-manager创建配置文件 /etc/kubernetes/kube-controller-manager \nKUBE_CONTROLLER_MANAGER_OPTS=&quot;--logtostderr=true \\--v=4 \\--master=127.0.0.1:8080 \\--leader-elect=true \\--address=127.0.0.1 \\--service-cluster-ip-range=10.66.0.0/24 \\--cluster-name=kubernetes \\--cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\--cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\--root-ca-file=/etc/kubernetes/ssl/ca.pem \\--service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem&quot;\n\n创建 systemd 文件 /usr/lib/systemd/system/kube-controller-manager.service\n[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/kube-controller-managerExecStart=/usr/local/kubernetes/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target\n\n启动服务\nsystemctl daemon-reloadsystemctl start kube-controller-managersystemctl enable kube-controller-managersystemctl status kube-controller-manager\n\n\n\n5. 检查在 master 中用 kubectl 命令查看集群信息\n# kubectl cluster-info    Kubernetes master is running at http://localhost:8080To further debug and diagnose cluster problems, use &#x27;kubectl cluster-info dump&#x27;.\n\n查看集群的服务状态\n# kubectl get cs    NAME                 STATUS    MESSAGE             ERRORcontroller-manager   Healthy   ok                  scheduler            Healthy   ok                  etcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   etcd-2               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   etcd-1               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   \n\n\n\n6. 其他\nkubectl 命令补全，支持 zsh 和 bash，在bash中请先安装 bash-completion ，然后执行如下命令\nkubectl completion bash &gt; /etc/bash_completion.d/kubectl.bashsource /etc/bash_completion.d/kubectl.bash \n\nkubectl 常用的查看命令\n\n-n 指定命名空间\n-o 指定输出，wide显示更多的信息，支持 json 和 yaml 输出\n\n# kubectl get namespaces kubectl get nodes -o widekubectl get nodes -n defaultkubectl get podskubectl get rskubectl get deployments# 查看详情，同样适用其他资源kubectl describe nodes NODE_NAME\n\n六、安装 worker 节点\n在 worker-01 和 worker-02 中安装，二进制包资源挂载了 master-01 的共享在 &#x2F;repo 目录\nmaster 节点也可以安装\n\nworker 节点运行 kubernetes 中的两个组件：\n\nkubelet\nkube-proxy\n\n另外 worker 节点是集群工作的主力，所以之前也安装有 docker ，并且搭配了 flannel，现在准备开始安装上面两个未安装的组件\n两个 worker 节点先复制组件的二进制文件到 /usr/local/kubernetes/bin/\ncp -a /repo/kubernetes/server/bin/&#123;kubelet,kube-proxy,kubectl&#125; /usr/local/kubernetes/bin/\n\n\n\n1. 创建 kubelet bootstrap 文件在 master 中执行，用到之前生成的 token ，再看一下它\ncat /etc/kubernetes/token.csv47ec1167391bc238ccdd44367465eb08,kubelet-bootstrap,10001,system:kubelet-bootstrap\n\n下面命令有个 token 变量是上面文件的 token 部分\n# token.csvBOOTSTRAP_TOKEN=47ec1167391bc238ccdd44367465eb08KUBE_APISERVER=&quot;https://172.16.10.20:6443&quot;BOOTSTRAP_TOKEN=c2ea7340a258997471f1dba5c0cef395KUBE_APISERVER=&quot;https://172.16.10.20:6443&quot;cd /etc/kubernetes/ssl# 设置集群参数kubectl config set-cluster kubernetes --certificate-authority=./ca.pem \\    --embed-certs=true --server=$&#123;KUBE_APISERVER&#125; \\    --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \\    --token=$&#123;BOOTSTRAP_TOKEN&#125; \\    --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context default \\    --cluster=kubernetes \\    --user=kubelet-bootstrap \\    --kubeconfig=bootstrap.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig# 创建kube-proxy kubeconfig文件kubectl config set-cluster kubernetes \\    --certificate-authority=./ca.pem \\    --embed-certs=true \\    --server=$&#123;KUBE_APISERVER&#125; \\    --kubeconfig=kube-proxy.kubeconfigkubectl config set-credentials kube-proxy \\    --client-certificate=./kube-proxy.pem \\    --client-key=./kube-proxy-key.pem \\    --embed-certs=true \\    --kubeconfig=kube-proxy.kubeconfigkubectl config set-context default \\    --cluster=kubernetes \\    --user=kube-proxy \\    --kubeconfig=kube-proxy.kubeconfigkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig\n\n将 kubelet-bootstrap 用户绑定到系统集群角色\nkubectl create clusterrolebinding kubelet-bootstrap \\  --clusterrole=system:node-bootstrapper \\  --user=kubelet-bootstrap\n\n将 master-01 中的 /etc/kubernetes/ssl 复制到 worker 节点\nscp -r /etc/kubernetes/ssl/ worker-01:/etc/kubernetes/scp -r /etc/kubernetes/ssl/ worker-02:/etc/kubernetes/\n\n\n\n2. 配置 kubelet\n未说明将是在 worker 节点执行命令，两个 worker 节点都要改的喔，这里以 worker-01 为例\n\n创建 kubelet 参数配置文件 /etc/kubernetes/kubelet.config 另外的机器修改 address 为自己 IP，其他一样\n# YAMLkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 172.16.10.25port: 10250readOnlyPort: 10255cgroupDriver: cgroupfsclusterDNS: [&quot;10.66.0.2&quot;]clusterDomain: cluster.local.failSwapOn: falseauthentication:  anonymous:    enabled: true\n\n创建配置文件 /etc/kubernetes/kubelet ，hostname-override 修改成本机 IP\nKUBELET_OPTS=&quot;--logtostderr=true \\--v=4 \\--hostname-override=172.16.10.25 \\--kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\--bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\--config=/etc/kubernetes/kubelet.config \\--cert-dir=/etc/kubernetes/ssl \\--pod-infra-container-image=k8s.gcr.io/pause-amd64:3.1 \\--client-ca-file=/etc/kubernetes/ssl/ca.pem&quot;\n\n\n/etc/kubernetes/kubelet.kubeconfig 文件开始是不存在的，等后面 master 通过 CSR 后，会产生它和 ssl 目录下的一些证书\nk8s.gcr.io/pause-amd64:3.1 是 Pod 的基础镜像，在谷歌的 gcr 仓库，在 kubelet 启动的时候会从镜像仓库拉取，正常应该无法拉取到。这里同步了一份，可以去这里下载 k8s.gcr.io_pause-amd64_3.1.tgz ，下载后先 docker load -i k8s.gcr.io_pause-amd64_3.1.tgz 导入节点的 docker 中，再启动 kubelet 。\n\n\n--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0 阿里云的镜像\n\n创建 systemd 文件 /usr/lib/systemd/system/kubelet.service\n[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/kubernetes/kubernetesAfter=docker.serviceRequires=docker.service[Service]EnvironmentFile=/etc/kubernetes/kubeletExecStart=/usr/local/kubernetes/bin/kubelet $KUBELET_OPTSRestart=on-failureKillMode=process[Install]WantedBy=multi-user.target\n\n启动服务\nsystemctl daemon-reloadsystemctl enable kubeletsystemctl start kubeletsystemctl status kubelet\n\n\n\n3. 通过 kubelet CSR 请求如果无异常，现在 kubelet 已经运行起来了，kubelet 通过 /etc/kubernetes/ssl/bootstrap.kubeconfig 知道怎么去连集群 master 上的 apiserver ，这时候它就会请求加入集群\n查看并批准 kubelet 的 CSR 请求\n# kubectl get csrNAME                                                   AGE     REQUESTOR           CONDITIONnode-csr-73s9OQf5HNcjoD7RPIJsrwXsrx4VPC89lKZdhK5i_Mk   11m     kubelet-bootstrap   Pendingnode-csr-U5TpUTOu5nR_L-8Ooccbv5hj4LKEeLsMzpFkbvC8UII   5m41s   kubelet-bootstrap   Pending# 批准对应NAME的CSR# kubectl certificate approve node-csr-73s9OQf5HNcjoD7RPIJsrwXsrx4VPC89lKZdhK5i_Mkcertificatesigningrequest.certificates.k8s.io/node-csr-73s9OQf5HNcjoD7RPIJsrwXsrx4VPC89lKZdhK5i_Mk approved# kubectl certificate approve node-csr-U5TpUTOu5nR_L-8Ooccbv5hj4LKEeLsMzpFkbvC8UIIcertificatesigningrequest.certificates.k8s.io/node-csr-U5TpUTOu5nR_L-8Ooccbv5hj4LKEeLsMzpFkbvC8UII approved# 再次查看CSR已经通过了# kubectl get csrNAME                                                   AGE     REQUESTOR           CONDITIONnode-csr-73s9OQf5HNcjoD7RPIJsrwXsrx4VPC89lKZdhK5i_Mk   13m     kubelet-bootstrap   Approved,Issuednode-csr-U5TpUTOu5nR_L-8Ooccbv5hj4LKEeLsMzpFkbvC8UII   7m31s   kubelet-bootstrap   Approved,Issued\n\n查看集群中的节点，发现无法看出哪个是 master 哪个是 worker 节点，ROLES 都是 none（据说是手动安装的问题，kubeadm会有 master 的角色标记）\n其实 ROLES 也是一个 LABEL 不过是一个特殊的 LABEL，看下如何添加\n# kubectl get nodes NAME           STATUS   ROLES    AGE   VERSION172.16.10.20   Ready    &lt;none&gt;   58m   v1.15.3172.16.10.25   Ready    &lt;none&gt;   62m   v1.15.3172.16.10.26   Ready    &lt;none&gt;   62m   v1.15.3# 尝试将 worker-01 的 roles 改成 worker# kubectl label node 172.16.10.25 node-role.kubernetes.io/worker=workernode/172.16.10.25 labeled# kubectl get nodes NAME           STATUS   ROLES    AGE   VERSION172.16.10.20   Ready    &lt;none&gt;   63m   v1.15.3172.16.10.25   Ready    worker   67m   v1.15.3172.16.10.26   Ready    &lt;none&gt;   67m   v1.15.3# 另外的节点也改一下# kubectl label node 172.16.10.26 node-role.kubernetes.io/worker=worker# kubectl label node 172.16.10.20 node-role.kubernetes.io/master=master# kubectl get nodes NAME           STATUS   ROLES    AGE   VERSION172.16.10.20   Ready    master   65m   v1.15.3172.16.10.25   Ready    worker   69m   v1.15.3172.16.10.26   Ready    worker   69m   v1.15.3# 如果配置错了，比如把worker-02 26这个节点误操作标记成了 master，可以如下操作清除，再重新标记# kubectl label node 172.16.10.26 node-role.kubernetes.io/master-\n\n\n\n4. 配置 kube-proxy\nworker 节点安装\n\n创建配置文件 /etc/kubernetes/kube-proxy \nKUBE_PROXY_OPTS=&quot;--logtostderr=true \\--v=4 \\--hostname-override=172.16.10.25 \\--cluster-cidr=10.66.0.0/24 \\--kubeconfig=/etc/kubernetes/ssl/kube-proxy.kubeconfig&quot;\n\n创建systemd 文件 /usr/lib/systemd/system/kube-proxy.service\n[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/kube-proxyExecStart=/usr/local/kubernetes/bin/kube-proxy $KUBE_PROXY_OPTSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target\n\n启动服务\nsystemctl daemon-reloadsystemctl enable kube-proxysystemctl start kube-proxysystemctl status kube-proxy\n\n\n\n七、测试集群\n一般主节点的 kubectl 会配置可以连接集群，下面在执行 kubectl 命令时，默认是 master 节点\n\n1. 创建 kubectl kubeconfig 文件\n这个没发现具体啥用处\n\nexport KUBE_APISERVER=&quot;https://172.16.10.20:6443&quot;# 设置集群参数kubectl config set-cluster kubernetes \\  --certificate-authority=/etc/kubernetes/ssl/ca.pem \\  --embed-certs=true \\  --server=$&#123;KUBE_APISERVER&#125;# 设置客户端认证参数kubectl config set-credentials admin \\  --client-certificate=/etc/kubernetes/ssl/admin.pem \\  --embed-certs=true \\  --client-key=/etc/kubernetes/ssl/admin-key.pem# 设置上下文参数kubectl config set-context kubernetes \\  --cluster=kubernetes \\  --user=admin# 设置默认上下文kubectl config use-context kubernetes\n\n\n\n2. 创建资源测试 kubernetes 的方式可以是创建一个资源观察一下，在 master-01 中操作，创建一个运行 nginx 的  deployment 资源，worker 节点会去拉取 nginx 镜像运行\n# kubectl run nginx --replicas=2 --labels=&quot;run=load-balancer-example&quot; --image=nginx  --port=80kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.deployment.apps/nginx created# kubectl get deploymentsNAME    READY   UP-TO-DATE   AVAILABLE   AGEnginx   0/2     2            0           2m14s# kubectl get pods       NAME                     READY   STATUS              RESTARTS   AGEnginx-5c47ff5dd6-98qbn   0/1     ContainerCreating   0          2m56snginx-5c47ff5dd6-lgjjv   0/1     ContainerCreating   0          2m56s# 已经运行在集群中了# kubectl get pods -o wideNAME                     READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATESnginx-5c47ff5dd6-98qbn   1/1     Running   0          5m37s   10.99.41.2   172.16.10.20   &lt;none&gt;           &lt;none&gt;nginx-5c47ff5dd6-lgjjv   1/1     Running   0          5m37s   10.99.76.2   172.16.10.26   &lt;none&gt;           &lt;none&gt;\n\n\n\n3. 安装 dashboard\ndashboard 项目地址\n在 master-01 中安装\n\nkubernetes 的 addons 中包含一个 dashboard ，它是一个 web 页面，为集群提供了一些基础的资源可视化、状态查看等功能，现在，在集群中安装它。\n3.1 创建资源在项目中，有说明安装方式，提供了一个 yaml 文件，在集群中用 kubectl apply  就算是安装完了。需要注意的是，在这个资源文件里面 dashboard 镜像，指定的是位于 gcr 中的镜像，如果访问 google 的镜像站有问题，可以把这个文件中的 dashboard 镜像地址改一下，国内的镜像站有转存，比如阿里云就有\n先看安装的方式\n# 如果网络正常，可按官网的来，一步到位# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml\n\n如果访问google镜像站网络不好，我们改用国内镜像站后创建\nmkdir ~/k8s/dashboardcd ~/k8s/dashboard# 修改 k8s.gcr.io 为阿里云中 google 容器地址curl https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml &gt; dashboard.yamlsed -i &quot;s#k8s.gcr.io#registry.cn-hangzhou.aliyuncs.com/google_containers#gp&quot; ./dashboard.yamlkubectl apply -f dashboard.yaml \n\n创建后查看一下，该资源创建的时候指定了 namespace ，我们通过它指定的 kube-system 命名空间可以看到资源情况\nkubectl get deployments -n kube-systemkubectl get service -n kube-systemkubectl get pods -n kube-system\n\n\n\n3.2 创建账号和集群关系绑定为 dashboard 创建账号，和角色绑定关系，我们需要 账号的 token 来登陆 dashboard 页面\n# 创建service account# kubectl create sa dashboard-admin -n kube-system# kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin# 查看token，先找到资源文件中定义生成的 secrets 名称# kubectl get secrets -n kube-system |grep &quot;dashboard-admin&quot; |awk &#x27;&#123;print $1&#125;&#x27;dashboard-admin-token-686fq    # 我这里的名称# 通过名称获得 token，最长那一串字符# kubectl describe secrets -n kube-system dashboard-admin-token-686fqName:         dashboard-admin-token-686fqNamespace:    kube-systemLabels:       &lt;none&gt;Annotations:  kubernetes.io/service-account.name: dashboard-admin              kubernetes.io/service-account.uid: a86541c1-5f1c-4b07-b84b-f9504727c203Type:  kubernetes.io/service-account-tokenData====ca.crt:     1371 bytesnamespace:  11 bytestoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tNjg2ZnEiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYTg2NTQxYzEtNWYxYy00YjA3LWI4NGItZjk1MDQ3MjdjMjAzIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.JtNwe-9Gfqo9-gS1vvf8AMLzXq8y6EPuxPSIO1uGZulJMQ3soFCwCji-HILhJ8L8hwbx4_sCoThcCDIMuiWgRxcFh_4zlcxnnjEfquYinKnVVCw_jovth2EIt9CXhmV_DLjOJcNaLXzCRDvi3usLA_QjT3uTLhoyTpLKpgxNL1XsMeE12ZJIe4iOpvvS-IQ_w89fqH6zhnfsVYQS1lYabNGkpKxMLyGFY9c76NUhbZxjEYP_jan2yawLXdJnJvOrS-HCRQaU01kikZ9wk38FRzrDU4Ya1O0Vw-tMhF91_v-uJI-XC-VVRw4yG6iZWokmVi28vtcEo-srBDmOp0NPGw\n\n\n\n3.4 创建和安装访问证书创建证书，让浏览器登陆，最后生成 p12 文件时需要输入并重复一次密码，密码需要记住。生成后下载 p12 证书到本机，安装它，密码是刚刚设置的\ngrep &#x27;client-certificate-data&#x27; ~/.kube/config | head -n 1 | awk &#x27;&#123;print $2&#125;&#x27; | base64 -d &gt;&gt; kubecfg.crtgrep &#x27;client-key-data&#x27; ~/.kube/config | head -n 1 | awk &#x27;&#123;print $2&#125;&#x27; | base64 -d &gt;&gt; kubecfg.keyopenssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name &quot;kubernetes-web-client&quot;\n\n最后访问 https://172.16.10.20:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ 打开（把 172.16.10.20 替换成自己的 master ip）\n打开浏览器时，将提示是否使用证书，我们点确认后，提示选择令牌还是Kubeconfig来登陆，我们选择令牌，在输入框中输入上面 kubectl describe secrets -n kube-system dashboard-admin-token-686fq 输出的 token 字符串，即可登陆\n在页面中可以看到所有命名空间中的资源，并且可以对容器执行命令\n八、问题记录\n执行exec时报错如下，按官网文档的说法是需要给 APIServer 和 kubelet 指定认证的 key 再启动，上面配置文件已经改过这个问题，另外可以参考这个地址\n unable to upgrade connection: forbidden (user-system:anonymous, verb=create, re...\n\n执行命令报错如下，需要创建 RBAC 规则（在 dashboard 中的容器组执行命令中碰到这个报错）\n# 报错unable to upgrade connection: forbidden (user=kubernetes, verb=create, r...# 措施，创建规则文件cat &gt; apiserver-to-kubelet.yaml &lt;&lt;EOFapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  annotations:    rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;  labels:    kubernetes.io/bootstrapping: rbac-defaults  name: system:kubernetes-to-kubeletrules:  - apiGroups:      - &quot;&quot;    resources:      - nodes/proxy      - nodes/stats      - nodes/log      - nodes/spec      - nodes/metrics    verbs:      - &quot;*&quot;---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: system:kubernetes  namespace: &quot;&quot;roleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: system:kubernetes-to-kubeletsubjects:  - apiGroup: rbac.authorization.k8s.io    kind: User    name: kubernetesEOF# 创建规则kubectl create -f apiserver-to-kubelet.yaml \n\n","categories":["kubernetes"],"tags":["kubernetes"]},{"title":"mysql-utilities 复制工具","url":"/2018/02/27/mysql-utilities-%E5%A4%8D%E5%88%B6%E5%B7%A5%E5%85%B7/","content":"MySQL复制工具\n了解 mysql-utilities 中关于复制的几个工具\n\n\nmysql-utilities：\nmysqlreplicate    # 快速部署主从\nmysqlrplcheck     # 检测从服务器状态\nmysqlrplsync      # 检测主从复制数据是否一致\nmysqlrplshow      # 获取主从拓扑\n\n\n\n\n\n贴一下工具包中其他脚本的用途\nBinary Log Operations(二进制日志操作)         mysqlbinlogmove    二进制日志移动        mysqlbinlogpurge   二进制日志清理        mysqlbinlogrotate  二进制日志轮转工具    Database Operations(数据库操作)        mysqldbexport      数据导出        mysqldbimport      数据导入        mysqldbcopy        库级别数据库复制        mysqldiff          数据库对象级别比较工具        mysqldbcompare     数据库库级别比较工具General Operations(通用用的操作)             mysqldiskusage     磁盘空间查看        mysqlfrm           恢复故障表.frm文件        mysqluserclone     用户克隆工具        mysqluc            Utilities帮助工具         mysqlindexcheck    索引检测工具        mysqlmetagrep      元数据过滤器        mysqlprocgrep      进程搜索及清理工具High Availability Operations(高可用)        mysqlreplicate     主从复制工具        mysqlrpladmin      主从复制管理工具        mysqlrplcheck      主从复制检测工具        mysqlrplms         主从多源复制工具        mysqlrplshow       主从复制拓扑图工具        mysqlrplsync       主从复制同步工具        mysqlfailover      主从failover工具        mysqlslavetrx      从库事务跳过工具 Server Operations(服务器操作)        mysqlserverinfo    服务器信息查看工具        mysqlserverclone   服务器克隆工具    Specialized Operations(特殊操作)        mysqlauditadmin    审计管理工具         mysqlauditgrep     审计日志过滤工具\n\n\n\nmysqlreplicate\n该实用程序允许管理员设置并启动从一台服务器（主设备）到另一台服务器（从设备）的复制。用户提供从站的登录信息和连接到主站的连接信息。还可以指定要用于测试复制的数据库。  \n\n\n参数解析  --master=master                 通常命令行： user[:passwd]@host[:port][:socket]--slave=slave                   同上，指定的用户需要有with grant option权限--rpl-user=user:passwd          指定用户名密码，用户可以不存在-b --start-from-beginning       从开头复制--master-log-file               从特定主二进制日志文件中的第一个事件开始复制--master-log-pos=master_log_pos 从master日志文件中的这个位置开始复制。除非--master-log-file给出，否则此选项无效-vv                             详细信息-vvv                            debug信息-p --pedantic       如果两台服务器没有相同的存储引擎集合，相同的默认存储引擎和相同的InnoDB存储引擎，则会失败。\n\n\n准备工作  \n\nmaster：192.168.122.66  \nslave：192.168.122.70\n分别开启二进制日志，配置不同的 server_id\n配置文件添加 report-host=[本机ip]（非必要）\n\n\nmaster 和 slave 为 mysqlreplicate 创建一个带授权权限的用户\n grant all on *.* to &#x27;root&#x27;@&#x27;%&#x27; identified by &#x27;111111&#x27; with grant option;\n\n在安装了 utilities 工具包并且可以连接到master和slave的任意机器上执行\n shell&gt; mysqlreplicate --master=root:111111@192.168.122.66:3306 \\                      --slave=root:111111@192.168.122.70:3306 \\                      --rpl-user=rpl:rpl -vv -bWARNING: Using a password on the command line interface can be insecure.# master on 192.168.122.66: ... connected.# slave on 192.168.122.70: ... connected.# master id = 11#  slave id = 12# master uuid = d09f71e0-0caf-11e8-b13e-525400eac085#  slave uuid = f733905a-1d1a-11e8-bab4-5254004dae68# Checking InnoDB statistics for type and version conflicts.# Checking storage engines...# Checking for binary logging on master...# Setting up replication...# Connecting slave to master...# CHANGE MASTER TO MASTER_HOST = &#x27;192.168.122.66&#x27;, MASTER_USER = &#x27;rpl&#x27;, MASTER_PASSWORD = &#x27;rpl&#x27;, MASTER_PORT = 3306, MASTER_AUTO_POSITION=1# Starting slave from the beginning...# IO status: Queueing master event to the relay log# IO thread running: Yes# IO error: None# SQL thread running: Yes# SQL error: None# Waiting for slave to synchronize with master# IO status: Queueing master event to the relay log# IO thread running: Yes# IO error: None# SQL thread running: Yes# SQL error: None# Waiting for slave to synchronize with master  -- 这里因为我是-b参数从头复制，所以要很长时间才追到master的进度，程序不等完成，会报错。其实slave还在追ERROR: failed to sync slave with master.ERROR: Cannot setup replication.\n\nmysqlrplcheck\n当主从启动后，可以通过另外的 mysqlrplcheck 工具来查看主从复制状态，指定 master 和 slave，常用的参数就是 -s, --show-slave-status 可以看到 slave 复制状态。\n\nmysqlrplcheck 工具检测\n mysql&gt; mysqlrplcheck --master=root:111111@192.168.122.66:3306 --slave=root:111111@192.168.122.70:3306 -sWARNING: Using a password on the command line interface can be insecure.# master on 192.168.122.66: ... connected.# slave on 192.168.122.70: ... connected.Test Description                                                     Status---------------------------------------------------------------------------Checking for binary logging on master                                [pass]Are there binlog exceptions?                                         [pass]Replication user exists?                                             [pass]Checking server_id values                                            [pass]Checking server_uuid values                                          [pass]Is slave connected to master?                                        [pass]Check master information file                                        [pass]Checking InnoDB compatibility                                        [pass]Checking storage engines compatibility                               [pass]Checking lower_case_table_names settings                             [pass]Checking slave delay (seconds behind master)                         [pass]## Slave status: #                Slave_IO_State : Waiting for master to send event                   Master_Host : 192.168.122.66                   Master_User : rpl                   Master_Port : 3306                 Connect_Retry : 60               Master_Log_File : bin.000010           Read_Master_Log_Pos : 17301                Relay_Log_File : relay.000009                 Relay_Log_Pos : 17502         Relay_Master_Log_File : bin.000010              Slave_IO_Running : Yes             Slave_SQL_Running : Yes               Replicate_Do_DB :            Replicate_Ignore_DB :             Replicate_Do_Table :         Replicate_Ignore_Table :        Replicate_Wild_Do_Table :    Replicate_Wild_Ignore_Table :                     Last_Errno : 0                    Last_Error :                   Skip_Counter : 0           Exec_Master_Log_Pos : 17301               Relay_Log_Space : 223036003               Until_Condition : None                Until_Log_File :                  Until_Log_Pos : 0            Master_SSL_Allowed : No            Master_SSL_CA_File :             Master_SSL_CA_Path :                Master_SSL_Cert :              Master_SSL_Cipher :                 Master_SSL_Key :          Seconds_Behind_Master : 0 Master_SSL_Verify_Server_Cert : No                 Last_IO_Errno : 0                 Last_IO_Error :                 Last_SQL_Errno : 0                Last_SQL_Error :    Replicate_Ignore_Server_Ids :               Master_Server_Id : 11                   Master_UUID : d09f71e0-0caf-11e8-b13e-525400eac085              Master_Info_File : mysql.slave_master_info                     SQL_Delay : 0           SQL_Remaining_Delay : None       Slave_SQL_Running_State : Slave has read all relay log; waiting for more updates            Master_Retry_Count : 86400                   Master_Bind :        Last_IO_Error_Timestamp :       Last_SQL_Error_Timestamp :                 Master_SSL_Crl :             Master_SSL_Crlpath :             Retrieved_Gtid_Set : d09f71e0-0caf-11e8-b13e-525400eac085:42-384             Executed_Gtid_Set : d09f71e0-0caf-11e8-b13e-525400eac085:1-384,f733905a-1d1a-11e8-bab4-5254004dae68:1-6                 Auto_Position : 1          Replicate_Rewrite_DB :                   Channel_Name :             Master_TLS_Version : # ...done.\n\nmysqlrplsyncmysqlrplsync 用于检测主从数据是否一致，不指定 test_rplsync_db 为检测全部\n# 测试test库是否一致# 不同步案例shell&gt; mysqlrplsync --master=root:111111@192.168.122.66:3306 \\                    --slave=root:111111@192.168.122.70:3306 test_rplsync_db test## GTID differences between Master and Slaves:# - Slave &#x27;192.168.122.70@3306&#x27; is up-to-date.## Checking data consistency.## Using Master &#x27;192.168.122.66@3306&#x27; as base server for comparison.# WARNING: specified database to check does not exist on base server and will be skipped: test_rplsync_db.# Checking &#x27;test&#x27; database...#   [DIFF] Table &#x27;test.a&#x27; NOT on server &#x27;192.168.122.70@3306&#x27;.     -- DIFF位置为OK表示一致#   [DIFF] Table &#x27;test.b&#x27; NOT on server &#x27;192.168.122.70@3306&#x27;.#   [DIFF] Table &#x27;test.customer&#x27; NOT on server &#x27;192.168.122.70@3306&#x27;.#   [DIFF] Table &#x27;test.district&#x27; NOT on server &#x27;192.168.122.70@3306&#x27;.#   [DIFF] Table &#x27;test.history&#x27; NOT on server &#x27;192.168.122.70@3306&#x27;.#   [DIFF] Table &#x27;test.item&#x27; NOT on server &#x27;192.168.122.70@3306&#x27;.#   [DIFF] Table &#x27;test.new_orders&#x27; NOT on server &#x27;192.168.122.70@3306&#x27;.#   [DIFF] Table &#x27;test.orders&#x27; NOT on server &#x27;192.168.122.70@3306&#x27;.#   [DIFF] Table &#x27;test.order_line&#x27; NOT on server &#x27;192.168.122.70@3306&#x27;.#   [DIFF] Table &#x27;test.stock&#x27; NOT on server &#x27;192.168.122.70@3306&#x27;.#   [DIFF] Table &#x27;test.warehouse&#x27; NOT on server &#x27;192.168.122.70@3306&#x27;.##...done.## SUMMARY: 11 data consistency issues found.   -- 上面11个表不一致，此时slave还在追日志## 正常案例shell&gt; mysqlrplsync --master=root:111111@192.168.122.66:3307 --slave=root:111111@192.168.122.80:3307WARNING: Using a password on the command line interface can be insecure.## GTID differences between Master and Slaves:# - Slave &#x27;192.168.122.80@3307&#x27; is up-to-date.## Checking data consistency.## Using Master &#x27;192.168.122.66@3307&#x27; as base server for comparison.# Checking &#x27;tpcc10&#x27; database...# - Checking &#x27;customer&#x27; table data...#   [OK] `tpcc10`.`customer` checksum for server &#x27;192.168.122.80@3307&#x27;.# - Checking &#x27;district&#x27; table data...#   [OK] `tpcc10`.`district` checksum for server &#x27;192.168.122.80@3307&#x27;.# - Checking &#x27;history&#x27; table data...#   [OK] `tpcc10`.`history` checksum for server &#x27;192.168.122.80@3307&#x27;.# - Checking &#x27;item&#x27; table data...#   [OK] `tpcc10`.`item` checksum for server &#x27;192.168.122.80@3307&#x27;.# - Checking &#x27;new_orders&#x27; table data...#   [OK] `tpcc10`.`new_orders` checksum for server &#x27;192.168.122.80@3307&#x27;.# - Checking &#x27;orders&#x27; table data...#   [OK] `tpcc10`.`orders` checksum for server &#x27;192.168.122.80@3307&#x27;.# - Checking &#x27;order_line&#x27; table data...#   [OK] `tpcc10`.`order_line` checksum for server &#x27;192.168.122.80@3307&#x27;.# - Checking &#x27;stock&#x27; table data...#   [OK] `tpcc10`.`stock` checksum for server &#x27;192.168.122.80@3307&#x27;.# - Checking &#x27;warehouse&#x27; table data...#   [OK] `tpcc10`.`warehouse` checksum for server &#x27;192.168.122.80@3307&#x27;.# Checking &#x27;an&#x27; database...# - Checking &#x27;a&#x27; table data...#   [OK] `an`.`a` checksum for server &#x27;192.168.122.80@3307&#x27;.##...done.## SUMMARY: No data consistency issue found.#\n\n\nmysqlrplshow\n列出 master 下所有的 slave，文字图形的方式展示，有层级时可以递归\n\n前提条件：需要在 slave 的 my.cnf 中添加 report-host = [本机ip]，report-port默认 3306\n\n用法和参数\n mysqlrplshow   --master=user:pass@host:port               --discovery-slave-login=user:pass    # 自动发现从机并通过这个用户查询信息，需要replicate client和slave权限               --recurse, -r      # 递归查找拓扑，在级联复制的时候很方便               --show-list, -l    # 列出表格描述slave对应的master               -vvv               # debug信息，-v可以显示slave线程状态shell&gt; mysqlrplshow --master=root:111111@&quot;192.168.122.66&quot; --discover-slaves-login=root:111111 -vvvWARNING: Using a password on the command line interface can be insecure.# master on 192.168.122.66: ... connected.# Finding slaves for master: 192.168.122.66:3306# Replication Topology Graph192.168.122.66:3306 (MASTER)   |   +--- 192.168.122.70:3306 [IO: Yes, SQL: Yes] - (SLAVE)   |   +--- 192.168.122.80:3306 [IO: Yes, SQL: Yes] - (SLAVE)   # 搭建级联后的示例shell&gt; mysqlrplshow --master=root:111111@&quot;192.168.122.66:3307&quot; --discover-slaves-login=root:111111 -r -vWARNING: Using a password on the command line interface can be insecure.# master on 192.168.122.66: ... connected.# Finding slaves for master: 192.168.122.66:3307# master on 192.168.122.70: ... connected.# Finding slaves for master: 192.168.122.70:3307# master on 192.168.122.70: ... connected.# Finding slaves for master: 192.168.122.70:3306# master on 192.168.122.80: ... connected.# Finding slaves for master: 192.168.122.80:3307# Replication Topology Graph192.168.122.66:3307 (MASTER)   |   +--- 192.168.122.70:3307 [IO: Yes, SQL: Yes] - (SLAVE + MASTER)   |   |   |   +--- 192.168.122.70:3306 [IO: Yes, SQL: Yes] - (SLAVE)   |   +--- 192.168.122.80:3307 [IO: Yes, SQL: Yes] - (SLAVE)\n\n","categories":["MySQL"],"tags":["mysql-utilities"]},{"title":"sysbench 压测工具","url":"/2018/05/17/sysbench-%E5%8E%8B%E6%B5%8B%E5%B7%A5%E5%85%B7/","content":"sysbench\n1.x版本和0.x操作选项有区别，注意看help\n\nsysbench 是一个比较通用的压测工具，可以用来做一些基准测试，看 help 中有说明提供了如下测试选项，另外在源码中提供了数据库 OLTP 场景的测试脚本，调整参数配置后，通过反复基准测试验证调优效果。\n\n\nfileio  - File I/O testcpu \t- CPU performance testmemory  - Memory functions speed testthreads - Threads subsystem performance testmutex   - Mutex performance testOLTP    - 通过脚本可以测试OLTP场景下数据库的能力，脚本在源码`src/lua`目录\n\n这里简单记录下 fileio 和 MySQL OLTP 的测试\n安装yum install -y autoconf automake libtool \\    perl-ExtUtils-Embed perl-develgit clone https://github.com/akopytov/sysbenchcd sysbench./autogen.sh./configuremake -j 4 &amp;&amp; make install\n\n\n测试文件IO\nsysbench-1.1 的 IO 测试示例 # 生成数据文件sysbench fileio --file-num=4  \\    --file-block-size=16384 \\    --file-total-size=20G \\    prepare# 三秒输出一次状态sysbench fileio --file-num=4  \\    --file-block-size=16384 \\    --file-total-size=20G \\    --file-test-mode=rndrd \\    --file-extra-flags=direct \\    --max-requests=0 \\    --max-time=1200 --num-threads=16 \\    --report-interval=3  \\    run# 新版本sysbench fileio --file-num=4  --file-block-size=16384 --file-total-size=8G --file-test-mode=rndrd --file-extra-flags=direct --time=1200 --threads=16  --report-interval=1   run | tee -a sysbench.out# 清除测试文件sysbench fileio --file-num=4  \\    --file-block-size=16384 \\    --file-total-size=20G \\    cleanup# 简单测试mkdir fileio_benchcd fileio_benchsysbench fileio --file-num=1 --file-block-size=4096 --file-total-size=2G preparesysbench fileio --file-num=1 --file-block-size=4096 --file-total-size=2G --file-test-mode=rndrd --file-extra-flags=direct --time=10 --threads=1  --report-interval=1 run | tee -a sysbench.out sysbench fileio --file-num=1  --file-block-size=4096 --file-total-size=2G cleanup\n\n测试MySQL\n生成数据文件，指定表的大小和数量，时间和线程数\n cd /root/chen/sysbenchsysbench src/lua/oltp_read_only.lua \\    --mysql-host=172.16.88.185 --mysql-port=3306 \\    --mysql-user=root --mysql-password=111111 \\    --mysql-db=sysbench --tables=10 \\    --table-size=10000000 --max-requests=0 \\    --report-interval=10 --time=120 --threads=12 \\    prepare\n\n压测run\n sysbench src/lua/oltp_read_only.lua \\    --mysql-host=172.16.88.185 --mysql-port=3306 \\    --mysql-user=root --mysql-password=111111 \\    --mysql-db=sysbench --tables=10 \\    --table-size=10000000 --max-requests=0 \\    --report-interval=10 --time=3600 --threads=12 \\    run\n\n执行 cleanup 清除压测表\n\n\n","categories":["Linux"],"tags":["sysbench"]},{"title":"zabbix-server 和 zabbix-proxy","url":"/2018/04/17/zabbix-server%E5%92%8Czabbix-proxy/","content":"zabbix多机房监控\nzabbix server和zabbix proxy连接模式如下图  \n一个zabbix server可以添加多个zabbix proxy，每个网络暴露一个proxy即可通过该proxy监控到具体网络内部的agent节点。  \n关于proxy主动模式和被动模式的选择  \n主动模式：proxy可以通过ip的方式直接找到server，可以选择主动模式，比如server有外网ip并且暴露出端口。  \n被动模式：正好相反，server的网络可以找到proxy（机群多可能导致server压力较大未测试）。\n\n\n\n\n\n\n示例环境\nserver和proxy系统环境为centos6，agent机器为centos7  \n\n172.16.88.0/24 模拟公网，其中124为kvm宿主机，内部网段192.168.122.0/24模拟内网，agent节点均在其中。\n安装zabbix server\n\nzabbix-server在172.16.88.185机器（需要mysql数据库，本文使用已有实例）  \n下面选择自带httpd跑web管理页面   \n准备remi源，因为需要php支持\n\n\n获取zabbix源码先去官网下载zabbix-3.4.7.tar.gz源码，是sourceforge的链接。  \nremi源地址remi源清华大学镜像站下载地址  \n\ncentos7\ncentos6\n\n安装依赖包# 获取remi源yum install https://mirrors.tuna.tsinghua.edu.cn/remi/enterprise/remi-release-6.rpm# 安装mysql-server，这里我已经有mysql服务yum install httpd libevent-devel \\            libjpeg* libmcrypt openssl-devel \\            pcre-devel net-snmp net-snmp-devel \\            gcc glibc mysql-devel libxml2 \\            libxml2-devel libcurl-devel            yum install --enablerepo=remi --enablerepo=remi-php56 \\            php php-opcache php-devel php-mbstring \\            php-mcrypt php-mysqlnd php-phpunit-PHPUnit \\            php-pecl-xdebug php-pecl-xhprof \\            php-gd php-bcmath\n配置php环境修改/etc/php.ini中如下配置项  \nmax_execution_time = 300max_input_time = 300memory_limit = 128Mpost_max_size = 16Mupload_max_filesize = 2Mdate.timezone = Asia/Shanghaialways_populate_raw_post_data = -1\n\n初始化Server数据库创建zabbix-server连接的数据库，执行SQL  \ncreate database zabbix default character set &#x27;utf8&#x27;;grant all on zabbix.* to zabbix@&#x27;%&#x27; identified by &#x27;111111&#x27;;flush privileges;\n\n导入数据库表，注意顺序  \ntar xf zabbix-3.4.7.tar.gz -C /usr/srccd /usr/src/zabbix-3.4.7/mysql zabbix -uzabbix -p111111 -h172.16.88.185 &lt; database/mysql/schema.sqlmysql zabbix -uzabbix -p111111 -h172.16.88.185 &lt; database/mysql/images.sqlmysql zabbix -uzabbix -p111111 -h172.16.88.185 &lt; database/mysql/data.sql\n\n编译安装zabbix-server在源码目录编译zabbix-server，-j可以不用，若报错再具体分析，一般为缺包  \n./configure --prefix=/usr/local/zabbix-server  --enable-server --with-mysql \\            --with-net-snmp --with-libcurl --with-libxml2 --enable-agent --with-opensslmake install -j4\n\n在源码目录copy启动脚本，创建连接，启动脚本有调用  \ncp -a misc/init.d/fedora/core/zabbix_agentd /etc/init.d/cp -a misc/init.d/fedora/core/zabbix_server /etc/init.d/ln -sv /usr/local/zabbix-server/sbin/* /usr/local/sbin/ln -sv /usr/local/zabbix-server/bin/* /usr/local/bin/\n\n修改zabbix配置文件，如下改动  \n# grep -Ev &quot;^$|^#&quot; /usr/local/zabbix-server/etc/zabbix_server.confLogFile=/tmp/zabbix_server.logDBHost=172.16.88.185DBName=zabbixDBUser=zabbixDBPassword=111111DBPort=3306Timeout=4LogSlowQueries=3000\n\n\nweb管理端配置，启动服务这里使用httpd，在源码目录复制web页面到httpd的站点下  \ncp -a frontends/php /var/www/html/zabbixchown -R apache:apache /var/www/html/zabbix/\n\n重启服务，打开浏览器登陆，如本示例中的http://172.16.88.185/zabbix，默认用户认证为admin&#x2F;zabbix，打开页面后按提示操作。  \n/etc/init.d/httpd restart/etc/init.d/zabbix_server restart/etc/init.d/zabbix_agentd restart\n\n安装zabbix proxy\n\nzabbix proxy机器为172.16.88.124（192.168.122.1也是它）   \nzabbix proxy也需要数据库支持，只需要导入上面三个SQL中的schema.sql，注意别使用zabbix-server的库，这里使用192.168.122.66的一个数据库实例   \n源码版本最好和zabbix-server一致，以防数据库字段更改造成意外。\n\n\n源码和依赖还是去官网下载源码，或者cp上面的。  \n安装依赖  \nyum install libssh2-devel pcre-devel openssl-devel openssl libcurl-devel \\            net-snmp-devel net-snmp gcc glibc mysql-devel\n\n初始化proxy数据库创建用户初始数据表（我这里库在192.168.122.66机器，proxy可以连接到）  \n# SQLcreate database zabbix_proxy default character set &#x27;utf8&#x27;;grant all on zabbix.* to zabbix@&#x27;%&#x27; identified by &#x27;111111&#x27;;# shelltar xf zabbix-3.4.7.tar.gz -C /usr/src/cd /usr/src/zabbix-3.4.7/mysql zabbix_proxy -uzabbix -p111111 -h192.168.122.66 &lt; database/mysql/schema.sql\n\n编译安装proxy编译proxy，并且指定开启agent  \n./configure --prefix=/usr/local/zabbix-proxy --enable-proxy --enable-agent \\            --with-net-snmp --with-mysql --with-ssh2 --with-opensslmake install -j4\n\n修改proxy配置文件，使用PSK加密请注意后四个字段  \n# 主动模式ProxyMode=0# 指定zabbix-serverServer=172.16.88.185# 此proxy在server段的名称，需要相同Hostname=zabbix proxyLogFile=/tmp/zabbix_proxy.logPidFile=/tmp/zabbix_proxy.pid# 数据库配置DBHost=192.168.122.66DBName=zabbix_proxyDBUser=zabbixDBPassword=111111DBPort=3306# 配置同步时间，别太长ConfigFrequency=300DataSenderFrequency=2Timeout=4LogSlowQueries=3000# 使用PSK加密TLSConnect=pskTLSAccept=psk# 预共享密钥，需要在zabbix-server端同写这串字符(自定义)和PSK文件内容，可以实现加密TLSPSKIdentity=PSK 2243# 此文件为&quot;openssl rand -hex 32&quot;命令生成，做PSK加密TLSPSKFile=/usr/local/zabbix-proxy/etc/auth.psk    \n\nproxy机器的agent配置，指定server为本机，通过proxy添加本机到server管理  \nLogFile=/tmp/zabbix_agentd.logServer=192.168.122.1ServerActive=192.168.122.1Hostname=192.168.122.1\n\nzabbix-server管理界面中添加proxyzabbix-server的web页面添加proxy，选择主动式，不指定proxy地址  \n\n创建代理 \n配置proxy信息 \n配置proxy连接加密，一定与自己配置文件对应 \n保存 \n\n启动proxy服务启动服务进程，直接执行，使用默认配置文件，或者-c指定配置文件，日志在/tmp目录下  \n/usr/local/zabbix-proxy/sbin/zabbix_proxy /usr/local/zabbix-proxy/sbin/zabbix_agentd\n\n下面日志表示正常（web页面配置PSK加密看后面，未配置时会报错连接不上zabbix-server）  \n# tail -f /tmp/zabbix_proxy.log     5372:20180327:104440.758 proxy #23 started [trapper #5]  5373:20180327:104440.758 proxy #24 started [icmp pinger #1]  5350:20180327:104440.771 received configuration data from server at &quot;172.16.88.185&quot;, datalen 17649  5350:20180327:104940.870 received configuration data from server at &quot;172.16.88.185&quot;, datalen 17649\n\n启动proxy并且连接到zabbix-server后，可以在web管理界面看到最近出现历时是近几秒（主机数先忽略）\n到这zabbix-proxy的配置就完成了，此机器的zabbix-agent使用默认配置。  \nzabbix agent安装\n分布在上图的192.168.122.80和192.168.122.70机器，都安装   \n\nagent源码和依赖还是之前那份源码，安装依赖  \nyum install net-snmp-devel net-snmp gcc glibc\nagent安装配置编译参数  \ntar xf zabbix-3.4.7.tar.gz -C /usr/src/cd /usr/src/./configure --prefix=/usr/local/zabbix --enable-agent --with-net-snmpmake install -j2\n\n配置文件，指定内网proxy为Server，指定主机名为自身ip，方便后面自动注册  \n# grep -Ev &#x27;^$|^#&#x27; /usr/local/zabbix/etc/zabbix_agentd.confLogFile=/tmp/zabbix_agentd.logServer=192.168.122.1ServerActive=192.168.122.1Hostname=192.168.122.80\n\n启动服务，使用默认配置文件  \n/usr/local/zabbix/sbin/zabbix_agentd\n\n配置agent自动注册\n前往zabbix-server的管理界面配置zabbix-agent自动注册    \n\n在zabbix中还有一直方式是自动发现，不过速度比较慢，尝试之后自动注册比较方便。由每个agent自身主动通过zabbix-proxy向zabbix-server注册。自动注册可以配置动作，将机器加入某个组群和添加监控模板。  \n\n添加主机群组，用于容纳不同porxy下的agent\n创建自动注册动作\n为自动注册动作指定proxy，，勾选启用\n打开操作选项卡，配置对该自动注册的机器进行的操作，我们将它加入到刚刚新增的主机群组，并加入Linux操作系统的监控模板\n添加监控模板\n\n接下来，就等agent们注册了，每个agent使用自身ip，去管理界面的主机中查看可能需要等待一段时间，但比较快，zabbix可用性是会比较慢，当zabbix可用性绿了之后就可以监控到模板的值了。   \n监控主机已经添加到了zabbix-server，若有多个网络区域可以每个网络区域之间通过一个zabbix-proxy与zabbix-server连接。  \n附批量安装的ansible-playbook\n因为安装并不复杂，配置也变动不大，所以套了ansible和jinja2模板，多机器安装比较方便。  \n\nansible中主机定义和分组配置如下# cat /etc/ansible/ansible.cfg    # 这里使用公钥连接，也可以在hosts配置对应密码，搭建完删掉 [defaults]remote_port = 22private_key_file = /root/.ssh/id_rsahost_key_checking = False# cat /etc/ansible/hosts [local]192.168.122.1[agents]192.168.122.66192.168.122.70192.168.122.80\n获取zabbix安装包存放如下目录结构，包含一些经常用到的脚本和自定义key配置文件    \n# tree ..├── files│   ├── script.tar.gz                 # 整理的配置脚本│   ├── zabbix-3.4.7.tar.gz│   ├── zabbix_agentd.conf│   ├── zabbix_agentd.userparams.conf # 和脚本对应的zabbix自定义key配置│   └── zabbix_proxy.conf├── install_zabbix_agent.yml          # agent playbook└── install_zabbix_proxy.yml          # proxy playbook，启用并包含agent1 directory, 7 files\ninstall_zabbix_proxy.yml--- - hosts: local   vars:      - zabbix_server_address: 172.16.88.185  # zabbix server地址      - zabbix_proxy_address: 192.168.122.1   # 当前主机成为proxy, 使用自身ip     - proxy_name: &quot;zabbix proxy&quot;            # server页面配置的名称，各区域之间的proxy应不通     - package: zabbix-3.4.7                 # 源码包     - basedir: /usr/local/zabbix-proxy      # proxy编译地址，包含agent     - db_host: 192.168.122.66               # 数据库连接信息     - db_port: 3306     - db_name: zabbbix_proxy     - db_user: zabbix     - db_passwd: 111111     - current_dir: /root/ansible/zabbix     # 当前目录请修改，最后别斜杠      tasks:      - name: 添加zabbix用户       user: name=zabbix state=present shell=/bin/false group=zabbix system=yes createhome=no     - name: 解软件压源码       unarchive: src=&#123;&#123; current_dir &#125;&#125;/files/&#123;&#123; package &#125;&#125;.tar.gz dest=/usr/src/ copy=no     - name: 安装依赖软件       yum: name=&#123;&#123; item &#125;&#125; state=installed       with_items:         - libssh2-devel         - pcre-devel         - openssl-devel         - openssl         - libcurl-devel         - net-snmp-devel         - net-snmp         - gcc         - glibc         - mysql-devel     - name: 编译zabbix proxy，时间稍长请等待...       shell: ./configure --prefix=&#123;&#123; basedir&#125;&#125; --enable-proxy --enable-agent --with-net-snmp --with-mysql --with-ssh2 --with-openssl &amp;&amp; make install chdir=/usr/src/&#123;&#123; package &#125;&#125;          - name: copy zabbix proxy配置文件       template: src=&#123;&#123; current_dir &#125;&#125;/files/zabbix_proxy.conf dest=&#123;&#123; basedir &#125;&#125;/etc/zabbix_proxy.conf     - name: copy zabbix proxy和服务器的PSK认证文件       copy: src=&#123;&#123; current_dir &#125;&#125;/files/auth.psk dest=&#123;&#123; basedir &#125;&#125;/etc/auth.psk     - name: copy zabbix agent配置文件       template: src=&#123;&#123; current_dir &#125;&#125;/files/zabbix_agentd.conf  dest=&#123;&#123; basedir &#125;&#125;/etc/zabbix_agentd.conf          - name: copy zabbix agent自定义key配置文件       template: src=&#123;&#123; current_dir &#125;&#125;/files/zabbix_agentd.userparams.conf  dest=&#123;&#123; basedir &#125;&#125;/etc/zabbix_agentd.userparams.conf     - name: 解压自定义脚本       unarchive: src=&#123;&#123; current_dir &#125;&#125;/files/script.tar.gz dest=&#123;&#123; basedir &#125;&#125;/etc copy=no\n\ninstall_zabbix_agent.yml--- - hosts: agents   vars:      - zabbix_proxy_address: 192.168.122.1   # 当前局域网的proxy服务ip地址     - package: zabbix-3.4.7                 # 源码包去除&quot;.tar.gz&quot;的名称     - basedir: /usr/local/zabbix-x          # 编译安装地址     - current_dir: /root/ansible/zabbix     # 请修改当前目录，下面需要绝对路径      tasks:      - name: 添加zabbix用户       user: name=zabbix state=present shell=/bin/false group=zabbix system=yes createhome=no     - name: 解软件压源码       unarchive: src=&#123;&#123; current_dir &#125;&#125;/files/&#123;&#123; package &#125;&#125;.tar.gz dest=/usr/src/ copy=no     - name: 安装依赖软件       yum: name=&#123;&#123; item &#125;&#125; state=installed       with_items:         - net-snmp-devel         - net-snmp         - gcc         - glibc     - name: 编译zabbix agent时间稍长...       shell: ./configure --prefix=&#123;&#123; basedir&#125;&#125; --enable-agent --with-net-snmp &amp;&amp; make install chdir=/usr/src/&#123;&#123; package &#125;&#125;          - name: copy zabbix agent配置文件       template: src=&#123;&#123; current_dir &#125;&#125;/files/zabbix_agentd.conf  dest=&#123;&#123; basedir &#125;&#125;/etc/zabbix_agentd.conf          - name: copy zabbix agent自定义key配置文件       template: src=&#123;&#123; current_dir &#125;&#125;/files/zabbix_agentd.userparams.conf  dest=&#123;&#123; basedir &#125;&#125;/etc/zabbix_agentd.userparams.conf     - name: 解压自定义脚本       unarchive: src=&#123;&#123; current_dir &#125;&#125;/files/script.tar.gz dest=&#123;&#123; basedir &#125;&#125;/etc copy=no\nfiles&#x2F;zabbix_agentd.confLogFile=/tmp/zabbix_agentd.logServer=&#123;&#123; zabbix_proxy_address &#125;&#125;ServerActive=&#123;&#123; zabbix_proxy_address &#125;&#125;Hostname=&#123;&#123; ansible_default_ipv4[&#x27;address&#x27;] &#125;&#125;Include=&#123;&#123; datadir &#125;&#125;/etc/zabbix_agentd.userparams.conf\n\nfiles&#x2F;zabbix_proxy.confProxyMode=0Server=&#123;&#123; zabbix_server_address &#125;&#125;Hostname=&#123;&#123; proxy_name &#125;&#125;LogFile=/tmp/zabbix_proxy.logPidFile=/tmp/zabbix_proxy.pidDBHost=&#123;&#123; db_host &#125;&#125;DBName=&#123;&#123; db_name &#125;&#125;DBUser=&#123;&#123; db_user &#125;&#125;DBPassword=&#123;&#123; db_passwd &#125;&#125;DBPort=&#123;&#123; db_port &#125;&#125;ConfigFrequency=300DataSenderFrequency=2Timeout=4LogSlowQueries=3000TLSConnect=pskTLSAccept=pskTLSPSKIdentity=PSK 2243TLSPSKFile=&#123;&#123; basedir &#125;&#125;/etc/auth.psk\n\n auth.psk为连接加密文件openssl rand -hex 32生成zabbix_agentd.userparams.conf文件是script目录中对应脚本的key，常用脚本。\n\n","categories":["Linux"],"tags":["zabbix"]},{"title":"使用 urfave/cli 解析命令行参数","url":"/2022/01/08/%E4%BD%BF%E7%94%A8-urfave-cli-%E8%A7%A3%E6%9E%90%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0/","content":"使用 urfave&#x2F;cli 解析命令行参数urfave/cli 是一个命令行辅助包，可以设置命令执行的方法。让服务可以根据参数不同，启动不一样的入口，非常方便。\n\n\npackage mainimport (\t&quot;fmt&quot;\t&quot;log&quot;\t&quot;os&quot;\t&quot;github.com/urfave/cli/v2&quot;)var host stringfunc main() &#123;\tapp := cli.NewApp()\tapp.Name = &quot;Cli Test Application&quot;\tapp.Usage = &quot;cli test app usage&quot;\tapp.Version = &quot;1.1.0&quot;\tapp.Flags = []cli.Flag&#123;\t\t&amp;cli.StringFlag&#123;\t\t\tName:        &quot;host&quot;,\t\t\tValue:       &quot;127.0.0.1&quot;,\t\t\tUsage:       &quot;host address&quot;,\t\t\tDestination: &amp;host, // 参数解析到 host 变量中\t\t&#125;,\t\t&amp;cli.Int64Flag&#123;\t\t\tName:  &quot;port&quot;,\t\t\tValue: 10202,\t\t\tUsage: &quot;port number&quot;,\t\t&#125;,\t&#125;\t// 执行单个命令用 Action\t// app.Action = func(c *cli.Context) error &#123;\t// \tfmt.Println(&quot;hello world&quot;)\t// \tfmt.Println(&quot;host=&quot;, host)\t// \tfmt.Println(&quot;port=&quot;, c.Int(&quot;port&quot;)) // 从 cli 中获取 port 参数\t// \treturn nil\t// &#125;\t// 多个命令，可以指定到 Commands 中\tapp.Commands = []*cli.Command&#123;\t\t&#123;\t\t\tName:    &quot;server&quot;,\t\t\tAliases: []string&#123;&quot;s&quot;&#125;,\t\t\tUsage:   &quot;run server&quot;,\t\t\tAction: func(c *cli.Context) error &#123;\t\t\t\tfmt.Println(&quot;run server arg:&quot;, c.Args().First())\t\t\t\treturn nil\t\t\t&#125;,\t\t&#125;,\t\t&#123;\t\t\tName:    &quot;agent&quot;,\t\t\tAliases: []string&#123;&quot;a&quot;&#125;,\t\t\tUsage:   &quot;run agent&quot;,\t\t\tAction: func(c *cli.Context) error &#123;\t\t\t\tfmt.Println(&quot;run agent arg:&quot;, c.Args().First())\t\t\t\treturn nil\t\t\t&#125;,\t\t&#125;,\t&#125;\tif err := app.Run(os.Args); err != nil &#123;\t\tlog.Fatalf(&quot;error: %v&quot;, err)\t&#125;&#125;\n\n测试# 单个命令的 Action 测试go run cli.go --host 1.2.3.4 --port 11022# 多入口命令测试go run cli.go server startgo run cli.go agent stop\n\n","categories":["Golang"]},{"title":"一首喜欢的诗","url":"/2019/11/03/%E4%B8%80%E9%A6%96%E5%96%9C%E6%AC%A2%E7%9A%84%E8%AF%97/","content":"春江花夜月春江潮水连海平，海上明月共潮生。\n滟滟随波千万里，何处春江无月明！\n江流宛转绕芳甸，月照花林皆似霰；\n空里流霜不觉飞，汀上白沙看不见。\n江天一色无纤尘，皎皎空中孤月轮。\n江畔何人初见月？江月何年初照人？\n人生代代无穷已，江月年年望相似。\n不知江月待何人，但见长江送流水。\n白云一片去悠悠，青枫浦上不胜愁。\n谁家今夜扁舟子？何处相思明月楼？\n可怜楼上月徘徊，应照离人妆镜台。\n玉户帘中卷不去，捣衣砧上拂还来。\n此时相望不相闻，愿逐月华流照君。\n鸿雁长飞光不度，鱼龙潜跃水成文。\n昨夜闲潭梦落花，可怜春半不还家。\n江水流春去欲尽，江潭落月复西斜。\n斜月沉沉藏海雾，碣石潇湘无限路。\n不知趁月几人归，落月摇情满江树。\n","categories":["古诗词"]},{"title":"在 salt 模块中获取 jid","url":"/2020/03/05/%E5%9C%A8-salt-%E6%A8%A1%E5%9D%97%E4%B8%AD%E8%8E%B7%E5%8F%96-jid/","content":"在 salt 模块中获取任务 jid\n在自定义的模块中获取 jid，以实现一些需求（客户端主动的方式，触发最新步骤的结果）\n\n需要自定义 salt 模块，并且接收 kwargs 参数（必须），在kwargs中会有一个 __pub_jid 的参数，是当前运行 job 的 id。\n监听事件命令，方便观察，主要看事件输出的 jid 和我们后面模块写入的 jid （__pub_jid）是否一致，对比 minion 日志中我们打的日志。\nsalt-run state.event &#x27;salt/job/*/prog/*/0&#x27;\n\n自定义测试模块\nconsole.log用 state.sls 在 sls 中跑是无效的，kwargs 中会没有 jid  \n\n接收 **kwargs 参数，其中包含 __pub_jid 就是 jid 了，注意如果没接收参数就无了。\n另外值的注意的是，在自定义模块中如果再调用其他模块（__salt__ 方式），子 job 运行时，事件监听获取不到该调用的 jid，这时候需要在调用时传入 __pub_jid，最好是把 **kwargs 传入，这样会使用当前的 jid。\n模块代码如下\n# /srv/salt/_modules/console.pyimport loggingimport requestslogger = logging.getLogger(__name__)def __virtual__():    &quot;&quot;&quot;&quot;&quot;&quot;    return &#x27;console&#x27;def _get_sls():    &quot;&quot;&quot;测试的从接口获取 sls 名称，可以去掉&quot;&quot;&quot;    req = requests.get(&quot;http://172.16.6.239:5000/api/v1/asset/sls_name/&quot;)    logger.info(&quot;=====&gt; %s&quot; % req.content)    return req.json()def log(value=None, **kwargs):    &quot;&quot;&quot;console.log&quot;&quot;&quot;    sls_name = _get_sls()    # __pub_jid是一个字符串，如果写死，从此处调用的job都将是同一个jid    # 最好传入 **kwargs 里面包含 __pub_jid    __salt__[&quot;state.sls&quot;](sls_name, **kwargs)    #__salt__[&quot;state.sls&quot;](sls_name, __pub_jid=&quot;aaaaaaaaaaaaaa&quot;)    ret = &#123;&#125;    ret[&#x27;sls_name&#x27;] = sls_name    ret[&#x27;msg&#x27;] = value    ret[&#x27;kwargs&#x27;] = kwargs    logger.info(&quot;++++&gt; &quot;)    logger.info(&quot;++++&gt; %s&quot; % ret)    logger.info(&quot;++++&gt; &quot;)    return ret\n\n同步模块（修改 _modules 下的文件需要同步）\nsalt &#x27;*&#x27; saltutil.sync_all\n\n调用的命令，执行 console.log 模块，输出的 kwargs 中含有 jid（用 state.sls 在 sls 中跑是无效的，kwargs 中没有 jid）\nsalt &quot;*&quot;  console.log &quot;hello world&quot;\n\n无效的方式\n（在 sls 中无法获取到）\n\n测试用 state.sls 方式运行下面的状态文件 salt &quot;*&quot; state.sls console，会看到 console.log 模块中是没有 jid 数据的，这点差异需要注意\n# /srv/states/console.sls test echo test:  cmd.run:    - name: echo &quot;111&quot;    - cwd: /tmp    - stateful: Falseconsole.log:  module.run:    - console.log:      - value: &quot;console.log.message&quot;\n\npillar schedule如果在 pillar 的 http_json 中有 schedule，也必须直接跑模块才生效，如下\n# http://xxxx.com/xxx/$&#123;minion_id&#125;/&#123;    &quot;schedule&quot;: &#123;        # 如下为刷新pillar，自动请求本接口        &quot;job_refresh&quot;: &#123;            &quot;function&quot;: &quot;saltutil.refresh_pillar&quot;,            &quot;seconds&quot;: 60,        &#125;,        &quot;an00_test_job&quot;: &#123;            &quot;function&quot;: &quot;console.log&quot;,            &quot;args&quot;: [],            &quot;kwargs&quot;: &#123;                &quot;value&quot;: &quot;xxxx&quot;,            &#125;,            &quot;seconds&quot;: 20,            &quot;once&quot;: None            # &quot;once&quot;: &quot;2022-01-07T11:35:00&quot;        &#125;    &#125;&#125;\n\n","categories":["saltstack"],"tags":["salt","jid"]},{"title":"修改 Linux TTY console 字体","url":"/2017/12/01/%E4%BF%AE%E6%94%B9Linux-TTY-console%E5%AD%97%E4%BD%93/","content":"Linux tty终端字体调整最近在用 KVM nat 虚拟机时，ssh 要上宿主机再跳，觉得麻烦。反而用物理机映射出去的 VNC 连接操作比较多，但是看 console 字体非常难受，发现也是有修改的方法\n\n\n\n安装软件，需要配置好源\n # 红帽系yum install console-setup# debian系apt install console-setup console-terminus\n\n编辑 &#x2F;etc&#x2F;default&#x2F;console-setup 配置文件，下面是一份感觉比较适合的配置  \n# CONFIGURATION FILE FOR SETUPCON# Consult the console-setup(5) manual page.ACTIVE_CONSOLES=&quot;/dev/tty[1-6]&quot;CHARMAP=&quot;UTF-8&quot;CODESET=&quot;Uni2&quot;FONTFACE=&quot;TerminusBold&quot;FONTSIZE=&quot;10x20&quot;VIDEOMODE=# The following is an example how to use a braille font# FONT=&#x27;lat9w-08.psf.gz brl-8x8.psf&#x27;\n\n执行 setupcon 命令加载，centos 和 debian 都有此命令\n\n在 debian 下有 dpkg-reconfigure console-setup 的命令行窗口工具，可以选择字体等，生成的配置文件如上。但是 centos 下没看到有这里工具，按的 debian 生成的文件改效果一样。\n\n\n","categories":["Linux"]},{"title":"网宿 CDN 刷新","url":"/2019/04/08/%E7%BD%91%E5%AE%BF-CDN-%E5%88%B7%E6%96%B0/","content":"网宿 CDN 刷新适用网宿 CDN 刷新，封装了签名，执行 sendRequest 调用服务商接口\n\n\nclass WangsuApi(object):    &quot;&quot;&quot;    :func: 网速CDN访问API刷新缓存的工具类    :create on: 2019-04-08    &quot;&quot;&quot;    def __init__(self, userName, apiKey):        self.date = datetime.datetime.utcnow().strftime(&#x27;%a, %d %b %Y %H:%M:%S GMT&#x27;)        self.apiAddress = &quot;https://open.chinanetcenter.com/&quot;        self.authStr = self.getAuth(userName, apiKey)    def getAuth(self, userName, apikey):        signed_apikey = hmac.new(apikey.encode(&#x27;utf-8&#x27;), self.date.encode(&#x27;utf-8&#x27;), hashlib.sha256).digest()        signed_apikey = base64.b64encode(signed_apikey)        signed_apikey = userName + &quot;:&quot; + signed_apikey.decode()        signed_apikey = base64.b64encode(signed_apikey.encode(&#x27;utf-8&#x27;))        return signed_apikey    def createHeader(self):        headers = &#123;            &#x27;Date&#x27;: self.date,            &#x27;Accept&#x27;: &#x27;application/json&#x27;,            &#x27;Content-type&#x27;: &#x27;application/json&#x27;,            &#x27;Authorization&#x27;: &#x27;Basic &#x27; + self.authStr.decode()        &#125;        return headers    def sendRequest(self, apiUrl, httpBodyParams,  method=&quot;POST&quot;):        httpUrl = self.apiAddress + apiUrl        headers = self.createHeader()        if method.upper() == &#x27;POST&#x27;:            resp = requests.post(httpUrl, data=json.dumps(httpBodyParams), headers=headers)        elif method.upper() == &#x27;GET&#x27;:            resp = requests.get(httpUrl, headers=headers)        else:            return None        # self.printResp(resp)        return resp.json()    @staticmethod    def printResp(resp):        headers_post = dict(resp.headers)        tmp_str = &quot;statusCode:&#123;&#125;\\nDate:&#123;&#125;\\nContent-Length:&#123;&#125;\\nConnection:&#123;&#125;\\nx-cnc-request-id:&#123;&#125;\\n\\n&#123;&#125;&quot;.format(            resp.status_code,            headers_post.get(&#x27;Date&#x27;),            headers_post.get(&#x27;Content-Length&#x27;),            headers_post.get(&#x27;Connection&#x27;),            headers_post.get(&#x27;x-cnc-request-id&#x27;),            resp.text)        print(tmp_str)","categories":["运维"],"tags":["CDN"]},{"title":"记录 DRF 学习片段","url":"/2020/12/18/%E8%AE%B0%E5%BD%95-DRF-%E5%AD%A6%E4%B9%A0/","content":"记录 DRF 学习片段刷官方 tutorial 的记录，官网地址 django-rest-framework\n\n\n开始\n代码不是全部都贴出，仅做记录（不难看懂）\n参考 q1mi老师翻译的文档\n\n示例 models.py 中的 Snippet\nfrom django.db import modelsfrom pygments.lexers import get_all_lexersfrom pygments.styles import get_all_stylesLEXERS = [item for item in get_all_lexers() if item[1]]LANGUAGE_CHOICES = sorted([(item[1][0], item[0]) for item in LEXERS])STYLE_CHOICES = sorted((item, item) for item in get_all_styles())class Snippet(models.Model):    created = models.DateTimeField(auto_now_add=True)    title = models.CharField(max_length=100, blank=True, default=&#x27;&#x27;)    code = models.TextField()    linenos = models.BooleanField(default=False)    language = models.CharField(choices=LANGUAGE_CHOICES, default=&#x27;python&#x27;, max_length=100)    style = models.CharField(choices=STYLE_CHOICES, default=&#x27;friendly&#x27;, max_length=100)    class Meta:        ordering = (&#x27;created&#x27;,)\n\n示例的序列化类 serializers.py 定义，写法的进化过程\nfrom rest_framework import serializersfrom snippets.models import Snippet, LANGUAGE_CHOICES, STYLE_CHOICES# 手写版本class SnippetSerializer(serializers.Serializer):    id = serializers.IntegerField(read_only=True)    title = serializers.CharField(required=False, allow_blank=True, max_length=100)    code = serializers.CharField(style=&#123;&#x27;base_template&#x27;: &#x27;textarea.html&#x27;&#125;)    linenos = serializers.BooleanField(required=False)    language = serializers.ChoiceField(choices=LANGUAGE_CHOICES, default=&#x27;python&#x27;)    style = serializers.ChoiceField(choices=STYLE_CHOICES, default=&#x27;friendly&#x27;)    def create(self, validated_data):        &quot;&quot;&quot;        根据提供的验证过的数据创建并返回一个新的`Snippet`实例。        &quot;&quot;&quot;        return Snippet.objects.create(**validated_data)    def update(self, instance, validated_data):        &quot;&quot;&quot;        根据提供的验证过的数据更新和返回一个已经存在的`Snippet`实例。        &quot;&quot;&quot;        instance.title = validated_data.get(&#x27;title&#x27;, instance.title)        instance.code = validated_data.get(&#x27;code&#x27;, instance.code)        instance.linenos = validated_data.get(&#x27;linenos&#x27;, instance.linenos)        instance.language = validated_data.get(&#x27;language&#x27;, instance.language)        instance.style = validated_data.get(&#x27;style&#x27;, instance.style)        instance.save()        return instance# 使用封装的序列化器class SnippetSerializer(serializers.ModelSerializer):    class Meta:        model = Snippet        fields = (&#x27;id&#x27;, &#x27;title&#x27;, &#x27;code&#x27;, &#x27;linenos&#x27;, &#x27;language&#x27;, &#x27;style&#x27;)\n\n\n\n序列化和反序列化基础的序列化过程，Snippet 是个表 model，以及它的序列化类 SnippetSerializer，先忽略他们定义的内容，看序列化做的事情\nfrom snippets.models import Snippetfrom snippets.serializers import SnippetSerializerfrom rest_framework.renderers import JSONRendererfrom rest_framework.parsers import JSONParsersnippet = Snippet(code=&#x27;foo = &quot;bar&quot;\\n&#x27;)snippet.save()snippet = Snippet(code=&#x27;print &quot;hello, world&quot;\\n&#x27;)snippet.save()serializer = SnippetSerializer(snippet)  # 将model对象转成python原生数据类型print(serializer.data)  # 字典数据，&#123;&#x27;id&#x27;: 1, ...&#125;# SnippetSerializer(Snippet.objects.all(), many=True)  # 不是单个对象而是querySet时，传many=True，可以序列化多个content = JSONRenderer().render(serializer.data)  # 将数据转成JSONprint(content)  # b&#x27;&#123;&quot;id&quot;: 2, ...&#125;&#x27;\n\n反序列化过程，类似 forms 验证，接上以上代码\nimport iostream = io.BytesIO(content)data = JSONParser().parse(stream)          # 接受JSON数据，模拟从请求中读取的数据serializer = SnippetSerializer(data=data)  # 传入反序列化的数据serializer.is_valid()      # 校验数据是否缺少serializer.validated_data  # 校验字段内容serializer.save()          # 保存对象\n\n\n\n序列化类介绍 rest_framework.serializers 中创建序列化器的快捷方式\nModelSerializer类似 django forms 中的 ModelForm\n\n可简单定义需要序列化的字段\n默认实现的 create 和 update 方法，用于增加和更新\n\nclass SnippetSerializer(serializers.ModelSerializer):    class Meta:        model = Snippet        fields = (&#x27;id&#x27;, &#x27;title&#x27;, &#x27;code&#x27;, &#x27;linenos&#x27;, &#x27;language&#x27;, &#x27;style&#x27;)\n\nHyperlinkedModelSerializer\n可以更好的处理对象实体之间的关系\n\n这个序列化器在有关联字段的时候，展示的不是 ID，而是相关对象的 URL，与 ModelSerializer 有区别\n\n默认不包括 ID 字段\n数据含一个url字段，使用HyperlinkedIdentityField\n使用的关联使用 HyperlinkedRelatedField 而不是 PrimaryKeyRelatedField (它会把关联的对象仅给出 ID 返回)\n\nPrimaryKeyRelatedField得到此对象对应的多个关联对象ID，如解析 User 实例的 Snippet 对象：\nclass UserSerializer(serializers.ModelSerializer):    snippets = serializers.PrimaryKeyRelatedField(many=True, queryset=Snippet.objects.all())    # 返回如: &#123;&quot;id&quot;:1, &quot;snippets&quot;: [1,2,3], ...&#125;    class Meta:        model = User        fields = (&#x27;id&#x27;, &#x27;username&#x27;, &#x27;snippets&#x27;)\n\n\n\nHyperlinkedRelatedField得到此对象对应的多个关联对象实体 URL（需要传入 view_name ）\n如下，解析 User 实例的 Snippet 对象，snippet-detail 对应 urls.py 中的 name 参数（参考后面关系和超链接部分）\nclass UserSerializer(serializers.HyperlinkedModelSerializer):    snippets = serializers.HyperlinkedRelatedField(many=True, view_name=&#x27;snippet-detail&#x27;, read_only=True)    # 返回如: &#123;&quot;id&quot;:1, &quot;snippets&quot;: [&quot;http://127.0.0.1:8000/snippets/1/&quot;, ...], ...&#125;    class Meta:        model = User        fields = (&#x27;id&#x27;, &#x27;username&#x27;, &#x27;snippets&#x27;, &#x27;url&#x27;)\n\n\n\n请求和响应请求对象request.POST  # 只试用与POST方法request.data  # 处理任意数据  适用于&#x27;POST&#x27;，&#x27;PUT&#x27;和&#x27;PATCH&#x27;方法\n\n响应对象继承了 django 中的 SimpleTemplateResponse ，但未渲染模板，会使用内容协商来确定返回给客户端的正确内容类型\n# from rest_framework.response import Responsereturn Response(data)\n\n状态码命名了 HTTP 状态码，便于使用\n# from rest_framework import statusreturn Response(data, status=status.HTTP_201_CREATED)\n\nAPI 视图包装器均可限制访问接口的方法，否则抛出 405 Method Not Allowed 错误\n\nAPIView 供类视图继承，与 django 的类视图相似，路由中传入 as_view，并内置dispatch方法，若请求方法未实现将抛出 405\napi_view 给函数视图使用的装饰器，默认仅允许 GET 请求，可传入 method 列表\naction 给视图集增加动作，如一个 viewSet 默认可能有detail、list 等方法，若它注册路由时设置 basename=snippets 将自动生成可解析的 snippets-detail 等动作。action可以增加自定义的动作，如下有增加 snippets-highlight 的示例，默认后缀为被装饰的方法名（viewSet 路由注册部分看 viewsets 视图集和 routers）\n\nfrom rest_framework.views import APIViewfrom rest_framework.decorators import api_view, actionclass Demo(APIView):    def get(self, request, format=None):        pass    def post(self, request, format=None):        pass@api_view([&#x27;GET&#x27;, &#x27;POST&#x27;])def api_func_demo(request)    if request.method == &quot;GET&quot;:        pass    elif request.method == &quot;POST&quot;:        passclass SnippetViewSet(viewsets.ModelViewSet):    &quot;&quot;&quot;    Snippet的常规增删查改、列表视图    还提供了一个额外的`highlight`操作    &quot;&quot;&quot;    queryset = Snippet.objects.all()    serializer_class = SnippetSerializer    permission_classes = (permissions.IsAuthenticatedOrReadOnly, IsOwnerOrReadOnly)    # 注意action装饰的方法替代了上面的SnippetHighlight    # 默认仅允许GET方法，methods指定    # detail表示是否适用于实例详细信息，需要实例参数pk=1这种情况    # 使用Router且basename=snippet时，此动作解析为&#x27;snippet-highlight&#x27;，与序列化中的view_name对应    @action(renderer_classes=[renderers.StaticHTMLRenderer], detail=True, methods=[&#x27;get&#x27;])    def highlight(self, request, *args, **kwargs):        snippet = self.get_object()        return Response(snippet.highlighted)    def perform_create(self, serializer):        serializer.save(owner=self.request.user)\n\n\n\n类视图mixins 混合类在 rest_framework.mixins 中，定义了很多可服用的行为，对应我们接口处理 model 的查询、修改、序列化、反序列化、保存等操作，它们需要和 generics.GenericAPIView 一起继承，搭配使用\n\nCreateModelMixin 用于创建对象，封装了 create ，调用序列化类校验数据、保存等操作\nListModelMixin 用于列出多个对象，封装了 list 方法，从 model 中查询数据，并序列化成数据、分页返回给请求\nRetrieveModelMixin  用于查找单个对象示例，封装了 retrieve 方法，返回单个对象已序列化的数据\nUpdateModelMixin 用于更新数据，封装了 update 方法，更新单个对象的数据\nDestroyModelMixin  用于删除对象，封装了 destroy 方法，查找对象实例进行删除\n\n继承了上面多个类将有多个类的功能，如下将 mixin 中的封装对应 HTTP 方法的处理函数。\nfrom snippets.models import Snippetfrom snippets.serializers import SnippetSerializerfrom rest_framework import mixinsfrom rest_framework import genericsclass SnippetList(mixins.ListModelMixin,                  mixins.CreateModelMixin,                  generics.GenericAPIView):    queryset = Snippet.objects.all()    serializer_class = SnippetSerializer    def get(self, request, *args, **kwargs):        return self.list(request, *args, **kwargs)    def post(self, request, *args, **kwargs):        return self.create(request, *args, **kwargs)\n\n\n\ngenerics 通用类在 rest_framework.generics 中有已经内置混合好的通用视图，用它可以将HTTP方法与 Mixin 封装对应上。\n\nGenericAPIView 是通用视图中最基础的类，继承 views.APIView，带有序列化、分页、对象过滤等封装\n\n使用通用类视图写接口，代码量将变的很少，逻辑就像配置一样，设置 queryset 和序列化类就可以了\nfrom snippets.models import Snippetfrom snippets.serializers import SnippetSerializerfrom rest_framework import genericsclass SnippetList(generics.ListCreateAPIView):    queryset = Snippet.objects.all()    serializer_class = SnippetSerializerclass SnippetDetail(generics.RetrieveUpdateDestroyAPIView):    queryset = Snippet.objects.all()    serializer_class = SnippetSerializer\n\n基于上面两个视图，修改 urls.py，增加 URL\nfrom django.contrib import adminfrom django.urls import pathfrom django.conf.urls import includefrom snippets import viewsurlpatterns = [    path(&#x27;admin/&#x27;, admin.site.urls),    # 方便调试接口，需要注意namespace    path(&#x27;api-auth/&#x27;, include(&#x27;rest_framework.urls&#x27;, namespace=&#x27;rest_framework&#x27;)),    # 注意path的传参，参数名为pk，类型int    path(&#x27;snippets/&#x27;, views.SnippetList.as_view()),    path(&#x27;snippets/&lt;int:pk&gt;/&#x27;, views.SnippetDetail.as_view()),]\n\n运行测试（创建用户访问 http://localhost:8000/api-auth/login 再打开 &#x2F;snippets&#x2F; 页面）\n\n&#x2F;snippets&#x2F; 列表数据\n\n\n&#x2F;snippets&#x2F;1&#x2F; 可以查看id&#x3D;1的对象，还显示了其他可操作的方法\n\n\n\nviewsets 视图集和 routers\n应该是最常用的方式，包括这种方式的路由\n\nViewSet 和 View 很类似，但它提供的是读取、更新、创建、删除的操作，而不是类里面 get 和 post 这种请求处理的函数。ViewSet 类可以不自己写 URL，使用 rest_framework.routers 中的路由类，可以自动生成 URL\n内置了几个常用类，继承自 mixin 和 generics 组成了几种常用的搭配\n# 提供as_view方法# 将GET、POST、PUT、DELETE请求映射到了子类的list|retrieve、create、update|partial_update、destory等方法class ViewSetMixin# 不包含任何操作class ViewSet(ViewSetMixin, views.APIView)# 包含GenericAPIView功能的通用视图集class GenericViewSet(ViewSetMixin, generics.GenericAPIView)# 只读方法，可查看多个和单个实例数据class ReadOnlyModelViewSet(mixins.RetrieveModelMixin, mixins.ListModelMixin, GenericViewSet)# 包含CRUD方法class ModelViewSet(mixins.CreateModelMixin, mixins.RetrieveModelMixin, mixins.UpdateModelMixin,                    mixins.DestroyModelMixin, mixins.ListModelMixin, GenericViewSet)\n\n使用示例，由上面的函数签名可以知道，ReadOnlyModelViewSet 只有读的方法\nfrom rest_framework import viewsetsclass UserViewSet(viewsets.ReadOnlyModelViewSet):    &quot;&quot;&quot;    此视图自动提供`list`和`detail`操作。    &quot;&quot;&quot;    queryset = User.objects.all()    serializer_class = UserSerializer\n\n增加路由的方式\nfrom django.contrib import adminfrom django.urls import pathfrom django.conf.urls import includefrom rest_framework.routers import DefaultRouterfrom snippets import views# 将根据basename生成解析，默认反解的api-root是接口目录路径router = DefaultRouter()# 各个方法将解析为类似&#x27;snippet-detail&#x27;等，@action装饰的动作如&#x27;snippet-方法名&#x27;router.register(&#x27;users&#x27;, views.UserViewSet, basename=&#x27;snippet&#x27;)router.register(&#x27;snippets&#x27;, views.SnippetViewSet, basename=&#x27;user&#x27;)# 将router2的路由扩展到router中router2 = DefaultRouter()router.registry.extend(router.registry)urlpatterns = [    path(&#x27;admin/&#x27;, admin.site.urls),    path(&#x27;api-auth/&#x27;, include(&#x27;rest_framework.urls&#x27;, namespace=&#x27;rest_framework&#x27;)),    path(&#x27;&#x27;, include(router.urls)),]\n\n\n\n权限内置权限类在 rest_framework.permissions 中有权限验证相关的内容\n通常权限继承自 BasePermission 基类，主要定义了两个方法，均返回 Bool 值，用于区分是否有权限\n# 权限的基类，定义了 has_permission 和 has_object_permission 两个方法class BasePermission(metaclass=BasePermissionMetaclass):    &quot;&quot;&quot;    A base class from which all permission classes should inherit.    &quot;&quot;&quot;    def has_permission(self, request, view):  # 视图级别权限        &quot;&quot;&quot;        Return `True` if permission is granted, `False` otherwise.        &quot;&quot;&quot;        return True    def has_object_permission(self, request, view, obj):  # 对象级别权限，多传递了一个对象参数        &quot;&quot;&quot;        Return `True` if permission is granted, `False` otherwise.        &quot;&quot;&quot;        return True\n\n自定义权限只需要继承 BasePermission 在两个方法中做判断，然后在视图中指定即可。\nrest_framework.permissions 中内置了以下几种权限：\n\nIsAuthenticated\nIsAdminUser\nIsAuthenticatedOrReadOnly\nDjangoModelPermissions\nDjangoModelPermissionsOrAnonReadOnly\nDjangoObjectPermissions\n\n自定义权限例如在 snippets model 增加了 owner 字段关联用户，并且增加 permissions.py，实现只有 owner 用户可以修改自己的 snippets，其他情况均只读的权限类\nfrom rest_framework import permissionsclass IsOwnerOrReadOnly(permissions.BasePermission):    &quot;&quot;&quot;    自定义权限只允许对象的所有者编辑它。    &quot;&quot;&quot;    def has_object_permission(self, request, view, obj):        # 读取权限允许任何请求，        # 所以我们总是允许GET，HEAD或OPTIONS请求。        if request.method in permissions.SAFE_METHODS:            return True        # 只有该snippet的所有者才允许写权限。        return obj.owner == request.user\n\n修改视图，导入新增的权限类并设置，这将只有 owner 可以修改\nfrom snippets.permissions import IsOwnerOrReadOnlyclass SnippetDetail(generics.RetrieveUpdateDestroyAPIView):    queryset = Snippet.objects.all()    serializer_class = SnippetSerializer    permission_classes = (permissions.IsAuthenticatedOrReadOnly, IsOwnerOrReadOnly)\n\n\n\n关系和超链接为 API 根路径设置一个入口，snippets/views.py 中增加一个函数视图，使用 reverse 返回完全限定的 URL\nfrom rest_framework.decorators import api_viewfrom rest_framework.response import Responsefrom rest_framework.reverse import reverse@api_view([&#x27;GET&#x27;])def api_root(request, format=None):    return Response(&#123;        &#x27;users&#x27;: reverse(&#x27;user-list&#x27;, request=request, format=format),        &#x27;snippets&#x27;: reverse(&#x27;snippet-list&#x27;, request=request, format=format)    &#125;)\n\n修改 urls.py 给 URL 加上 name，用于 reverse 解析\nfrom django.contrib import adminfrom django.urls import pathfrom django.conf.urls import includefrom snippets import viewsurlpatterns = [    path(&#x27;admin/&#x27;, admin.site.urls),    path(&#x27;api-auth/&#x27;, include(&#x27;rest_framework.urls&#x27;, namespace=&#x27;rest_framework&#x27;)),    path(&#x27;snippets/&#x27;, views.SnippetList.as_view(), name=&#x27;snippet-list&#x27;),    path(&#x27;snippets/&lt;int:pk&gt;/&#x27;, views.SnippetDetail.as_view(), name=&#x27;snippet-detail&#x27;),    path(&#x27;snippets/&lt;int:pk&gt;/highlight/&#x27;, views.SnippetHighlight.as_view(), name=&#x27;snippet-highlight&#x27;),    path(&#x27;users/&#x27;, views.UserList.as_view(), name=&#x27;user-list&#x27;),    path(&#x27;users/&lt;int:pk&gt;/&#x27;, views.UserDetail.as_view(), name=&#x27;user-detail&#x27;),    path(&#x27;&#x27;, views.api_root)]\n\n\n此时访问接口根路径可以得到接口列表\n \n\n访问连接得到实体 URL 链接（代码未贴出，参考项目）\n\n\n\n","categories":["Python"],"tags":["DRF"]},{"title":"记录 django 中的查询优化","url":"/2020/12/09/%E8%AE%B0%E5%BD%95-django-%E4%B8%AD%E7%9A%84%E5%85%B3%E8%81%94%E6%9F%A5%E8%AF%A2/","content":"记录 django 中的查询优化恰当的使用 select_related 和 prefetch_related 方法，可以减少数据库重复查询的次数  \n两种方法均支持双下划线指定需要查询的关联对象的字段名\n\nselect_related\n适合一对一，一对多的外键字段\n\n\nprefetch_related:\n适合多对多字段、外键反查(related_name)的情况\n在方法中使用 Prefetch 可以增加查询条件\n执行两次数据库查询\n\n\n\n\n\n示例 model \nclass Article(models.Model):    &quot;&quot;&quot;文章模型&quot;&quot;&quot;    title = models.CharField(&#x27;标题&#x27;, max_length=200, db_index=True)    category = models.ForeignKey(&#x27;Category&#x27;, verbose_name=&#x27;分类&#x27;, on_delete=models.CASCADE, blank=False, null=False)    tags = models.ManyToManyField(&#x27;Tag&#x27;, verbose_name=&#x27;标签集合&#x27;, blank=True)\n\nselect_related适用于外键这种一对一，一对多的情况，执行其实是生成一条 inner join 的 SQL 语句，一次查询获取对象和关联对象的内容。  \n使用方法如下，遍历结果集 articles 调用 item.category.name  获取信息时，不产生另外的查询：\n# 获取对象的同时，获取相关的字段信息articles = Article.objects.all().select_related(&#x27;category&#x27;)articles = Article.objects.all().select_related(&#x27;category__name&#x27;)articles = Article.objects.all().select_related(&#x27;category&#x27;, &#x27;author__name&#x27;)# 不指定字段，直接使用select_related()将查出所有关联信息articles = Article.objects.all().select_related()# 再加 filter 组合，顺序不重要articles = Article.objects.all().filter(pk__in=(1, 2, 3)).select_related()\n\n在模板中遍历的示例，未使用 select_related 方式：\n&lt;ul&gt;&#123;% for article in articles %&#125;    &lt;li&gt;&#123;&#123; article.title &#125;&#125; &lt;/li&gt;    &#123;# category.name 每次都将生成一条查询。若按上述方法，这里将从对象中直接拿到，不用额外的一次查询 #&#125;    &lt;li&gt;&#123;&#123; article.category.name &#125;&#125;&lt;/li&gt;    &lt;li&gt;            &#123;% for tag in article.tags.all %&#125;             &#123;&#123; tag.name &#125;&#125;,  &#123;# 每次都将生成一条查询 #&#125;             &#123;% endfor %&#125;    &lt;/li&gt;&#123;% endfor %&#125;&lt;/ul&gt;\n\n\nprefetch_related对于多对多的字段，不能使用 select_related 方式，避免多对多字段 join 后结果很大。prefect_related 就是用于多对多关系的，也可以用于外键的反查(related_name)\n使用方法如下，遍历结果集，拿关联的 tags 对象信息时，不用每次遍历执行一次查询\n# 获取对象列表并加载关联 tags 对象的 name 字段articles = Article.objects.all().prefetch_related(&#x27;tags__name&#x27;)# 获取关联对象 tags 的所有信息articles = Article.objects.all().prefetch_related(&#x27;tags&#x27;)# 上面模版中的例子，可以使用如下查询，加载 category 和 tags 对象信息articles = Article.objects.all().select_related(&#x27;category__name&#x27;).prefetch_related(&#x27;tags&#x27;)\n\n在 prefetch_related 中，还可以对所查的关联对象进行过滤\n# 获取文章列表及每篇文章相关的名字以P开头的tags对象信息Article.objects.all().prefetch_related(    Prefetch(&#x27;tags&#x27;, queryset=Tag.objects.filter(name__startswith=&quot;P&quot;)))# 文章列表及每篇文章的名字以P开头的tags对象信息, 放在article_p_tag列表Article.objects.all().prefetch_related(    Prefetch(&#x27;tags&#x27;, queryset=Tag.objects.filter(name__startswith=&quot;P&quot;)),to_attr=&#x27;article_p_tag&#x27;)","categories":["Python"],"tags":["django"]},{"title":"记录 django 外键关联统计","url":"/2020/12/09/%E8%AE%B0%E5%BD%95-django-%E5%A4%96%E9%94%AE%E5%85%B3%E8%81%94%E7%BB%9F%E8%AE%A1/","content":"记录 django 外键关联统计记录几个统计问题实例\n\n\n\n从主机表统计每个主机的装服数量，ServerModel 的 host 字段关联 HostModel，在 1.11 中实测可用。统计后的对象将额外生成一个 __count 后缀的字段，并且可以用于排序\n\nHostModel.objects.using(gameName).values(&#x27;id&#x27;).annotate(Count(&#x27;servermodel&#x27;)).order_by(&#x27;servermodel__count&#x27;)添加过滤&gt;&gt;&gt; from django.db.models import Count, When, Case&gt;&gt;&gt; person = Person.objects.annotate(Count(Case(When(task_set__is_done=True, then=0))).first()&gt;&gt;&gt; person.task_set__count# 2.0往后&gt;&gt;&gt; from django.db.models import Count, Q&gt;&gt;&gt; person = Person.objects.annotate(...             Count(&#x27;task_set&#x27;, filter=Q(task_set__is_done=True))...         ).first()&gt;&gt;&gt; person.task_set__count10\n\n\nsum(x) .. group by field如下PerformanceStatsModel 有字段[host_id(外键),mem1,mem2,mem3,created_date]，想得到某机器在多天时各 mem 字段统计的总和数据。这里的特点是 annotate 注解（分组）会按 order_by 字段，若想按host_id分组，model中默认可能会是 &quot;-id&quot; 排序。我们这里的结果必须用 .values(&quot;host&quot;).order_by(&quot;host&quot;) 才能实现像 SQL 中的 group by host_id，最后 annotate 中的参数与 select sum(mem1) as mem_1, sum(mem2) as mem_2, sum(mem3) as mem_3 from ... 相同\n\nPerformanceStatsModel.objects.using(gameName).filter(    host__in=hosts, mem3=0, created_date__gte=d).values(&quot;host&quot;).order_by(&quot;host&quot;).annotate(        mem_1=Sum(&quot;mem1&quot;), mem_2=Sum(&quot;mem2&quot;), mem_3=Sum(&quot;mem3&quot;)    )# 结果集是一个个统计好的字典：&#123;&#x27;host&#x27;: 175, &#x27;mem_1&#x27;: 2880, &#x27;mem_2&#x27;: 0, &#x27;mem_3&#x27;: 0&#125;&#123;&#x27;host&#x27;: 178, &#x27;mem_1&#x27;: 2880, &#x27;mem_2&#x27;: 0, &#x27;mem_3&#x27;: 0&#125;# 结果集中的字典是可以用filter过滤的，相当于SQL中group by后面的: having maxMem = 0PerformanceStatsModel.objects.using(gameName).filter(host__in=hosts, created_date__gte=d).values(&quot;host&quot;).order_by(&quot;host&quot;).annotate(maxMem=Sum(&quot;mem3&quot;)).filter(maxMem=0)","categories":["Python"],"tags":["django"]},{"title":"记录 golang context","url":"/2021/10/16/%E8%AE%B0%E5%BD%95-golang-context/","content":"记录 golang context有四种\n\ncontext.WithCancel\n可取消的context\n\n\ncontext.WithDeadline\n在某时间结束的context\n\n\ncontext.WithTimeout(context.Background(), 2 * time.Second)\n设置超时的context，也返回 ctx 和 cancel，可以等待自动超时，也可以提前执行cancel，ctx.Done都可以接收到值\n\n\ncontext.WithValue\n值\n\n\n\n\n\nWithCancel示例package mainimport (\t&quot;context&quot;\t&quot;fmt&quot;\t&quot;sync&quot;\t&quot;time&quot;)var wg sync.WaitGroupfunc cpuInfo(ctx context.Context) &#123;\tdefer wg.Done()\t// 再生成子context，给memoryInfo，不接收cancel方法，当父context被cancel时也能接受到ctx.Done消息\tctx2, _ := context.WithCancel(ctx)\tgo memoryInfo(ctx2)\tfor &#123;\t\tselect &#123;\t\tcase &lt;-ctx.Done():\t\t\tfmt.Println(&quot;==&gt; 退出CPU监控&quot;)\t\t\treturn\t\tdefault:\t\t\ttime.Sleep(time.Second)\t\t\tfmt.Println(&quot;获取CPU信息&quot;)\t\t&#125;\t&#125;&#125;func memoryInfo(ctx context.Context) &#123;\tdefer wg.Done()\tfor &#123;\t\tselect &#123;\t\tcase &lt;-ctx.Done():\t\t\tfmt.Println(&quot;==&gt; 退出内存监控&quot;)\t\t\treturn\t\tdefault:\t\t\ttime.Sleep(time.Second)\t\t\tfmt.Println(&quot;获取内存信息&quot;)\t\t&#125;\t&#125;&#125;func main() &#123;\t// 主goroutine生成父ctx，默认用Background生成父context\tctx, cancel := context.WithCancel(context.Background())\twg.Add(2)\tgo cpuInfo(ctx)\ttime.Sleep(time.Second * 5)\t// 父context，调用cancel时，同时会调用基于此context创建的子context的cancel方法\t// 这里的效果也就是一起退出\tcancel()\twg.Wait()&#125;\n输出\n获取内存信息获取CPU信息获取CPU信息获取内存信息获取内存信息获取CPU信息获取CPU信息获取内存信息获取CPU信息==&gt; 退出CPU监控获取内存信息==&gt; 退出内存监控\n\n\nWithTimeout示例package mainimport (\t&quot;context&quot;\t&quot;fmt&quot;\t&quot;sync&quot;\t&quot;time&quot;)var wg sync.WaitGroupfunc cpuInfo(ctx context.Context) &#123;\tdefer wg.Done()\tfor &#123;\t\tselect &#123;\t\tcase &lt;-ctx.Done():\t\t\tfmt.Println(&quot;==&gt; 退出CPU监控&quot;)\t\t\treturn\t\tdefault:\t\t\ttime.Sleep(time.Second)\t\t\tfmt.Println(&quot;获取CPU信息&quot;)\t\t&#125;\t&#125;&#125;func main() &#123;\t// 主goroutine生成父ctx，默认用Background生成父context\t// 也返回cancel，不调用不接收\tctx, _ := context.WithTimeout(context.Background(), time.Second*3)\twg.Add(1)\tgo cpuInfo(ctx)\t// timeout context的cancel只能在未超时前执行\t//cancel()\twg.Wait()&#125;\n\n输出\n获取CPU信息获取CPU信息获取CPU信息==&gt; 退出CPU监控","categories":["Golang"]},{"title":"记录 golang 信号处理","url":"/2022/01/08/%E8%AE%B0%E5%BD%95-golang-%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/","content":"记录 golang 信号处理\n 系统信号处理的方式，纯内置的处理。\n\n逻辑是定义一个容量为 1 的 chan 接收系统信号(os.Signal)，触发指定信号时往 chan 接收到值，以此判断做相应的处理。\n\n\npackage mainimport (\t&quot;fmt&quot;\t&quot;os&quot;\t&quot;os/signal&quot;\t&quot;syscall&quot;\t&quot;time&quot;)func main() &#123;\tvar code = 1\t// 容量是1的channel，接受信号\tvar sc = make(chan os.Signal, 1)\t// 触发这些信号时，通知 sc channel\tsignal.Notify(sc, syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT)\t// 其他go routine执行操作，这里每秒执行打印时间\tticker := time.NewTicker(time.Second * 1)\tgo func() &#123;\t\tfor &#123;\t\t\t&lt;-ticker.C\t\t\tfmt.Printf(&quot;  ++&gt; working time: %v\\n&quot;, time.Now().Format(&quot;2006-01-02 15:04:05.000&quot;))\t\t&#125;\t&#125;()\t// 信号接收和相应的处理LOOP:\tfor &#123;\t\t// 没接收到信号时，这里会阻塞\t\tsig := &lt;-sc\t\tfmt.Println(&quot;recevived signal:&quot;, sig.String())\t\tswitch sig &#123;\t\tcase syscall.SIGTERM, syscall.SIGINT, syscall.SIGQUIT:\t\t\t// 收到信号结束的，这里可能还做一些清理操作，退出循环\t\t\tcode = 0\t\t\tbreak LOOP\t\tcase syscall.SIGHUP:\t\t\tfmt.Println(&quot;==&gt; reload config&quot;)\t\tdefault:\t\t\t// 跳出循环\t\t\tbreak LOOP\t\t&#125;\t&#125;\t// 退出\tfmt.Println(&quot;==&gt; server exit&quot;)\tos.Exit(code)&#125;","categories":["Golang"]},{"title":"记录前端 flex 布局","url":"/2020/07/11/%E8%AE%B0%E5%BD%95%E5%89%8D%E7%AB%AF-flex-%E5%B8%83%E5%B1%80/","content":"记录前端 flex 布局弹性布局，任何一个容器都可以指定为 flex 布局。当为父盒子设定为 flex 布局后，子元素的float、clear和vertical-align 属性将失效。\n采用flex布局的元素，称为flex container（容器），它的所有子元素称为flex item（项目）\nflex 布局原理：通过给父盒子添加flex属性，来控制子盒子的位置和排列方式。\n\n\n容器属性容器属性是给父元素添加的属性，通常有下面几种：\n\nflex-direction 设置主轴的方向\njustify-content 设置主轴上子元素的排列方式\nflex-wrap 设置子元素是否换行\nalign-content 设置侧轴上的子元素排列方式（多行）\nalign-items 设置侧轴上子元素的排列方式（单行）\nflex-flow 复合属性，相当于同时设置了flex-direction和 flex-wrap\n\nflex-directionflex 有两个方向的概念，以默认方式介绍：\n\n水平方向（默认）：主轴、X轴、行，默认水平向右\n垂直方向（默认）：侧轴、Y轴、列，默认垂直向下\n但是主轴和侧轴会变换，就看 flex-direction 定义那个是主轴\n\n而 flex-direction 是控制主轴方向的属性（控制了flex item的排列方向）。主轴和侧轴是会变化的，就看flex-direction设置谁为主轴，剩下的就是侧轴，子元素是根据主轴方向排列的。\nflex-direction属性：\n\n\n\nfex-direction 属性值\n含义\n\n\n\nrow\n从左到右，默认值\n\n\nrow-reverse\n从右到左\n\n\ncolumn\n从上到下\n\n\ncolumn-reverse\n从下到上\n\n\n示例代码\n&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;style&gt;    div &#123;        display: flex;        /* flex-direction: row-reverse; */        flex-direction: column;        width: 800px;        height: 500px;        background-color: burlywood;    &#125;    div span &#123;        width: 200px;        height: 100px;        background-color: cadetblue;    &#125;&lt;/style&gt;&lt;body&gt;    &lt;div&gt;        &lt;span&gt;1&lt;/span&gt;        &lt;span&gt;2&lt;/span&gt;        &lt;span&gt;3&lt;/span&gt;    &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\n\n\n\njustify-content用于设置主轴上子元素的对齐方式，使用此属性前一定要确定好主轴\n\n\n\nJustify-content 属性值\n含义\n\n\n\nflex-start\n从头部开始（如果主轴是x轴，则从左到右），默认\n\n\nflex-end\n从尾部开始排列\n\n\ncenter\n在主轴居中对齐（若主轴是x轴，则水平居中）\n\n\nspace-around\n平分剩余空间（每个子元素左右间隙一样，相当于左右margin一样）\n\n\nspace-between\n先两边贴边，再平分剩余空间，元素间间隔一样（重要）\n\n\nspace-evenly\n平分剩余空间到元素间的间隔，每个元素间的间隔相等，不贴边\n\n\n示例\n&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;style&gt;    div &#123;        display: flex;        /* justify-content: space-between;*/        /* justify-content: space-around; */        justify-content: space-evenly;        width: 800px;        height: 500px;        background-color: burlywood;    &#125;    div span &#123;        width: 200px;        height: 100px;        background-color: cadetblue;    &#125;&lt;/style&gt;&lt;body&gt;    &lt;div&gt;        &lt;span&gt;1&lt;/span&gt;        &lt;span&gt;2&lt;/span&gt;        &lt;span&gt;3&lt;/span&gt;        &lt;span&gt;4&lt;/span&gt;        &lt;span&gt;5&lt;/span&gt;    &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\n\n\n\nflex-wrapflex 布局中，默认子元素是不换行的，flex item 都排在一条轴线上。如果子元素太多，flex 会压缩元素的大小（宽或高）让子元素继续挤在一行，但是可以用flex-wrap 控制是否换行。\n\n\n\nFlex-wrap 属性值\n含义\n\n\n\nnowrap\n不换行，默认\n\n\nwrap\n换行\n\n\n示例\n&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;style&gt;    div &#123;        display: flex;        flex-wrap: wrap;  /* 默认nowrap，不换行 */        width: 800px;        height: 500px;        background-color: burlywood;    &#125;    div span &#123;        width: 200px;        height: 100px;        background-color: cadetblue;    &#125;&lt;/style&gt;&lt;body&gt;    &lt;div&gt;        &lt;span&gt;1&lt;/span&gt;        &lt;span&gt;2&lt;/span&gt;        &lt;span&gt;3&lt;/span&gt;        &lt;span&gt;4&lt;/span&gt;        &lt;span&gt;5&lt;/span&gt;    &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\n\n\n\nalign-items控制侧轴上（默认侧轴是y轴）子元素的排列方式（单行），在子项是单项的时候使用，注意确定侧轴。\n\n\n\nAlign-items 属性值\n含义\n\n\n\nflex-start\n从上到下（如默认侧轴是y轴），默认值\n\n\nflex-end\n从下到上（如默认侧轴是y轴）\n\n\ncenter\n挤在一起居中（垂直居中）\n\n\nstretch\n拉伸子元素与父元素一样高（若子元素设置了高度会无效果）\n\n\n示例，实现水平垂直居中，分别控制主轴和侧轴居中\n&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;style&gt;    div &#123;        display: flex;        justify-content: center;        align-items: center;        /* align-items: stretch; */        width: 800px;        height: 500px;        background-color: burlywood;    &#125;    div span &#123;        width: 200px;        height: 100px;        background-color: cadetblue;    &#125;&lt;/style&gt;&lt;body&gt;    &lt;div&gt;        &lt;span&gt;1&lt;/span&gt;        &lt;span&gt;2&lt;/span&gt;        &lt;span&gt;3&lt;/span&gt;    &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\n\n\n\nalign-content设置子项在侧轴上的排列方式，并且只能用于子项出现换行的情况（多行），子项单行是没效果的。\n\n\n\nalign-content 属性值\n含义\n\n\n\nflex-start\n在侧轴的头部开始排列，默认\n\n\nflex-end\n在侧轴的尾部开始排列\n\n\ncenter\n在侧轴的中间显示\n\n\nspace-around\n子项在侧轴平分剩余空间（相当于两个子项有相同的margin值）\n\n\nspace-between\n子项在侧轴先分布在两头（贴边），再平分剩余空间\n\n\nstretch\n设置子项元素高度平分父元素高度\n\n\n与 align-items 的区别：\n\nalign-items 适用于单行情况，只有上对齐、下对齐、居中和拉伸几种方式\nalign-content 适用于子项多行（换行）的情况，单行无效，属性值较多，如上\n所以单行用 align-items，多行用 align-content，同时注意确定侧轴\n\n示例，可以实现多行子项的各种对齐排列\n&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;style&gt;    div &#123;        display: flex;        flex-wrap: wrap;        justify-content: center;        /* align-content: space-between;  /* 一行贴上沿，一行贴着下沿 */        /* align-content: stretch;  /* 一行贴上沿，一行贴着下沿 */         align-content: center;  /* 水平、垂直居中 */        width: 800px;        height: 500px;        background-color: burlywood;    &#125;    div span &#123;        width: 250px;        height: 100px;        background-color: cadetblue;    &#125;&lt;/style&gt;&lt;body&gt;    &lt;div&gt;        &lt;span&gt;1&lt;/span&gt;        &lt;span&gt;2&lt;/span&gt;        &lt;span&gt;3&lt;/span&gt;        &lt;span&gt;4&lt;/span&gt;        &lt;span&gt;5&lt;/span&gt;        &lt;span&gt;6&lt;/span&gt;    &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\n\n\n\nflex-flowflex-flow 属性是 flex-direction 和 flex-wrap 属性的复合属性，如下写法：\ndiv &#123;\tflex-direction: row;\tflex-wrap: wrap;&#125;/* 与下面等效 */div &#123;\tflex-flow: row wrap;&#125;\n\n\n\n子项属性\nflex 子项占的份数\n\nalign-self 控制自己在侧轴上的排列方式\n\norder 属性定义子项的排列顺序（前后顺序）\n\n\nflex 属性flex 属性定义子项目分配剩余空间，用 flex 来表示占多少份数，默认值是 0\n.item &#123;\tflex: &lt;number&gt;;&#125;\n\n典型场景：一行中三个元素，最左侧元素固定、最右侧元素固定，中间元素占的宽度自适应（圣杯布局）。\n还可以类似 bootstrap 中的 col，给元素平均、指定分配占比，并且拉伸自适应。\n示例：\n&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;style&gt;    section &#123;        display: flex;        background-color: burlywood;    &#125;    section div:nth-child(1) &#123; /* 表示第一个子项 */        width: 100px;        height: 100px;        background-color: cadetblue;    &#125;    section div:nth-child(2) &#123; /* 表示第二个子项 */        flex: 1;  /* 占用所有剩余空间 */        height: 100px;        background-color: grey;    &#125;    section div:nth-child(3) &#123; /* 表示第三个子项 */        width: 100px;        height: 100px;        background-color: cadetblue;    &#125;    p &#123;        margin: 40px;        display: flex;        background-color: burlywood;    &#125;    p span &#123;        flex: 1;   /* 三个子项宽度平均分配 */        border: 1px dashed;        height: 100px;    &#125;    #demo1 &#123;        margin: 40px;        display: flex;        background-color: burlywood;    &#125;    #demo1 span &#123;        flex: 1;   /* 三个子项宽度平均分配 */        border: 1px dashed;        height: 100px;    &#125;    #demo1 span:nth-child(2) &#123;        flex: 2;  /* 第二个元素占2份，其他两个格子各占1份 */    &#125;&lt;/style&gt;&lt;body&gt;    &lt;section&gt;        &lt;div&gt;1&lt;/div&gt;        &lt;div&gt;2&lt;/div&gt;        &lt;div&gt;3&lt;/div&gt;    &lt;/section&gt;    &lt;p&gt;        &lt;span&gt;1&lt;/span&gt;        &lt;span&gt;2&lt;/span&gt;        &lt;span&gt;3&lt;/span&gt;    &lt;/p&gt;    &lt;div id=&quot;demo1&quot;&gt;        &lt;span&gt;1&lt;/span&gt;        &lt;span&gt;2&lt;/span&gt;        &lt;span&gt;3&lt;/span&gt;    &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\n\n\n\nalign-self控制子项自己在侧轴上的排列方式，可以允许单个子项与其他子项有不一样的对齐方式，可以覆盖父元素 align-items 属性。默认值为 auto，表示继承父元素的 align-items 属性。如果没有父元素，等同于 stretch。\n示例：\n&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;style&gt;    p &#123;        margin: 40px;        height: 400px;        display: flex;        align-items: center;        background-color: burlywood;    &#125;    p span &#123;        flex: 1;   /* 三个子项宽度平均分配 */        border: 1px dashed;        height: 100px;    &#125;    p span:nth-child(2) &#123;  /* 让第二个子项向侧轴的底部对齐 */        align-self: flex-end;    &#125;&lt;/style&gt;&lt;body&gt;    &lt;p&gt;        &lt;span&gt;1&lt;/span&gt;        &lt;span&gt;2&lt;/span&gt;        &lt;span&gt;3&lt;/span&gt;    &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\n\n\n\norder定义子项的排列顺序，数值越小，排列越靠前，默认为 0。\n示例：让原本1、2、3排列的三个子项变成2、1、3排列。\n&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;style&gt;    p &#123;        margin: 40px;        height: 400px;        display: flex;        align-items: center;        background-color: burlywood;    &#125;    p span &#123;        flex: 1;        border: 1px dashed;        height: 100px;    &#125;    p span:nth-child(2) &#123;        order: -1; /* 让第二个子项排前面，顺序变成2 1 3，不设默认是1 2 3排列的 */    &#125;&lt;/style&gt;&lt;body&gt;    &lt;p&gt;        &lt;span&gt;1&lt;/span&gt;        &lt;span&gt;2&lt;/span&gt;        &lt;span&gt;3&lt;/span&gt;    &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;","categories":["前端"],"tags":["flex","css"]},{"title":"验证 X.509 证书有效性","url":"/2021/08/05/%E9%AA%8C%E8%AF%81-X-509-%E8%AF%81%E4%B9%A6%E6%9C%89%E6%95%88%E6%80%A7/","content":"验证 X.509 证书的有效性X.509 是公钥证书的格式标准，非常常见。但是证书可以自签、被撤销，就需要验证 CA 证书是否有效，下面以 python 为例。\n检查 CA 的有效性，从 OCSP（在线证书状态协议）可以得到证书是否被签发方回收\n\n\nimport base64import sslimport requestsfrom urllib.parse import urljoinfrom cryptography import x509from cryptography.hazmat.backends import default_backendfrom cryptography.hazmat.primitives import serializationfrom cryptography.hazmat.primitives.hashes import SHA256from cryptography.x509 import ocspfrom cryptography.x509.ocsp import OCSPResponseStatusfrom cryptography.x509.oid import ExtensionOID, AuthorityInformationAccessOIDdef get_cert_for_hostname(hostname, port=443):    conn = ssl.create_connection((hostname, port))    context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)    sock = context.wrap_socket(conn, server_hostname=hostname)    certDER = sock.getpeercert(True)    certPEM = ssl.DER_cert_to_PEM_cert(certDER)    return x509.load_pem_x509_certificate(certPEM.encode(&#x27;ascii&#x27;), default_backend())def get_issuer(cert):    aia = cert.extensions.get_extension_for_oid(ExtensionOID.AUTHORITY_INFORMATION_ACCESS).value    issuers = [ia for ia in aia if ia.access_method == AuthorityInformationAccessOID.CA_ISSUERS]    if not issuers:        raise Exception(f&#x27;no issuers entry in AIA&#x27;)    return issuers[0].access_location.valuedef get_ocsp_server(cert):    aia = cert.extensions.get_extension_for_oid(ExtensionOID.AUTHORITY_INFORMATION_ACCESS).value    ocsps = [ia for ia in aia if ia.access_method == AuthorityInformationAccessOID.OCSP]    if not ocsps:        raise Exception(f&#x27;no ocsp server entry in AIA&#x27;)    return ocsps[0].access_location.valuedef get_issuer_cert(ca_issuer):    issuer_response = requests.get(ca_issuer)    if issuer_response.ok:        issuerDER = issuer_response.content        issuerPEM = ssl.DER_cert_to_PEM_cert(issuerDER)        return x509.load_pem_x509_certificate(issuerPEM.encode(&#x27;ascii&#x27;), default_backend())    raise Exception(f&#x27;fetching issuer cert  failed with response status: &#123;issuer_response.status_code&#125;&#x27;)def get_oscp_request(ocsp_server, cert, issuer_cert):    builder = ocsp.OCSPRequestBuilder()    builder = builder.add_certificate(cert, issuer_cert, SHA256())    req = builder.build()    req_path = base64.b64encode(req.public_bytes(serialization.Encoding.DER))    return urljoin(ocsp_server + &#x27;/&#x27;, req_path.decode(&#x27;ascii&#x27;))def get_ocsp_cert_status(ocsp_server, cert, issuer_cert):    ocsp_resp = requests.get(get_oscp_request(ocsp_server, cert, issuer_cert))    if ocsp_resp.ok:        ocsp_decoded = ocsp.load_der_ocsp_response(ocsp_resp.content)        if ocsp_decoded.response_status == OCSPResponseStatus.SUCCESSFUL:            return ocsp_decoded.certificate_status        else:            raise Exception(f&#x27;decoding ocsp response failed: &#123;ocsp_decoded.response_status&#125;&#x27;)    raise Exception(f&#x27;fetching ocsp cert status failed with response status: &#123;ocsp_resp.status_code&#125;&#x27;)def get_cert_status_for_host(hostname, port=443):    print(&#x27;   hostname:&#x27;, hostname, &quot;port:&quot;, port)    cert = get_cert_for_hostname(hostname, port)    ca_issuer = get_issuer(cert)    print(&#x27;   issuer -&gt;&#x27;, ca_issuer)    issuer_cert = get_issuer_cert(ca_issuer)    ocsp_server = get_ocsp_server(cert)    print(&#x27;   ocsp_server -&gt;&#x27;, ocsp_server)    return get_ocsp_cert_status(ocsp_server, cert, issuer_cert)if __name__ == &#x27;__main__&#x27;:    # CA证书处于有效期间    # host = &quot;blog.an00.cn&quot;        # 已回收的例子    host = &quot;revoked.badssl.com&quot;    status = get_cert_status_for_host(host)    print(host, status)\n检查结果\n   hostname: blog.an00.cn port: 443   issuer -&gt; http://cacerts.digitalcertvalidation.com/TrustAsiaTLSRSACA.crt   ocsp_server -&gt; http://statuse.digitalcertvalidation.comblog.an00.cn OCSPCertStatus.GOOD===========================================   hostname: revoked.badssl.com port: 443   issuer -&gt; http://cacerts.digicert.com/RapidSSLTLSDVRSAMixedSHA2562020CA-1.crt   ocsp_server -&gt; http://ocsp.digicert.comrevoked.badssl.com OCSPCertStatus.REVOKED","categories":["CA"],"tags":["X.509"]}]